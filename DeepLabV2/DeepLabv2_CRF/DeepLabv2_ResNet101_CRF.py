# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LFw8XKo0jwr56saJ9fz7qDmH82KEyRft
"""

import os
import zipfile

# URL for the dataset
url = "https://zenodo.org/records/5706578/files/Train.zip?download=1"

# Download the file using wget
!wget -O /content/Train.zip "$url"

# Define the extraction path
extract_path = '/content/datasets/Train/'

# Create the extraction directory if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile('/content/Train.zip', 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List the contents of the extracted folder
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

import os
import zipfile

# URL for the dataset
url = "https://zenodo.org/records/5706578/files/Val.zip?download=1"

# Download the file using wget
!wget -O /content/Val.zip "$url"

# Define the extraction path
extract_path = '/content/datasets/Val/'

# Create the extraction directory if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile('/content/Val.zip', 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# List the contents of the extracted folder
extracted_files = os.listdir(extract_path)
print("Extracted files:", extracted_files)

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
from torchvision import models

expansion = 4

class ConvBN(nn.Module):  # Convolutional followed by Batch Norm
    def __init__(self, in_planes, out_planes, kernel_size=1, stride=1, padding=0, dilation=1):
        super(ConvBN, self).__init__()
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,
                              padding=padding, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=1e-3)

    def forward(self, x):
        return self.bn(self.conv(x))

class Bottleneck(nn.Module):
    def __init__(self, in_planes, out_planes, stride=1, dilation=1, downsample=False, dropout_rate=0.5):
        super(Bottleneck, self).__init__()
        mid_planes = out_planes // expansion
        self.conv1 = ConvBN(in_planes, mid_planes, kernel_size=1, stride=stride)
        self.relu1 = nn.ReLU(inplace=True)

        # Dropout after the first convolution
        self.dropout1 = nn.Dropout2d(p=dropout_rate)

        self.conv2 = ConvBN(mid_planes, mid_planes, kernel_size=3, stride=1, padding=dilation, dilation=dilation)
        self.relu2 = nn.ReLU(inplace=True)

        # Dropout after the second convolution
        self.dropout2 = nn.Dropout2d(p=dropout_rate)

        self.conv3 = ConvBN(mid_planes, out_planes, kernel_size=1)
        self.relu3 = nn.ReLU(inplace=True)

        # Dropout after the third convolution
        self.dropout3 = nn.Dropout2d(p=dropout_rate)

        if downsample:
            self.shortcut = ConvBN(in_planes, out_planes, kernel_size=1, stride=stride)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x):
        identity = self.shortcut(x)
        out = self.relu1(self.conv1(x))

        # Apply dropout after the first convolution
        out = self.dropout1(out)

        out = self.relu2(self.conv2(out))

        # Apply dropout after the second convolution
        out = self.dropout2(out)

        out = self.conv3(out)

        # Apply dropout after the third convolution
        out = self.dropout3(out)

        out += identity
        return self.relu3(out)

def make_layer(blocks, in_planes, out_planes, stride, dilation, dropout_rate=0.5):
    layers = OrderedDict()
    layers['block1'] = Bottleneck(in_planes, out_planes, stride=stride, dilation=dilation, downsample=True, dropout_rate=dropout_rate)
    for i in range(1, blocks):
        layers[f'block{i+1}'] = Bottleneck(out_planes, out_planes, stride=1, dilation=dilation, dropout_rate=dropout_rate)
    return nn.Sequential(layers)

class ASPP(nn.Module):
    def __init__(self, in_planes, out_planes, atrous_rates):
        super(ASPP, self).__init__()
        self.convs = nn.ModuleList([
            nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,
                      padding=rate, dilation=rate, bias=True) for rate in atrous_rates
        ])
        self._init_weights()

    def _init_weights(self):
        for conv in self.convs:
            nn.init.normal_(conv.weight, mean=0, std=0.01)
            nn.init.constant_(conv.bias, 0)

    def forward(self, x):
        return sum(conv(x) for conv in self.convs)


# Define DeepLabV2 Model
class DeepLabV2(nn.Module):
    def __init__(self, n_classes):
        super(DeepLabV2, self).__init__()
        # Use torchvision's resnet101 pretrained on ImageNet by default
        backbone = models.resnet101(pretrained=True)  # Use pretrained=True directly

        # Keep only layers up to layer4
        self.backbone = nn.Sequential(*(list(backbone.children())[:-2]))  # Exclude the final FC layer
        self.aspp = nn.ModuleList([
            nn.Conv2d(2048, 256, kernel_size=3, padding=r, dilation=r, bias=True) # list of modules with different dilation rates
            for r in [6, 12, 18, 24]
        ])
        self.classifier = nn.Conv2d(256, n_classes, kernel_size=1)

         # Add upsampling layer
        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True) # Upsample by 32 to match input size

    def forward(self, x):
        x = self.backbone(x)
        aspp_out = sum(aspp(x) for aspp in self.aspp) # the outputs of the four convolutions are summed together

        x = self.classifier(aspp_out) # Apply the classifier

        # Upsample the output
        x = self.upsample(x) # Apply upsampling

        return x

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from PIL import Image
import numpy as np

import albumentations as A
from albumentations.pytorch import ToTensorV2

# Define Dataset for Segmentation (Train and Validation)
class SimpleSegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=None, augment=False):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.augment = augment
        self.images = sorted(os.listdir(image_dir))
        self.masks = sorted(os.listdir(mask_dir))
        self.aug_transform = A.Compose([
            A.HorizontalFlip(p=0.5),  # Random horizontal flip
            A.VerticalFlip(p=0.5),    # Random vertical flip
            A.Rotate(limit=30, p=0.5),  # Random rotation (-30 to +30 degrees)
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
            A.GaussianBlur(p=0.3),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.open(os.path.join(self.image_dir, self.images[idx])).convert('RGB')
        mask = Image.open(os.path.join(self.mask_dir, self.masks[idx]))

        # Convert image and mask to numpy for augmentation
        image = np.array(image)
        mask = np.array(mask)

        # Apply augmentation for training
        if self.augment and self.aug_transform:
            augmented = self.aug_transform(image=image, mask=mask)
            image = augmented["image"]
            mask = augmented["mask"]
        else:
            # Apply default transform (resize, normalization, etc.)
            if self.transform:
                image = self.transform(Image.fromarray(image))
                mask = torch.tensor(mask, dtype=torch.long)  # Convert to LongTensor for CrossEntropy

        # Ensure mask is always torch.long before returning
        mask = mask.type(torch.long) # Convert the mask to torch.long

        return image, mask

# Define Transform for Validation
val_transform = transforms.Compose([
    transforms.Resize((1024, 1024)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Calculate IoU (Intersection over Union) for validation
def calculate_iou(output, target, num_classes):
    output = torch.argmax(output, dim=1)
    iou_list = []
    for i in range(num_classes):
        intersection = ((output == i) & (target == i)).sum().float()
        union = ((output == i) | (target == i)).sum().float()
        iou = intersection / (union + 1e-6)  # Avoid division by zero
        iou_list.append(iou.item())
    return np.mean(iou_list)



def train_with_crf():

    import cython

    !pip install -U cython
    !pip install git+https://github.com/lucasb-eyer/pydensecrf.git
    from pydensecrf import densecrf as dcrf
    from pydensecrf.utils import unary_from_softmax, create_pairwise_gaussian, create_pairwise_bilateral

    def apply_crf(image, probs, num_classes):
      """
      Refines the segmentation map using DenseCRF.
      Args:
          image (numpy.ndarray): The input image (H, W, 3) in RGB format.
          probs (numpy.ndarray): The predicted probabilities (num_classes, H, W).
          num_classes (int): The number of classes in the segmentation task.
      Returns:
          numpy.ndarray: The refined segmentation map (H, W).
      """
      # Ensure the image is C-contiguous
      image = np.ascontiguousarray(image) # Added this line to fix the contiguity

      # Convert the image to uint8 before passing to CRF
      image = (image * 255).astype(np.uint8) # Convert the image to uint8 format

      H, W, _ = image.shape
      d = dcrf.DenseCRF2D(W, H, num_classes)

      # Compute the unary potential from predicted probabilities
      unary = unary_from_softmax(probs)
      d.setUnaryEnergy(unary)

      # Add pairwise Gaussian potential, sxy controls spacial extent of the smoothing
      d.addPairwiseGaussian(sxy=3, compat=3, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)

      # Add pairwise Bilateral potential, srgb controls spacial extent of smoothing and srgb controls spatial extent of bilateral filter
      # compact determines how strongly the pairwise potential influence the final segmentation
      d.addPairwiseBilateral(sxy=50, srgb=13, rgbim=image, compat=10, kernel=dcrf.DIAG_KERNEL, normalization=dcrf.NORMALIZE_SYMMETRIC)

      # Perform CRF inference
      Q = d.inference(5)  # Perform 5 iterations
      refined_segmentation = np.argmax(Q, axis=0).reshape(H, W)
      return refined_segmentation

    dataset_dir = "datasets/Train/Train/Rural"
    output_dir = "checkpoints"
    os.makedirs(output_dir, exist_ok=True)
    log_dir = "logs"
    batch_size = 4
    num_classes = 8
    lr = 0.001
    epochs = 20
    save_interval = 2

    # Paths for training and validation data
    train_images = os.path.join(dataset_dir, "images_png")
    train_masks = os.path.join(dataset_dir, "masks_png")
    val_dir = "datasets/Val/Val/Rural"
    val_images = os.path.join(val_dir, "images_png")
    val_masks = os.path.join(val_dir, "masks_png")

    # Datasets and DataLoaders
    train_dataset = SimpleSegmentationDataset(train_images, train_masks, transform=None, augment=True)
    val_dataset = SimpleSegmentationDataset(val_images, val_masks, transform=val_transform, augment=False)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    # Model Setup
    model = DeepLabV2(n_classes=num_classes)
    model = nn.DataParallel(model).cuda()

    # Optimizer and Loss
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()

    # TensorBoard Setup
    writer = SummaryWriter(log_dir=log_dir)

    # Training Loop
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for images, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            images, masks = images.cuda(), masks.cuda()

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        writer.add_scalar("Loss/train", avg_loss, epoch)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

        # Validation
        model.eval()
        val_loss = 0.0
        val_iou = 0.0

        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc="Validation"):
                images, masks = images.cuda(), masks.cuda()

                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item()

                probs = F.softmax(outputs, dim=1).cpu().numpy()  # Convert to probabilities

                # Apply CRF refinement for each image
                refined_preds = []
                for i in range(len(images)):
                    image = images[i].permute(1, 2, 0).cpu().numpy()  # Convert to HWC
                    prob_map = probs[i]
                    refined_pred = apply_crf(image, prob_map, num_classes)
                    refined_preds.append(refined_pred)

                # Evaluate refined predictions
                for refined_pred, mask in zip(refined_preds, masks):
                    iou = calculate_iou(torch.tensor(refined_pred), mask.cpu(), num_classes)
                    val_iou += iou

        avg_val_loss = val_loss / len(val_loader)
        avg_val_iou = val_iou / len(val_loader)
        writer.add_scalar("Loss/val", avg_val_loss, epoch)
        writer.add_scalar("IoU/val", avg_val_iou, epoch)
        print(f"Validation - Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}")

        # Save Model Checkpoint
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = os.path.join(output_dir, f"model_epoch_{epoch+1}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)

    writer.close()


# Start training the model with CRF
if __name__ == "__main__":
    train_with_crf()