2025-01-25 21:24:37,796 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-25 21:24:37,796 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa-Rural/train.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: True
  AUG: True
  AUG1: False
  AUG2: True
  AUG3: True
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  D1: False
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.0002
  LR: 0.001
  LR_D1: 0.0001
  LR_D2: 0.0001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-25 21:24:38,063 Attention!!!
2025-01-25 21:24:38,064 Loaded 302 parameters!
2025-01-25 21:24:38,064 Over!!!
2025-01-25 21:24:45,314 Epoch: [0/20] Iter:[0/290], Time: 6.94, lr: [0.0002], Loss: 5.729713, Loss_D1: 0.693202, Loss_D2: 0.693266, Acc:0.150900, Semantic loss: 0.625235
2025-01-25 21:25:28,717 Epoch: [0/20] Iter:[10/290], Time: 4.58, lr: [0.0001996896284016207], Loss: 3.378727, Loss_D1: 0.693350, Loss_D2: 0.693459, Acc:0.222659, Semantic loss: 0.509070
2025-01-25 21:26:10,977 Epoch: [0/20] Iter:[20/290], Time: 4.41, lr: [0.00019937920319381742], Loss: 2.931175, Loss_D1: 0.694204, Loss_D2: 0.691261, Acc:0.255511, Semantic loss: 0.452218
2025-01-25 21:26:55,409 Epoch: [0/20] Iter:[30/290], Time: 4.42, lr: [0.0001990687242745565], Loss: 2.671792, Loss_D1: 0.692800, Loss_D2: 0.692445, Acc:0.276401, Semantic loss: 0.413286
2025-01-25 21:27:39,099 Epoch: [0/20] Iter:[40/290], Time: 4.41, lr: [0.00019875819154143275], Loss: 2.497204, Loss_D1: 0.689672, Loss_D2: 0.692411, Acc:0.299794, Semantic loss: 0.383328
2025-01-25 21:28:22,995 Epoch: [0/20] Iter:[50/290], Time: 4.40, lr: [0.0001984476048916676], Loss: 2.395917, Loss_D1: 0.692739, Loss_D2: 0.692267, Acc:0.318278, Semantic loss: 0.354027
2025-01-25 21:29:06,692 Epoch: [0/20] Iter:[60/290], Time: 4.40, lr: [0.00019813696422210707], Loss: 2.256531, Loss_D1: 0.695087, Loss_D2: 0.693767, Acc:0.327271, Semantic loss: 0.329719
2025-01-25 21:29:50,209 Epoch: [0/20] Iter:[70/290], Time: 4.39, lr: [0.00019782626942921983], Loss: 2.194376, Loss_D1: 0.693525, Loss_D2: 0.692766, Acc:0.337497, Semantic loss: 0.314404
2025-01-25 21:30:34,075 Epoch: [0/20] Iter:[80/290], Time: 4.39, lr: [0.000197515520409095], Loss: 2.112527, Loss_D1: 0.694480, Loss_D2: 0.693226, Acc:0.350045, Semantic loss: 0.298098
2025-01-25 21:31:18,461 Epoch: [0/20] Iter:[90/290], Time: 4.40, lr: [0.0001972047170574402], Loss: 2.038873, Loss_D1: 0.686737, Loss_D2: 0.693763, Acc:0.362416, Semantic loss: 0.285472
2025-01-25 21:32:02,604 Epoch: [0/20] Iter:[100/290], Time: 4.40, lr: [0.0001968938592695795], Loss: 1.987799, Loss_D1: 0.692932, Loss_D2: 0.690121, Acc:0.370519, Semantic loss: 0.276003
2025-01-25 21:32:47,157 Epoch: [0/20] Iter:[110/290], Time: 4.40, lr: [0.00019658294694045123], Loss: 1.946056, Loss_D1: 0.709474, Loss_D2: 0.695457, Acc:0.375357, Semantic loss: 0.268144
2025-01-25 21:33:31,708 Epoch: [0/20] Iter:[120/290], Time: 4.41, lr: [0.00019627197996460594], Loss: 1.908619, Loss_D1: 0.692489, Loss_D2: 0.695092, Acc:0.378807, Semantic loss: 0.261159
2025-01-25 21:34:16,875 Epoch: [0/20] Iter:[130/290], Time: 4.42, lr: [0.00019596095823620432], Loss: 1.901278, Loss_D1: 0.668020, Loss_D2: 0.694111, Acc:0.387684, Semantic loss: 0.255235
2025-01-25 21:35:01,024 Epoch: [0/20] Iter:[140/290], Time: 4.42, lr: [0.000195649881649015], Loss: 1.876136, Loss_D1: 0.698105, Loss_D2: 0.691535, Acc:0.392186, Semantic loss: 0.249979
2025-01-25 21:35:45,139 Epoch: [0/20] Iter:[150/290], Time: 4.42, lr: [0.00019533875009641242], Loss: 1.839966, Loss_D1: 0.683305, Loss_D2: 0.693465, Acc:0.395268, Semantic loss: 0.244557
2025-01-25 21:36:25,496 Epoch: [0/20] Iter:[160/290], Time: 4.39, lr: [0.00019502756347137466], Loss: 1.808447, Loss_D1: 0.703934, Loss_D2: 0.693276, Acc:0.401541, Semantic loss: 0.239976
2025-01-25 21:37:10,825 Epoch: [0/20] Iter:[170/290], Time: 4.40, lr: [0.0001947163216664814], Loss: 1.790827, Loss_D1: 0.685617, Loss_D2: 0.693667, Acc:0.404381, Semantic loss: 0.236509
2025-01-25 21:37:55,047 Epoch: [0/20] Iter:[180/290], Time: 4.40, lr: [0.00019440502457391143], Loss: 1.773821, Loss_D1: 0.682757, Loss_D2: 0.693247, Acc:0.408506, Semantic loss: 0.232509
2025-01-25 21:38:40,473 Epoch: [0/20] Iter:[190/290], Time: 4.41, lr: [0.00019409367208544069], Loss: 1.756512, Loss_D1: 0.612839, Loss_D2: 0.692999, Acc:0.411769, Semantic loss: 0.229444
2025-01-25 21:39:26,824 Epoch: [0/20] Iter:[200/290], Time: 4.42, lr: [0.00019378226409244], Loss: 1.749100, Loss_D1: 0.681865, Loss_D2: 0.692483, Acc:0.417747, Semantic loss: 0.225959
2025-01-25 21:40:10,427 Epoch: [0/20] Iter:[210/290], Time: 4.42, lr: [0.00019347080048587277], Loss: 1.739464, Loss_D1: 0.719187, Loss_D2: 0.694642, Acc:0.420347, Semantic loss: 0.222902
2025-01-25 21:40:49,526 Epoch: [0/20] Iter:[220/290], Time: 4.39, lr: [0.0001931592811562927], Loss: 1.729320, Loss_D1: 0.650044, Loss_D2: 0.691528, Acc:0.423104, Semantic loss: 0.220620
2025-01-25 21:41:29,273 Epoch: [0/20] Iter:[230/290], Time: 4.38, lr: [0.0001928477059938416], Loss: 1.711574, Loss_D1: 0.718101, Loss_D2: 0.693707, Acc:0.423905, Semantic loss: 0.218326
2025-01-25 21:42:12,898 Epoch: [0/20] Iter:[240/290], Time: 4.38, lr: [0.00019253607488824707], Loss: 1.694214, Loss_D1: 0.652292, Loss_D2: 0.693096, Acc:0.425559, Semantic loss: 0.216041
2025-01-25 21:42:55,396 Epoch: [0/20] Iter:[250/290], Time: 4.37, lr: [0.00019222438772882014], Loss: 1.689973, Loss_D1: 0.651327, Loss_D2: 0.692979, Acc:0.428196, Semantic loss: 0.213957
2025-01-25 21:43:38,507 Epoch: [0/20] Iter:[260/290], Time: 4.37, lr: [0.00019191264440445303], Loss: 1.677018, Loss_D1: 0.681328, Loss_D2: 0.693206, Acc:0.429000, Semantic loss: 0.212201
2025-01-25 21:44:20,902 Epoch: [0/20] Iter:[270/290], Time: 4.36, lr: [0.00019160084480361665], Loss: 1.673393, Loss_D1: 0.674261, Loss_D2: 0.692888, Acc:0.432402, Semantic loss: 0.209953
2025-01-25 21:45:09,801 Epoch: [0/20] Iter:[280/290], Time: 4.38, lr: [0.00019128898881435848], Loss: 1.657132, Loss_D1: 0.690080, Loss_D2: 0.692987, Acc:0.432617, Semantic loss: 0.207800
2025-01-25 21:49:49,185 0 [0.         0.41360335 0.14985017 0.0915825  0.23056898 0.14993792
 0.12613788 0.00187601] 0.16622240196195798
2025-01-25 21:49:49,186 1 [0.         0.4345371  0.18447471 0.17634107 0.34297693 0.10797437
 0.14056265 0.00473698] 0.1988005445970265
2025-01-25 21:49:49,197 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-25 21:49:49,623 Loss: 1.227, MeanIU:  0.1988, Best_mIoU:  0.1988
2025-01-25 21:49:49,623 [0.         0.4345371  0.18447471 0.17634107 0.34297693 0.10797437
 0.14056265 0.00473698]
2025-01-25 21:49:53,587 Epoch: [1/20] Iter:[0/290], Time: 3.95, lr: [0.0003819541526485999], Loss: 0.865156, Loss_D1: 0.637651, Loss_D2: 0.693066, Acc:0.257495, Semantic loss: 0.173023
2025-01-25 21:51:03,879 Epoch: [1/20] Iter:[10/290], Time: 6.75, lr: [0.00038133021444126844], Loss: 1.377165, Loss_D1: 0.639177, Loss_D2: 0.693319, Acc:0.474550, Semantic loss: 0.148858
2025-01-25 21:52:12,925 Epoch: [1/20] Iter:[20/290], Time: 6.82, lr: [0.0003807061627802472], Loss: 1.372965, Loss_D1: 0.659073, Loss_D2: 0.693044, Acc:0.477870, Semantic loss: 0.157639
2025-01-25 21:53:16,769 Epoch: [1/20] Iter:[30/290], Time: 6.68, lr: [0.00038008199743819464], Loss: 1.345965, Loss_D1: 0.655448, Loss_D2: 0.693262, Acc:0.475482, Semantic loss: 0.157564
2025-01-25 21:54:16,151 Epoch: [1/20] Iter:[40/290], Time: 6.50, lr: [0.0003794577181868978], Loss: 1.374631, Loss_D1: 0.696281, Loss_D2: 0.693304, Acc:0.482234, Semantic loss: 0.159953
2025-01-25 21:55:08,546 Epoch: [1/20] Iter:[50/290], Time: 6.25, lr: [0.0003788333247972675], Loss: 1.381281, Loss_D1: 0.655063, Loss_D2: 0.693022, Acc:0.468806, Semantic loss: 0.163112
2025-01-25 21:56:19,258 Epoch: [1/20] Iter:[60/290], Time: 6.39, lr: [0.0003782088170393335], Loss: 1.396634, Loss_D1: 0.624684, Loss_D2: 0.692864, Acc:0.470649, Semantic loss: 0.163798
2025-01-25 21:57:34,608 Epoch: [1/20] Iter:[70/290], Time: 6.55, lr: [0.000377584194682239], Loss: 1.417774, Loss_D1: 0.652849, Loss_D2: 0.695037, Acc:0.479401, Semantic loss: 0.163584
2025-01-25 21:58:34,831 Epoch: [1/20] Iter:[80/290], Time: 6.48, lr: [0.00037695945749423657], Loss: 1.425120, Loss_D1: 0.620057, Loss_D2: 0.693786, Acc:0.483967, Semantic loss: 0.161762
2025-01-25 21:59:41,216 Epoch: [1/20] Iter:[90/290], Time: 6.50, lr: [0.0003763346052426817], Loss: 1.428169, Loss_D1: 0.662029, Loss_D2: 0.690375, Acc:0.480436, Semantic loss: 0.162725
2025-01-25 22:00:31,603 Epoch: [1/20] Iter:[100/290], Time: 6.36, lr: [0.00037570963769402904], Loss: 1.397265, Loss_D1: 0.636814, Loss_D2: 0.690283, Acc:0.479497, Semantic loss: 0.162521
2025-01-25 22:01:34,084 Epoch: [1/20] Iter:[110/290], Time: 6.35, lr: [0.0003750845546138262], Loss: 1.409312, Loss_D1: 0.658620, Loss_D2: 0.702092, Acc:0.476588, Semantic loss: 0.162846
2025-01-25 22:02:40,050 Epoch: [1/20] Iter:[120/290], Time: 6.37, lr: [0.0003744593557667091], Loss: 1.422692, Loss_D1: 0.700955, Loss_D2: 0.692890, Acc:0.478194, Semantic loss: 0.163246
2025-01-25 22:03:41,939 Epoch: [1/20] Iter:[130/290], Time: 6.35, lr: [0.0003738340409163965], Loss: 1.412690, Loss_D1: 0.657789, Loss_D2: 0.693089, Acc:0.478392, Semantic loss: 0.161732
2025-01-25 22:04:42,265 Epoch: [1/20] Iter:[140/290], Time: 6.33, lr: [0.00037320860982568494], Loss: 1.402627, Loss_D1: 0.657186, Loss_D2: 0.693306, Acc:0.474791, Semantic loss: 0.161554
2025-01-25 22:05:39,792 Epoch: [1/20] Iter:[150/290], Time: 6.29, lr: [0.00037258306225644295], Loss: 1.405103, Loss_D1: 0.629584, Loss_D2: 0.693679, Acc:0.470868, Semantic loss: 0.161353
2025-01-25 22:06:28,985 Epoch: [1/20] Iter:[160/290], Time: 6.21, lr: [0.00037195739796960645], Loss: 1.395340, Loss_D1: 0.631012, Loss_D2: 0.694227, Acc:0.470478, Semantic loss: 0.161346
2025-01-25 22:07:40,816 Epoch: [1/20] Iter:[170/290], Time: 6.26, lr: [0.0003713316167251724], Loss: 1.393267, Loss_D1: 0.669529, Loss_D2: 0.692147, Acc:0.471854, Semantic loss: 0.160275
2025-01-25 22:08:35,722 Epoch: [1/20] Iter:[180/290], Time: 6.22, lr: [0.00037070571828219423], Loss: 1.384866, Loss_D1: 0.617128, Loss_D2: 0.692541, Acc:0.474369, Semantic loss: 0.159275
2025-01-25 22:09:36,277 Epoch: [1/20] Iter:[190/290], Time: 6.21, lr: [0.0003700797023987757], Loss: 1.377731, Loss_D1: 0.640255, Loss_D2: 0.692700, Acc:0.474695, Semantic loss: 0.158669
2025-01-25 22:10:38,472 Epoch: [1/20] Iter:[200/290], Time: 6.21, lr: [0.0003694535688320657], Loss: 1.367994, Loss_D1: 0.623275, Loss_D2: 0.697482, Acc:0.473752, Semantic loss: 0.157848
2025-01-25 22:11:37,795 Epoch: [1/20] Iter:[210/290], Time: 6.20, lr: [0.00036882731733825275], Loss: 1.362927, Loss_D1: 0.653085, Loss_D2: 0.701795, Acc:0.472771, Semantic loss: 0.157482
2025-01-25 22:12:41,028 Epoch: [1/20] Iter:[220/290], Time: 6.21, lr: [0.000368200947672559], Loss: 1.370208, Loss_D1: 0.657775, Loss_D2: 0.693837, Acc:0.474902, Semantic loss: 0.157209
2025-01-25 22:13:33,808 Epoch: [1/20] Iter:[230/290], Time: 6.17, lr: [0.00036757445958923496], Loss: 1.363888, Loss_D1: 0.654733, Loss_D2: 0.693850, Acc:0.475314, Semantic loss: 0.156814
2025-01-25 22:14:31,992 Epoch: [1/20] Iter:[240/290], Time: 6.15, lr: [0.00036694785284155346], Loss: 1.362758, Loss_D1: 0.677992, Loss_D2: 0.692613, Acc:0.475446, Semantic loss: 0.156194
2025-01-25 22:15:35,161 Epoch: [1/20] Iter:[250/290], Time: 6.16, lr: [0.0003663211271818044], Loss: 1.360982, Loss_D1: 0.632783, Loss_D2: 0.693403, Acc:0.476100, Semantic loss: 0.155781
2025-01-25 22:16:40,609 Epoch: [1/20] Iter:[260/290], Time: 6.17, lr: [0.00036569428236128817], Loss: 1.357182, Loss_D1: 0.660157, Loss_D2: 0.693099, Acc:0.477107, Semantic loss: 0.155439
2025-01-25 22:17:41,603 Epoch: [1/20] Iter:[270/290], Time: 6.17, lr: [0.0003650673181303107], Loss: 1.355591, Loss_D1: 0.672809, Loss_D2: 0.693452, Acc:0.477327, Semantic loss: 0.155047
2025-01-25 22:18:52,711 Epoch: [1/20] Iter:[280/290], Time: 6.20, lr: [0.0003644402342381768], Loss: 1.356547, Loss_D1: 0.624488, Loss_D2: 0.693651, Acc:0.477884, Semantic loss: 0.155063
2025-01-25 22:23:30,330 0 [0.         0.41629906 0.13188625 0.10257065 0.14361929 0.11114238
 0.10215202 0.00909062] 0.14525146790039287
2025-01-25 22:23:30,331 1 [0.         0.45514577 0.17419086 0.21281483 0.12474065 0.008323
 0.08801315 0.13775514] 0.1715690572321195
2025-01-25 22:23:30,332 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-25 22:23:30,542 Loss: 1.250, MeanIU:  0.1716, Best_mIoU:  0.1988
2025-01-25 22:23:30,542 [0.         0.45514577 0.17419086 0.21281483 0.12474065 0.008323
 0.08801315 0.13775514]
2025-01-25 22:23:38,769 Epoch: [2/20] Iter:[0/290], Time: 8.22, lr: [0.0005457195456497774], Loss: 1.586132, Loss_D1: 0.646275, Loss_D2: 0.692926, Acc:0.597313, Semantic loss: 0.136970
2025-01-25 22:24:44,957 Epoch: [2/20] Iter:[10/290], Time: 6.76, lr: [0.0005447785596939306], Loss: 1.413984, Loss_D1: 0.668253, Loss_D2: 0.693173, Acc:0.494691, Semantic loss: 0.152798
2025-01-25 22:25:49,612 Epoch: [2/20] Iter:[20/290], Time: 6.62, lr: [0.0005438373931091254], Loss: 1.358027, Loss_D1: 0.610984, Loss_D2: 0.692922, Acc:0.501372, Semantic loss: 0.155906
2025-01-25 22:26:45,957 Epoch: [2/20] Iter:[30/290], Time: 6.30, lr: [0.0005428960455132244], Loss: 1.384422, Loss_D1: 0.622534, Loss_D2: 0.693768, Acc:0.500208, Semantic loss: 0.155100
2025-01-25 22:27:44,924 Epoch: [2/20] Iter:[40/290], Time: 6.20, lr: [0.0005419545165225449], Loss: 1.403709, Loss_D1: 0.681365, Loss_D2: 0.692857, Acc:0.498765, Semantic loss: 0.153565
2025-01-25 22:28:51,913 Epoch: [2/20] Iter:[50/290], Time: 6.30, lr: [0.0005410128057518475], Loss: 1.388974, Loss_D1: 0.629268, Loss_D2: 0.693182, Acc:0.484663, Semantic loss: 0.156743
2025-01-25 22:29:51,231 Epoch: [2/20] Iter:[60/290], Time: 6.24, lr: [0.0005400709128143285], Loss: 1.365938, Loss_D1: 0.609176, Loss_D2: 0.693330, Acc:0.483414, Semantic loss: 0.155766
2025-01-25 22:30:48,945 Epoch: [2/20] Iter:[70/290], Time: 6.17, lr: [0.0005391288373216097], Loss: 1.387012, Loss_D1: 0.677963, Loss_D2: 0.693155, Acc:0.484186, Semantic loss: 0.154210
2025-01-25 22:31:46,176 Epoch: [2/20] Iter:[80/290], Time: 6.12, lr: [0.0005381865788837288], Loss: 1.382393, Loss_D1: 0.658050, Loss_D2: 0.692847, Acc:0.478329, Semantic loss: 0.156267
2025-01-25 22:32:46,801 Epoch: [2/20] Iter:[90/290], Time: 6.11, lr: [0.0005372441371091307], Loss: 1.365555, Loss_D1: 0.666401, Loss_D2: 0.693624, Acc:0.480590, Semantic loss: 0.155357
2025-01-25 22:33:40,120 Epoch: [2/20] Iter:[100/290], Time: 6.04, lr: [0.0005363015116046565], Loss: 1.358431, Loss_D1: 0.604555, Loss_D2: 0.693323, Acc:0.480823, Semantic loss: 0.155335
2025-01-25 22:34:35,221 Epoch: [2/20] Iter:[110/290], Time: 5.99, lr: [0.0005353587019755353], Loss: 1.346295, Loss_D1: 0.590158, Loss_D2: 0.692892, Acc:0.480803, Semantic loss: 0.152935
2025-01-25 22:35:30,782 Epoch: [2/20] Iter:[120/290], Time: 5.95, lr: [0.0005344157078253733], Loss: 1.336566, Loss_D1: 0.687407, Loss_D2: 0.693064, Acc:0.474651, Semantic loss: 0.153299
2025-01-25 22:36:30,365 Epoch: [2/20] Iter:[130/290], Time: 5.95, lr: [0.0005334725287561445], Loss: 1.330303, Loss_D1: 0.685832, Loss_D2: 0.693338, Acc:0.469658, Semantic loss: 0.153672
2025-01-25 22:37:24,864 Epoch: [2/20] Iter:[140/290], Time: 5.92, lr: [0.0005325291643681802], Loss: 1.336681, Loss_D1: 0.647418, Loss_D2: 0.692883, Acc:0.471438, Semantic loss: 0.153730
2025-01-25 22:38:26,089 Epoch: [2/20] Iter:[150/290], Time: 5.93, lr: [0.00053158561426016], Loss: 1.347990, Loss_D1: 0.641539, Loss_D2: 0.693076, Acc:0.473779, Semantic loss: 0.153977
2025-01-25 22:39:17,960 Epoch: [2/20] Iter:[160/290], Time: 5.88, lr: [0.0005306418780291005], Loss: 1.345781, Loss_D1: 0.630845, Loss_D2: 0.693161, Acc:0.474892, Semantic loss: 0.153563
2025-01-25 22:40:20,160 Epoch: [2/20] Iter:[170/290], Time: 5.90, lr: [0.000529697955270346], Loss: 1.335912, Loss_D1: 0.681912, Loss_D2: 0.693328, Acc:0.474912, Semantic loss: 0.153624
2025-01-25 22:41:14,751 Epoch: [2/20] Iter:[180/290], Time: 5.88, lr: [0.0005287538455775576], Loss: 1.354141, Loss_D1: 0.663054, Loss_D2: 0.693161, Acc:0.478505, Semantic loss: 0.153752
2025-01-25 22:42:17,840 Epoch: [2/20] Iter:[190/290], Time: 5.90, lr: [0.0005278095485427036], Loss: 1.359337, Loss_D1: 0.654776, Loss_D2: 0.693088, Acc:0.477640, Semantic loss: 0.154398
2025-01-25 22:43:17,188 Epoch: [2/20] Iter:[200/290], Time: 5.90, lr: [0.0005268650637560482], Loss: 1.361770, Loss_D1: 0.622195, Loss_D2: 0.693172, Acc:0.478338, Semantic loss: 0.154286
2025-01-25 22:44:25,235 Epoch: [2/20] Iter:[210/290], Time: 5.95, lr: [0.0005259203908061416], Loss: 1.364863, Loss_D1: 0.730782, Loss_D2: 0.693146, Acc:0.479656, Semantic loss: 0.154197
2025-01-25 22:45:16,661 Epoch: [2/20] Iter:[220/290], Time: 5.91, lr: [0.0005249755292798091], Loss: 1.361402, Loss_D1: 0.667774, Loss_D2: 0.693109, Acc:0.479742, Semantic loss: 0.153930
2025-01-25 22:46:21,293 Epoch: [2/20] Iter:[230/290], Time: 5.93, lr: [0.0005240304787621406], Loss: 1.372424, Loss_D1: 0.652668, Loss_D2: 0.693192, Acc:0.482404, Semantic loss: 0.153313
2025-01-25 22:47:22,882 Epoch: [2/20] Iter:[240/290], Time: 5.94, lr: [0.0005230852388364795], Loss: 1.370237, Loss_D1: 0.644336, Loss_D2: 0.693120, Acc:0.483023, Semantic loss: 0.152996
2025-01-25 22:48:26,659 Epoch: [2/20] Iter:[250/290], Time: 5.96, lr: [0.0005221398090844122], Loss: 1.378158, Loss_D1: 0.705917, Loss_D2: 0.693127, Acc:0.485666, Semantic loss: 0.152657
2025-01-25 22:49:15,665 Epoch: [2/20] Iter:[260/290], Time: 5.92, lr: [0.000521194189085757], Loss: 1.374731, Loss_D1: 0.660017, Loss_D2: 0.693164, Acc:0.484279, Semantic loss: 0.152834
2025-01-25 22:50:11,520 Epoch: [2/20] Iter:[270/290], Time: 5.91, lr: [0.000520248378418553], Loss: 1.373543, Loss_D1: 0.686015, Loss_D2: 0.693161, Acc:0.485380, Semantic loss: 0.152434
2025-01-25 22:51:17,395 Epoch: [2/20] Iter:[280/290], Time: 5.93, lr: [0.0005193023766590487], Loss: 1.376231, Loss_D1: 0.673876, Loss_D2: 0.693152, Acc:0.487547, Semantic loss: 0.152027
2025-01-25 22:56:20,793 0 [0.         0.38927563 0.12990547 0.10359656 0.30532257 0.15280445
 0.07890256 0.0026217 ] 0.16606127719631442
2025-01-25 22:56:20,793 1 [0.         0.44236543 0.17621464 0.13427675 0.35168867 0.1285791
 0.02740431 0.01314721] 0.18195373045504465
2025-01-25 22:56:20,794 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-25 22:56:20,956 Loss: 1.272, MeanIU:  0.1820, Best_mIoU:  0.1988
2025-01-25 22:56:20,956 [0.         0.44236543 0.17621464 0.13427675 0.35168867 0.1285791
 0.02740431 0.01314721]
2025-01-25 22:56:28,674 Epoch: [3/20] Iter:[0/290], Time: 7.71, lr: [0.0006911415778422553], Loss: 1.364164, Loss_D1: 0.650820, Loss_D2: 0.693103, Acc:0.470562, Semantic loss: 0.135248
2025-01-25 22:57:25,794 Epoch: [3/20] Iter:[10/290], Time: 5.89, lr: [0.0006898797308788209], Loss: 1.363065, Loss_D1: 0.656843, Loss_D2: 0.693151, Acc:0.505242, Semantic loss: 0.135969
2025-01-25 22:58:26,817 Epoch: [3/20] Iter:[20/290], Time: 5.99, lr: [0.0006886176274161748], Loss: 1.421429, Loss_D1: 0.693609, Loss_D2: 0.693182, Acc:0.520939, Semantic loss: 0.140216
2025-01-25 22:59:23,751 Epoch: [3/20] Iter:[30/290], Time: 5.90, lr: [0.000687355266879616], Loss: 1.450990, Loss_D1: 0.648137, Loss_D2: 0.693157, Acc:0.515564, Semantic loss: 0.145358
