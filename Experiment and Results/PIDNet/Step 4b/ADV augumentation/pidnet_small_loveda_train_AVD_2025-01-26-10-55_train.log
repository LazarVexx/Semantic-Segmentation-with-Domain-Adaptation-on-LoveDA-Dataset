2025-01-26 10:55:18,531 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-26 10:55:18,532 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa-Rural/train.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: True
  AUG: True
  AUG1: False
  AUG2: True
  AUG3: True
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  D1: False
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.0002
  LR: 0.001
  LR_D1: 0.0001
  LR_D2: 0.0001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: True
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-26 10:55:18,789 Attention!!!
2025-01-26 10:55:18,789 Loaded 302 parameters!
2025-01-26 10:55:18,789 Over!!!
2025-01-26 10:55:19,249 => loaded checkpoint (epoch 15)
2025-01-26 10:55:25,579 Epoch: [15/20] Iter:[0/286], Time: 6.31, lr: [7.510636108781823e-08], Loss: 1.461188, Loss_D1: 0.693116, Loss_D2: 0.693190, Acc:0.613618, Semantic loss: 0.131997
2025-01-26 10:55:44,953 Epoch: [15/20] Iter:[10/286], Time: 2.34, lr: [7.463349800618467e-08], Loss: 1.355066, Loss_D1: 0.706345, Loss_D2: 0.693380, Acc:0.566714, Semantic loss: 0.127353
2025-01-26 10:56:04,211 Epoch: [15/20] Iter:[20/286], Time: 2.14, lr: [7.416030180220696e-08], Loss: 1.284064, Loss_D1: 0.695741, Loss_D2: 0.692044, Acc:0.570515, Semantic loss: 0.122573
2025-01-26 10:56:23,474 Epoch: [15/20] Iter:[30/286], Time: 2.07, lr: [7.368676987609311e-08], Loss: 1.290545, Loss_D1: 0.693070, Loss_D2: 0.692793, Acc:0.580435, Semantic loss: 0.120693
2025-01-26 10:56:41,871 Epoch: [15/20] Iter:[40/286], Time: 2.02, lr: [7.321289958903922e-08], Loss: 1.312960, Loss_D1: 0.675335, Loss_D2: 0.692857, Acc:0.576145, Semantic loss: 0.122654
2025-01-26 10:57:00,459 Epoch: [15/20] Iter:[50/286], Time: 1.98, lr: [7.273868826235933e-08], Loss: 1.299185, Loss_D1: 0.674285, Loss_D2: 0.691926, Acc:0.575973, Semantic loss: 0.123182
2025-01-26 10:57:19,317 Epoch: [15/20] Iter:[60/286], Time: 1.97, lr: [7.2264133176589e-08], Loss: 1.292091, Loss_D1: 0.685191, Loss_D2: 0.693500, Acc:0.568978, Semantic loss: 0.123527
2025-01-26 10:57:37,866 Epoch: [15/20] Iter:[70/286], Time: 1.95, lr: [7.178923157056236e-08], Loss: 1.291475, Loss_D1: 0.670901, Loss_D2: 0.692907, Acc:0.571971, Semantic loss: 0.123341
2025-01-26 10:57:56,107 Epoch: [15/20] Iter:[80/286], Time: 1.94, lr: [7.131398064046089e-08], Loss: 1.280582, Loss_D1: 0.654505, Loss_D2: 0.693159, Acc:0.574292, Semantic loss: 0.122480
2025-01-26 10:58:14,234 Epoch: [15/20] Iter:[90/286], Time: 1.92, lr: [7.083837753883395e-08], Loss: 1.284642, Loss_D1: 0.670009, Loss_D2: 0.693481, Acc:0.577503, Semantic loss: 0.121789
2025-01-26 10:58:32,434 Epoch: [15/20] Iter:[100/286], Time: 1.91, lr: [7.03624193735888e-08], Loss: 1.292807, Loss_D1: 0.669153, Loss_D2: 0.691951, Acc:0.574372, Semantic loss: 0.123048
2025-01-26 10:58:50,408 Epoch: [15/20] Iter:[110/286], Time: 1.90, lr: [6.988610320694955e-08], Loss: 1.282904, Loss_D1: 0.703624, Loss_D2: 0.693754, Acc:0.575185, Semantic loss: 0.122647
2025-01-26 10:59:08,718 Epoch: [15/20] Iter:[120/286], Time: 1.90, lr: [6.940942605438407e-08], Loss: 1.274423, Loss_D1: 0.643290, Loss_D2: 0.693894, Acc:0.575696, Semantic loss: 0.122384
2025-01-26 10:59:27,101 Epoch: [15/20] Iter:[130/286], Time: 1.89, lr: [6.893238488349682e-08], Loss: 1.287479, Loss_D1: 0.689375, Loss_D2: 0.693418, Acc:0.579685, Semantic loss: 0.122163
2025-01-26 10:59:45,385 Epoch: [15/20] Iter:[140/286], Time: 1.89, lr: [6.845497661288724e-08], Loss: 1.286418, Loss_D1: 0.686234, Loss_D2: 0.693076, Acc:0.582212, Semantic loss: 0.122042
2025-01-26 11:00:03,809 Epoch: [15/20] Iter:[150/286], Time: 1.88, lr: [6.797719811097152e-08], Loss: 1.277647, Loss_D1: 0.613278, Loss_D2: 0.693170, Acc:0.580876, Semantic loss: 0.122211
2025-01-26 11:00:22,127 Epoch: [15/20] Iter:[160/286], Time: 1.88, lr: [6.749904619476666e-08], Loss: 1.283891, Loss_D1: 0.666189, Loss_D2: 0.693287, Acc:0.580618, Semantic loss: 0.122549
2025-01-26 11:00:40,066 Epoch: [15/20] Iter:[170/286], Time: 1.88, lr: [6.702051762863538e-08], Loss: 1.282729, Loss_D1: 0.713316, Loss_D2: 0.693305, Acc:0.580715, Semantic loss: 0.122275
2025-01-26 11:00:58,053 Epoch: [15/20] Iter:[180/286], Time: 1.87, lr: [6.654160912298996e-08], Loss: 1.277945, Loss_D1: 0.683658, Loss_D2: 0.693137, Acc:0.579769, Semantic loss: 0.122585
2025-01-26 11:01:16,759 Epoch: [15/20] Iter:[190/286], Time: 1.87, lr: [6.606231733295387e-08], Loss: 1.278030, Loss_D1: 0.634855, Loss_D2: 0.693265, Acc:0.578892, Semantic loss: 0.122756
2025-01-26 11:01:34,940 Epoch: [15/20] Iter:[200/286], Time: 1.87, lr: [6.558263885697875e-08], Loss: 1.275714, Loss_D1: 0.612140, Loss_D2: 0.692664, Acc:0.579285, Semantic loss: 0.122550
2025-01-26 11:01:52,899 Epoch: [15/20] Iter:[210/286], Time: 1.87, lr: [6.510257023541565e-08], Loss: 1.271375, Loss_D1: 0.658502, Loss_D2: 0.694708, Acc:0.577926, Semantic loss: 0.122887
2025-01-26 11:02:10,688 Epoch: [15/20] Iter:[220/286], Time: 1.86, lr: [6.462210794903776e-08], Loss: 1.276882, Loss_D1: 0.705738, Loss_D2: 0.692678, Acc:0.578435, Semantic loss: 0.123085
2025-01-26 11:02:28,480 Epoch: [15/20] Iter:[230/286], Time: 1.86, lr: [6.414124841751362e-08], Loss: 1.274143, Loss_D1: 0.653651, Loss_D2: 0.693580, Acc:0.579156, Semantic loss: 0.122744
2025-01-26 11:02:46,401 Epoch: [15/20] Iter:[240/286], Time: 1.86, lr: [6.365998799782768e-08], Loss: 1.268032, Loss_D1: 0.657874, Loss_D2: 0.693173, Acc:0.578546, Semantic loss: 0.122683
2025-01-26 11:03:04,186 Epoch: [15/20] Iter:[250/286], Time: 1.85, lr: [6.317832298264668e-08], Loss: 1.269979, Loss_D1: 0.723509, Loss_D2: 0.693004, Acc:0.579113, Semantic loss: 0.122683
2025-01-26 11:03:22,252 Epoch: [15/20] Iter:[260/286], Time: 1.85, lr: [6.26962495986291e-08], Loss: 1.271785, Loss_D1: 0.607165, Loss_D2: 0.693255, Acc:0.578196, Semantic loss: 0.122713
2025-01-26 11:03:39,846 Epoch: [15/20] Iter:[270/286], Time: 1.85, lr: [6.221376400467538e-08], Loss: 1.274057, Loss_D1: 0.665436, Loss_D2: 0.693085, Acc:0.578364, Semantic loss: 0.122629
2025-01-26 11:03:57,430 Epoch: [15/20] Iter:[280/286], Time: 1.84, lr: [6.173086229011634e-08], Loss: 1.270259, Loss_D1: 0.606189, Loss_D2: 0.693223, Acc:0.577163, Semantic loss: 0.122214
2025-01-26 11:07:49,144 0 [0.         0.46308546 0.19663484 0.11578865 0.33271851 0.18197376
 0.12368651 0.02833052] 0.20603117706476418
2025-01-26 11:07:49,144 1 [0.         0.49873234 0.20467148 0.25386452 0.39887579 0.1168063
 0.0508911  0.34672787] 0.26722419886396
2025-01-26 11:07:49,145 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-26 11:07:49,314 Loss: 1.228, MeanIU:  0.2672, Best_mIoU:  0.2744
2025-01-26 11:07:49,315 [0.         0.49873234 0.20467148 0.25386452 0.39887579 0.1168063
 0.0508911  0.34672787]
2025-01-26 11:07:51,087 Epoch: [16/20] Iter:[0/286], Time: 1.77, lr: [2.1645652967558446e-08], Loss: 0.775943, Loss_D1: 0.624005, Loss_D2: 0.692964, Acc:0.522005, Semantic loss: 0.117744
2025-01-26 11:08:09,401 Epoch: [16/20] Iter:[10/286], Time: 1.83, lr: [2.1475289072907595e-08], Loss: 1.264266, Loss_D1: 0.657167, Loss_D2: 0.693081, Acc:0.578243, Semantic loss: 0.117195
2025-01-26 11:08:27,243 Epoch: [16/20] Iter:[20/286], Time: 1.81, lr: [2.1304774877239434e-08], Loss: 1.323378, Loss_D1: 0.640338, Loss_D2: 0.691521, Acc:0.586268, Semantic loss: 0.121696
2025-01-26 11:08:45,000 Epoch: [16/20] Iter:[30/286], Time: 1.80, lr: [2.1134108908943354e-08], Loss: 1.336541, Loss_D1: 0.675497, Loss_D2: 0.693942, Acc:0.580696, Semantic loss: 0.122043
2025-01-26 11:09:02,666 Epoch: [16/20] Iter:[40/286], Time: 1.79, lr: [2.096328966865387e-08], Loss: 1.311526, Loss_D1: 0.595297, Loss_D2: 0.691427, Acc:0.580431, Semantic loss: 0.121434
2025-01-26 11:09:20,279 Epoch: [16/20] Iter:[50/286], Time: 1.78, lr: [2.0792315628470908e-08], Loss: 1.320640, Loss_D1: 0.596051, Loss_D2: 0.692178, Acc:0.582478, Semantic loss: 0.122695
2025-01-26 11:09:38,083 Epoch: [16/20] Iter:[60/286], Time: 1.78, lr: [2.0621185231150814e-08], Loss: 1.304390, Loss_D1: 0.612305, Loss_D2: 0.698519, Acc:0.578014, Semantic loss: 0.122881
2025-01-26 11:09:55,844 Epoch: [16/20] Iter:[70/286], Time: 1.78, lr: [2.0449896889266726e-08], Loss: 1.303451, Loss_D1: 0.634622, Loss_D2: 0.679198, Acc:0.578332, Semantic loss: 0.123226
2025-01-26 11:10:13,393 Epoch: [16/20] Iter:[80/286], Time: 1.78, lr: [2.0278448984336983e-08], Loss: 1.308012, Loss_D1: 0.666161, Loss_D2: 0.714955, Acc:0.583000, Semantic loss: 0.121950
2025-01-26 11:10:31,274 Epoch: [16/20] Iter:[90/286], Time: 1.78, lr: [2.0106839865919798e-08], Loss: 1.299805, Loss_D1: 0.566658, Loss_D2: 0.693201, Acc:0.582584, Semantic loss: 0.121627
2025-01-26 11:10:48,952 Epoch: [16/20] Iter:[100/286], Time: 1.78, lr: [1.9935067850672775e-08], Loss: 1.295324, Loss_D1: 0.679245, Loss_D2: 0.683487, Acc:0.582547, Semantic loss: 0.121455
2025-01-26 11:11:06,589 Epoch: [16/20] Iter:[110/286], Time: 1.78, lr: [1.9763131221375427e-08], Loss: 1.275499, Loss_D1: 0.595553, Loss_D2: 0.705069, Acc:0.581467, Semantic loss: 0.120663
2025-01-26 11:11:24,394 Epoch: [16/20] Iter:[120/286], Time: 1.78, lr: [1.9591028225912983e-08], Loss: 1.278518, Loss_D1: 0.642336, Loss_D2: 0.691686, Acc:0.581210, Semantic loss: 0.120857
2025-01-26 11:11:42,059 Epoch: [16/20] Iter:[130/286], Time: 1.78, lr: [1.9418757076219398e-08], Loss: 1.273423, Loss_D1: 0.615337, Loss_D2: 0.699735, Acc:0.578664, Semantic loss: 0.121309
2025-01-26 11:11:59,754 Epoch: [16/20] Iter:[140/286], Time: 1.78, lr: [1.9246315947177653e-08], Loss: 1.272516, Loss_D1: 0.714025, Loss_D2: 0.690662, Acc:0.576908, Semantic loss: 0.122117
2025-01-26 11:12:17,615 Epoch: [16/20] Iter:[150/286], Time: 1.78, lr: [1.9073702975475114e-08], Loss: 1.257922, Loss_D1: 0.663632, Loss_D2: 0.692019, Acc:0.571039, Semantic loss: 0.122462
2025-01-26 11:12:36,023 Epoch: [16/20] Iter:[160/286], Time: 1.78, lr: [1.8900916258411612e-08], Loss: 1.255302, Loss_D1: 0.630917, Loss_D2: 0.698771, Acc:0.572318, Semantic loss: 0.122454
2025-01-26 11:12:54,172 Epoch: [16/20] Iter:[170/286], Time: 1.78, lr: [1.8727953852657913e-08], Loss: 1.257531, Loss_D1: 0.575166, Loss_D2: 0.691580, Acc:0.573851, Semantic loss: 0.122265
2025-01-26 11:13:12,195 Epoch: [16/20] Iter:[180/286], Time: 1.78, lr: [1.8554813772961815e-08], Loss: 1.251638, Loss_D1: 0.695135, Loss_D2: 0.686994, Acc:0.573344, Semantic loss: 0.122027
2025-01-26 11:13:30,359 Epoch: [16/20] Iter:[190/286], Time: 1.79, lr: [1.838149399079922e-08], Loss: 1.249398, Loss_D1: 0.625841, Loss_D2: 0.700103, Acc:0.572930, Semantic loss: 0.121860
2025-01-26 11:13:48,336 Epoch: [16/20] Iter:[200/286], Time: 1.79, lr: [1.8207992432967152e-08], Loss: 1.245686, Loss_D1: 0.665938, Loss_D2: 0.682248, Acc:0.572259, Semantic loss: 0.121528
2025-01-26 11:14:06,312 Epoch: [16/20] Iter:[210/286], Time: 1.79, lr: [1.8034306980115726e-08], Loss: 1.243820, Loss_D1: 0.604283, Loss_D2: 0.678375, Acc:0.570535, Semantic loss: 0.121814
2025-01-26 11:14:24,287 Epoch: [16/20] Iter:[220/286], Time: 1.79, lr: [1.7860435465215544e-08], Loss: 1.239157, Loss_D1: 0.682875, Loss_D2: 0.690156, Acc:0.569593, Semantic loss: 0.122037
2025-01-26 11:14:42,328 Epoch: [16/20] Iter:[230/286], Time: 1.79, lr: [1.7686375671957048e-08], Loss: 1.240942, Loss_D1: 0.746747, Loss_D2: 0.693890, Acc:0.570284, Semantic loss: 0.121992
2025-01-26 11:15:00,087 Epoch: [16/20] Iter:[240/286], Time: 1.79, lr: [1.7512125333078103e-08], Loss: 1.233901, Loss_D1: 0.664856, Loss_D2: 0.693090, Acc:0.570435, Semantic loss: 0.121714
2025-01-26 11:15:18,035 Epoch: [16/20] Iter:[250/286], Time: 1.79, lr: [1.7337682128615548e-08], Loss: 1.236032, Loss_D1: 0.611137, Loss_D2: 0.697962, Acc:0.569904, Semantic loss: 0.121804
2025-01-26 11:15:35,854 Epoch: [16/20] Iter:[260/286], Time: 1.79, lr: [1.7163043684076676e-08], Loss: 1.238230, Loss_D1: 0.808815, Loss_D2: 0.694160, Acc:0.568702, Semantic loss: 0.122307
2025-01-26 11:15:53,765 Epoch: [16/20] Iter:[270/286], Time: 1.79, lr: [1.6988207568525695e-08], Loss: 1.242187, Loss_D1: 0.605489, Loss_D2: 0.693434, Acc:0.568437, Semantic loss: 0.122078
2025-01-26 11:16:11,583 Epoch: [16/20] Iter:[280/286], Time: 1.79, lr: [1.681317129258046e-08], Loss: 1.245538, Loss_D1: 0.653158, Loss_D2: 0.693292, Acc:0.569875, Semantic loss: 0.121892
2025-01-26 11:19:44,238 0 [0.         0.47184158 0.23527447 0.12478248 0.32255417 0.18114671
 0.12508762 0.02630148] 0.21242693057552434
2025-01-26 11:19:44,239 1 [0.         0.51448413 0.26505924 0.26786348 0.3807977  0.13057635
 0.05847968 0.32174853] 0.2770013013461085
2025-01-26 11:19:44,239 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-26 11:19:44,475 Loss: 1.237, MeanIU:  0.2770, Best_mIoU:  0.2770
2025-01-26 11:19:44,475 [0.         0.51448413 0.26505924 0.26786348 0.3807977  0.13057635
 0.05847968 0.32174853]
2025-01-26 11:19:46,272 Epoch: [17/20] Iter:[0/286], Time: 1.79, lr: [1.277279474869787e-08], Loss: 0.627867, Loss_D1: 0.633597, Loss_D2: 0.693124, Acc:0.365327, Semantic loss: 0.133748
2025-01-26 11:20:03,449 Epoch: [17/20] Iter:[10/286], Time: 1.72, lr: [1.2638735971346062e-08], Loss: 1.156036, Loss_D1: 0.581541, Loss_D2: 0.692861, Acc:0.555617, Semantic loss: 0.121168
2025-01-26 11:20:20,785 Epoch: [17/20] Iter:[20/286], Time: 1.73, lr: [1.2504519008712421e-08], Loss: 1.189189, Loss_D1: 0.670808, Loss_D2: 0.691840, Acc:0.560892, Semantic loss: 0.124087
2025-01-26 11:20:38,214 Epoch: [17/20] Iter:[30/286], Time: 1.73, lr: [1.2370141783041654e-08], Loss: 1.267920, Loss_D1: 0.601676, Loss_D2: 0.692681, Acc:0.585613, Semantic loss: 0.123241
2025-01-26 11:20:55,457 Epoch: [17/20] Iter:[40/286], Time: 1.73, lr: [1.2235602163846212e-08], Loss: 1.275829, Loss_D1: 0.564379, Loss_D2: 0.695257, Acc:0.591827, Semantic loss: 0.120596
2025-01-26 11:21:12,727 Epoch: [17/20] Iter:[50/286], Time: 1.73, lr: [1.2100897965906423e-08], Loss: 1.290118, Loss_D1: 0.627714, Loss_D2: 0.693965, Acc:0.591814, Semantic loss: 0.122387
2025-01-26 11:21:29,916 Epoch: [17/20] Iter:[60/286], Time: 1.73, lr: [1.1966026947169183e-08], Loss: 1.240742, Loss_D1: 0.634397, Loss_D2: 0.693378, Acc:0.580331, Semantic loss: 0.121147
2025-01-26 11:21:46,991 Epoch: [17/20] Iter:[70/286], Time: 1.73, lr: [1.1830986806538491e-08], Loss: 1.247987, Loss_D1: 0.616404, Loss_D2: 0.693014, Acc:0.581856, Semantic loss: 0.120426
2025-01-26 11:22:04,074 Epoch: [17/20] Iter:[80/286], Time: 1.72, lr: [1.1695775181550996e-08], Loss: 1.260663, Loss_D1: 0.583274, Loss_D2: 0.692830, Acc:0.579489, Semantic loss: 0.120923
2025-01-26 11:22:21,233 Epoch: [17/20] Iter:[90/286], Time: 1.72, lr: [1.156038964592891e-08], Loss: 1.260001, Loss_D1: 0.592029, Loss_D2: 0.692599, Acc:0.577544, Semantic loss: 0.120683
2025-01-26 11:22:38,414 Epoch: [17/20] Iter:[100/286], Time: 1.72, lr: [1.1424827707002264e-08], Loss: 1.280644, Loss_D1: 0.596545, Loss_D2: 0.692700, Acc:0.580007, Semantic loss: 0.120418
2025-01-26 11:22:55,790 Epoch: [17/20] Iter:[110/286], Time: 1.72, lr: [1.1289086802991519e-08], Loss: 1.270910, Loss_D1: 0.716458, Loss_D2: 0.691173, Acc:0.579129, Semantic loss: 0.120021
2025-01-26 11:23:12,990 Epoch: [17/20] Iter:[120/286], Time: 1.72, lr: [1.115316430014118e-08], Loss: 1.267942, Loss_D1: 0.626953, Loss_D2: 0.693566, Acc:0.574650, Semantic loss: 0.119911
2025-01-26 11:23:30,224 Epoch: [17/20] Iter:[130/286], Time: 1.72, lr: [1.1017057489694008e-08], Loss: 1.268890, Loss_D1: 0.540533, Loss_D2: 0.693337, Acc:0.574506, Semantic loss: 0.119589
2025-01-26 11:23:47,371 Epoch: [17/20] Iter:[140/286], Time: 1.72, lr: [1.0880763584694663e-08], Loss: 1.253338, Loss_D1: 0.602704, Loss_D2: 0.693619, Acc:0.570478, Semantic loss: 0.119925
2025-01-26 11:24:04,565 Epoch: [17/20] Iter:[150/286], Time: 1.72, lr: [1.0744279716610738e-08], Loss: 1.261302, Loss_D1: 0.678904, Loss_D2: 0.693261, Acc:0.569393, Semantic loss: 0.120752
2025-01-26 11:24:21,973 Epoch: [17/20] Iter:[160/286], Time: 1.72, lr: [1.0607602931757866e-08], Loss: 1.262146, Loss_D1: 0.644285, Loss_D2: 0.693177, Acc:0.570008, Semantic loss: 0.121032
2025-01-26 11:24:39,299 Epoch: [17/20] Iter:[170/286], Time: 1.72, lr: [1.0470730187514713e-08], Loss: 1.267675, Loss_D1: 0.540083, Loss_D2: 0.693459, Acc:0.573142, Semantic loss: 0.120746
2025-01-26 11:24:56,516 Epoch: [17/20] Iter:[180/286], Time: 1.72, lr: [1.0333658348312189e-08], Loss: 1.266425, Loss_D1: 0.619903, Loss_D2: 0.693171, Acc:0.573547, Semantic loss: 0.121084
2025-01-26 11:25:13,918 Epoch: [17/20] Iter:[190/286], Time: 1.72, lr: [1.0196384181379869e-08], Loss: 1.266021, Loss_D1: 0.567971, Loss_D2: 0.693048, Acc:0.576077, Semantic loss: 0.121870
2025-01-26 11:25:31,194 Epoch: [17/20] Iter:[200/286], Time: 1.72, lr: [1.0058904352231183e-08], Loss: 1.270520, Loss_D1: 0.662360, Loss_D2: 0.693022, Acc:0.575190, Semantic loss: 0.121936
2025-01-26 11:25:48,522 Epoch: [17/20] Iter:[210/286], Time: 1.73, lr: [9.921215419866939e-09], Loss: 1.270258, Loss_D1: 0.569389, Loss_D2: 0.693752, Acc:0.576106, Semantic loss: 0.122094
2025-01-26 11:26:05,815 Epoch: [17/20] Iter:[220/286], Time: 1.73, lr: [9.78331383167511e-09], Loss: 1.275841, Loss_D1: 0.602765, Loss_D2: 0.693216, Acc:0.577142, Semantic loss: 0.121980
2025-01-26 11:26:23,163 Epoch: [17/20] Iter:[230/286], Time: 1.73, lr: [9.645195918002522e-09], Loss: 1.270593, Loss_D1: 0.634461, Loss_D2: 0.693024, Acc:0.573433, Semantic loss: 0.122681
2025-01-26 11:26:40,267 Epoch: [17/20] Iter:[240/286], Time: 1.73, lr: [9.506857886371847e-09], Loss: 1.270240, Loss_D1: 0.681862, Loss_D2: 0.693151, Acc:0.574838, Semantic loss: 0.122422
2025-01-26 11:26:57,540 Epoch: [17/20] Iter:[250/286], Time: 1.73, lr: [9.36829581531449e-09], Loss: 1.273044, Loss_D1: 0.613961, Loss_D2: 0.693155, Acc:0.576290, Semantic loss: 0.122159
2025-01-26 11:27:14,798 Epoch: [17/20] Iter:[260/286], Time: 1.73, lr: [9.229505647787175e-09], Loss: 1.273664, Loss_D1: 0.557468, Loss_D2: 0.693137, Acc:0.577104, Semantic loss: 0.122282
2025-01-26 11:27:32,217 Epoch: [17/20] Iter:[270/286], Time: 1.73, lr: [9.090483184136731e-09], Loss: 1.273243, Loss_D1: 0.566891, Loss_D2: 0.693199, Acc:0.576554, Semantic loss: 0.122253
2025-01-26 11:27:49,444 Epoch: [17/20] Iter:[280/286], Time: 1.73, lr: [8.951224074573704e-09], Loss: 1.271742, Loss_D1: 0.647501, Loss_D2: 0.693281, Acc:0.575746, Semantic loss: 0.122641
2025-01-26 11:31:17,772 0 [0.         0.45789369 0.18947345 0.11559765 0.30812278 0.17627685
 0.10101928 0.02454464] 0.19613261993133208
2025-01-26 11:31:17,773 1 [0.         0.49027812 0.17863358 0.25130237 0.35263097 0.12729837
 0.02985104 0.30669317] 0.24809823154333596
2025-01-26 11:31:17,773 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-26 11:31:17,939 Loss: 1.265, MeanIU:  0.2481, Best_mIoU:  0.2770
2025-01-26 11:31:17,939 [0.         0.49027812 0.17863358 0.25130237 0.35263097 0.12729837
 0.02985104 0.30669317]
2025-01-26 11:31:19,762 Epoch: [18/20] Iter:[0/286], Time: 1.82, lr: [1.0766583814002225e-08], Loss: 1.239760, Loss_D1: 0.556260, Loss_D2: 0.693226, Acc:0.545238, Semantic loss: 0.124872
2025-01-26 11:31:36,844 Epoch: [18/20] Iter:[10/286], Time: 1.72, lr: [1.0597030484505657e-08], Loss: 1.294511, Loss_D1: 0.609312, Loss_D2: 0.693217, Acc:0.583451, Semantic loss: 0.119031
2025-01-26 11:31:54,243 Epoch: [18/20] Iter:[20/286], Time: 1.73, lr: [1.042717517337281e-08], Loss: 1.261619, Loss_D1: 0.744449, Loss_D2: 0.692891, Acc:0.593374, Semantic loss: 0.116318
2025-01-26 11:32:11,577 Epoch: [18/20] Iter:[30/286], Time: 1.73, lr: [1.0257011856745716e-08], Loss: 1.273224, Loss_D1: 0.558833, Loss_D2: 0.693036, Acc:0.592579, Semantic loss: 0.118630
2025-01-26 11:32:28,739 Epoch: [18/20] Iter:[40/286], Time: 1.73, lr: [1.0086534277115837e-08], Loss: 1.280769, Loss_D1: 0.630829, Loss_D2: 0.693418, Acc:0.590859, Semantic loss: 0.118182
2025-01-26 11:32:46,141 Epoch: [18/20] Iter:[50/286], Time: 1.73, lr: [9.915735929693198e-09], Loss: 1.313561, Loss_D1: 0.611620, Loss_D2: 0.692823, Acc:0.590865, Semantic loss: 0.117717
2025-01-26 11:33:03,355 Epoch: [18/20] Iter:[60/286], Time: 1.73, lr: [9.744610047703614e-09], Loss: 1.296292, Loss_D1: 0.603091, Loss_D2: 0.692718, Acc:0.589275, Semantic loss: 0.118649
2025-01-26 11:33:20,535 Epoch: [18/20] Iter:[70/286], Time: 1.73, lr: [9.573149586506989e-09], Loss: 1.274692, Loss_D1: 0.678939, Loss_D2: 0.692995, Acc:0.588377, Semantic loss: 0.117775
2025-01-26 11:33:37,816 Epoch: [18/20] Iter:[80/286], Time: 1.73, lr: [9.401347206416929e-09], Loss: 1.268652, Loss_D1: 0.624536, Loss_D2: 0.692412, Acc:0.588405, Semantic loss: 0.119645
2025-01-26 11:33:55,170 Epoch: [18/20] Iter:[90/286], Time: 1.73, lr: [9.229195254086611e-09], Loss: 1.271010, Loss_D1: 0.637741, Loss_D2: 0.692233, Acc:0.590394, Semantic loss: 0.119933
2025-01-26 11:34:12,270 Epoch: [18/20] Iter:[100/286], Time: 1.73, lr: [9.05668574230918e-09], Loss: 1.268040, Loss_D1: 0.595872, Loss_D2: 0.692840, Acc:0.584673, Semantic loss: 0.120924
2025-01-26 11:34:29,581 Epoch: [18/20] Iter:[110/286], Time: 1.73, lr: [8.883810328061051e-09], Loss: 1.262060, Loss_D1: 0.588541, Loss_D2: 0.694040, Acc:0.581980, Semantic loss: 0.121384
2025-01-26 11:34:46,896 Epoch: [18/20] Iter:[120/286], Time: 1.73, lr: [8.710560288593795e-09], Loss: 1.269022, Loss_D1: 0.632506, Loss_D2: 0.707825, Acc:0.580614, Semantic loss: 0.121517
2025-01-26 11:35:04,123 Epoch: [18/20] Iter:[130/286], Time: 1.73, lr: [8.53692649535425e-09], Loss: 1.257687, Loss_D1: 0.569943, Loss_D2: 0.693947, Acc:0.578292, Semantic loss: 0.120821
2025-01-26 11:35:21,458 Epoch: [18/20] Iter:[140/286], Time: 1.73, lr: [8.362899385481651e-09], Loss: 1.263755, Loss_D1: 0.567814, Loss_D2: 0.698379, Acc:0.580009, Semantic loss: 0.120648
2025-01-26 11:35:38,500 Epoch: [18/20] Iter:[150/286], Time: 1.73, lr: [8.18846893059543e-09], Loss: 1.260922, Loss_D1: 0.616110, Loss_D2: 0.690877, Acc:0.582942, Semantic loss: 0.120394
2025-01-26 11:35:55,729 Epoch: [18/20] Iter:[160/286], Time: 1.73, lr: [8.01362460254561e-09], Loss: 1.270520, Loss_D1: 0.516966, Loss_D2: 0.699345, Acc:0.583791, Semantic loss: 0.120375
2025-01-26 11:36:12,955 Epoch: [18/20] Iter:[170/286], Time: 1.73, lr: [7.838355335749146e-09], Loss: 1.273853, Loss_D1: 0.581858, Loss_D2: 0.704428, Acc:0.582157, Semantic loss: 0.120990
2025-01-26 11:36:30,113 Epoch: [18/20] Iter:[180/286], Time: 1.72, lr: [7.662649485678433e-09], Loss: 1.272701, Loss_D1: 0.650691, Loss_D2: 0.691102, Acc:0.584369, Semantic loss: 0.120895
2025-01-26 11:36:47,219 Epoch: [18/20] Iter:[190/286], Time: 1.72, lr: [7.486494783000183e-09], Loss: 1.270601, Loss_D1: 0.674172, Loss_D2: 0.692621, Acc:0.583355, Semantic loss: 0.121284
2025-01-26 11:37:04,499 Epoch: [18/20] Iter:[200/286], Time: 1.72, lr: [7.3098782827831015e-09], Loss: 1.268912, Loss_D1: 0.546556, Loss_D2: 0.690833, Acc:0.582440, Semantic loss: 0.121252
2025-01-26 11:37:21,764 Epoch: [18/20] Iter:[210/286], Time: 1.72, lr: [7.13278630809672e-09], Loss: 1.265901, Loss_D1: 0.559400, Loss_D2: 0.700699, Acc:0.582472, Semantic loss: 0.121317
2025-01-26 11:37:39,084 Epoch: [18/20] Iter:[220/286], Time: 1.72, lr: [6.955204387209813e-09], Loss: 1.265177, Loss_D1: 0.633442, Loss_D2: 0.697164, Acc:0.582543, Semantic loss: 0.121395
2025-01-26 11:37:56,534 Epoch: [18/20] Iter:[230/286], Time: 1.73, lr: [6.77711718345879e-09], Loss: 1.257666, Loss_D1: 0.546505, Loss_D2: 0.690102, Acc:0.580571, Semantic loss: 0.121750
2025-01-26 11:38:13,613 Epoch: [18/20] Iter:[240/286], Time: 1.72, lr: [6.5985084166907335e-09], Loss: 1.259208, Loss_D1: 0.604358, Loss_D2: 0.698255, Acc:0.580336, Semantic loss: 0.121688
2025-01-26 11:38:31,013 Epoch: [18/20] Iter:[250/286], Time: 1.73, lr: [6.419360774983753e-09], Loss: 1.259411, Loss_D1: 0.548733, Loss_D2: 0.690508, Acc:0.580472, Semantic loss: 0.121657
2025-01-26 11:38:48,107 Epoch: [18/20] Iter:[260/286], Time: 1.72, lr: [6.239655815100931e-09], Loss: 1.255011, Loss_D1: 0.560473, Loss_D2: 0.689054, Acc:0.579324, Semantic loss: 0.121493
2025-01-26 11:39:05,470 Epoch: [18/20] Iter:[270/286], Time: 1.73, lr: [6.059373849831761e-09], Loss: 1.259862, Loss_D1: 0.641204, Loss_D2: 0.688274, Acc:0.579244, Semantic loss: 0.121813
2025-01-26 11:39:22,836 Epoch: [18/20] Iter:[280/286], Time: 1.73, lr: [5.878493820000612e-09], Loss: 1.257746, Loss_D1: 0.657639, Loss_D2: 0.693425, Acc:0.579346, Semantic loss: 0.121337
2025-01-26 11:42:51,804 0 [0.         0.46592577 0.21504214 0.12427288 0.29914127 0.16406236
 0.11938595 0.02703459] 0.2021235648719546
2025-01-26 11:42:51,804 1 [0.         0.5053747  0.23097749 0.26452728 0.36894443 0.1267783
 0.05446633 0.30943722] 0.2657865374214475
2025-01-26 11:42:51,805 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-26 11:42:51,971 Loss: 1.235, MeanIU:  0.2658, Best_mIoU:  0.2770
2025-01-26 11:42:51,971 [0.         0.5053747  0.23097749 0.26452728 0.36894443 0.1267783
 0.05446633 0.30943722]
2025-01-26 11:42:53,854 Epoch: [19/20] Iter:[0/286], Time: 1.88, lr: [7.186949295008714e-09], Loss: 0.731666, Loss_D1: 0.662294, Loss_D2: 0.696426, Acc:0.461109, Semantic loss: 0.102430
2025-01-26 11:43:11,100 Epoch: [19/20] Iter:[10/286], Time: 1.74, lr: [6.960386001337131e-09], Loss: 1.169735, Loss_D1: 0.657123, Loss_D2: 0.695285, Acc:0.607420, Semantic loss: 0.117196
2025-01-26 11:43:28,430 Epoch: [19/20] Iter:[20/286], Time: 1.74, lr: [6.733000147894809e-09], Loss: 1.212332, Loss_D1: 0.610254, Loss_D2: 0.694417, Acc:0.581969, Semantic loss: 0.118266
2025-01-26 11:43:45,651 Epoch: [19/20] Iter:[30/286], Time: 1.73, lr: [6.50475763922416e-09], Loss: 1.268896, Loss_D1: 0.566191, Loss_D2: 0.692248, Acc:0.589669, Semantic loss: 0.118943
2025-01-26 11:44:02,854 Epoch: [19/20] Iter:[40/286], Time: 1.73, lr: [6.2756215754729454e-09], Loss: 1.286716, Loss_D1: 0.596937, Loss_D2: 0.693655, Acc:0.582593, Semantic loss: 0.119329
2025-01-26 11:44:20,032 Epoch: [19/20] Iter:[50/286], Time: 1.73, lr: [6.045551897915221e-09], Loss: 1.295124, Loss_D1: 0.531011, Loss_D2: 0.694155, Acc:0.586388, Semantic loss: 0.120763
2025-01-26 11:44:37,355 Epoch: [19/20] Iter:[60/286], Time: 1.73, lr: [5.8145049726751775e-09], Loss: 1.310665, Loss_D1: 0.611699, Loss_D2: 0.692876, Acc:0.584866, Semantic loss: 0.122272
2025-01-26 11:44:54,550 Epoch: [19/20] Iter:[70/286], Time: 1.73, lr: [5.582433098653783e-09], Loss: 1.341244, Loss_D1: 0.583886, Loss_D2: 0.693657, Acc:0.587636, Semantic loss: 0.122460
2025-01-26 11:45:11,894 Epoch: [19/20] Iter:[80/286], Time: 1.73, lr: [5.3492839216862495e-09], Loss: 1.322250, Loss_D1: 0.578657, Loss_D2: 0.696065, Acc:0.582421, Semantic loss: 0.123232
2025-01-26 11:45:28,901 Epoch: [19/20] Iter:[90/286], Time: 1.72, lr: [5.114999731611632e-09], Loss: 1.303945, Loss_D1: 0.758277, Loss_D2: 0.692204, Acc:0.577077, Semantic loss: 0.123223
2025-01-26 11:45:46,047 Epoch: [19/20] Iter:[100/286], Time: 1.72, lr: [4.879516611640861e-09], Loss: 1.282910, Loss_D1: 0.585783, Loss_D2: 0.693780, Acc:0.573185, Semantic loss: 0.122838
2025-01-26 11:46:03,167 Epoch: [19/20] Iter:[110/286], Time: 1.72, lr: [4.642763399314277e-09], Loss: 1.275175, Loss_D1: 0.594256, Loss_D2: 0.692414, Acc:0.572837, Semantic loss: 0.121988
2025-01-26 11:46:20,497 Epoch: [19/20] Iter:[120/286], Time: 1.72, lr: [4.404660404145041e-09], Loss: 1.276583, Loss_D1: 0.650814, Loss_D2: 0.695174, Acc:0.574296, Semantic loss: 0.121539
2025-01-26 11:46:37,616 Epoch: [19/20] Iter:[130/286], Time: 1.72, lr: [4.165117806737883e-09], Loss: 1.267637, Loss_D1: 0.695813, Loss_D2: 0.694403, Acc:0.571701, Semantic loss: 0.121867
2025-01-26 11:46:54,749 Epoch: [19/20] Iter:[140/286], Time: 1.72, lr: [3.924033634551874e-09], Loss: 1.266221, Loss_D1: 0.712643, Loss_D2: 0.692704, Acc:0.573256, Semantic loss: 0.121672
2025-01-26 11:47:11,993 Epoch: [19/20] Iter:[150/286], Time: 1.72, lr: [3.6812911653295962e-09], Loss: 1.264765, Loss_D1: 0.603135, Loss_D2: 0.692347, Acc:0.575006, Semantic loss: 0.121633
2025-01-26 11:47:29,339 Epoch: [19/20] Iter:[160/286], Time: 1.72, lr: [3.4367555418058226e-09], Loss: 1.267157, Loss_D1: 0.583884, Loss_D2: 0.693518, Acc:0.578068, Semantic loss: 0.121354
2025-01-26 11:47:46,641 Epoch: [19/20] Iter:[170/286], Time: 1.72, lr: [3.1902692755138154e-09], Loss: 1.275418, Loss_D1: 0.503831, Loss_D2: 0.693526, Acc:0.580633, Semantic loss: 0.121352
2025-01-26 11:48:03,711 Epoch: [19/20] Iter:[180/286], Time: 1.72, lr: [2.9416461461860407e-09], Loss: 1.271182, Loss_D1: 0.685869, Loss_D2: 0.692696, Acc:0.581076, Semantic loss: 0.121147
2025-01-26 11:48:20,940 Epoch: [19/20] Iter:[190/286], Time: 1.72, lr: [2.690662715595439e-09], Loss: 1.274568, Loss_D1: 0.547280, Loss_D2: 0.693230, Acc:0.579743, Semantic loss: 0.121667
2025-01-26 11:48:38,228 Epoch: [19/20] Iter:[200/286], Time: 1.72, lr: [2.4370461708898272e-09], Loss: 1.285498, Loss_D1: 0.657119, Loss_D2: 0.693393, Acc:0.580064, Semantic loss: 0.122006
2025-01-26 11:48:55,335 Epoch: [19/20] Iter:[210/286], Time: 1.72, lr: [2.180456284863395e-09], Loss: 1.294541, Loss_D1: 0.613172, Loss_D2: 0.693295, Acc:0.580709, Semantic loss: 0.121796
2025-01-26 11:49:12,486 Epoch: [19/20] Iter:[220/286], Time: 1.72, lr: [1.9204574664761774e-09], Loss: 1.289928, Loss_D1: 0.619956, Loss_D2: 0.693020, Acc:0.579963, Semantic loss: 0.121350
2025-01-26 11:49:29,795 Epoch: [19/20] Iter:[230/286], Time: 1.72, lr: [1.6564730531536372e-09], Loss: 1.285129, Loss_D1: 0.655620, Loss_D2: 0.693253, Acc:0.579892, Semantic loss: 0.121519
2025-01-26 11:49:46,927 Epoch: [19/20] Iter:[240/286], Time: 1.72, lr: [1.3877051484745744e-09], Loss: 1.278849, Loss_D1: 0.673098, Loss_D2: 0.693055, Acc:0.579466, Semantic loss: 0.121321
2025-01-26 11:50:04,003 Epoch: [19/20] Iter:[250/286], Time: 1.72, lr: [1.1129801063516466e-09], Loss: 1.277972, Loss_D1: 0.599156, Loss_D2: 0.693873, Acc:0.580180, Semantic loss: 0.121373
2025-01-26 11:50:21,231 Epoch: [19/20] Iter:[260/286], Time: 1.72, lr: [8.304073112185002e-10], Loss: 1.279199, Loss_D1: 0.692054, Loss_D2: 0.692028, Acc:0.579928, Semantic loss: 0.121295
2025-01-26 11:50:38,462 Epoch: [19/20] Iter:[270/286], Time: 1.72, lr: [5.364424476635351e-10], Loss: 1.279736, Loss_D1: 0.515739, Loss_D2: 0.693898, Acc:0.580669, Semantic loss: 0.120937
2025-01-26 11:50:55,648 Epoch: [19/20] Iter:[280/286], Time: 1.72, lr: [2.2189692115312284e-10], Loss: 1.274311, Loss_D1: 0.619064, Loss_D2: 0.692455, Acc:0.579575, Semantic loss: 0.120815
2025-01-26 11:54:24,340 0 [0.         0.4548326  0.18379285 0.11518564 0.31428468 0.1633451
 0.11356706 0.01998348] 0.19499877156876938
2025-01-26 11:54:24,340 1 [0.         0.48076563 0.16348445 0.25409171 0.37249924 0.12753099
 0.04909835 0.29430779] 0.2488254509494652
2025-01-26 11:54:24,341 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-26 11:54:24,498 Loss: 1.251, MeanIU:  0.2488, Best_mIoU:  0.2770
2025-01-26 11:54:24,498 [0.         0.48076563 0.16348445 0.25409171 0.37249924 0.12753099
 0.04909835 0.29430779]
2025-01-26 11:54:24,567 Hours: 0
2025-01-26 11:54:24,567 Done
