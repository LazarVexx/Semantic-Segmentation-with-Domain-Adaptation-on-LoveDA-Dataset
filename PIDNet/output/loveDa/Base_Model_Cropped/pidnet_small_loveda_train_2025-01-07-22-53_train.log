2025-01-07 22:53:21,425 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-07 22:53:21,425 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.8
  SB_WEIGHTS: 1.0
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: False
  BASE_SIZE: 1440
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADVT1: 0.001
  LAMBDA_ADVT2: 0.001
  LR: 0.01
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-07 22:53:21,620 Attention!!!
2025-01-07 22:53:21,620 Loaded 302 parameters!
2025-01-07 22:53:21,620 Over!!!
2025-01-07 22:53:26,212 Epoch: [0/20] Iter:[0/192], Time: 2.92, lr: [0.01], Loss: 5.643767, Acc:0.156241, Semantic loss: 0.587147, BCE loss: 4.589938, SB loss: 0.466681
2025-01-07 22:53:32,488 Epoch: [0/20] Iter:[10/192], Time: 0.83, lr: [0.009976559445324192], Loss: 3.871829, Acc:0.241079, Semantic loss: 0.539469, BCE loss: 2.949057, SB loss: 0.383304
2025-01-07 22:53:38,888 Epoch: [0/20] Iter:[20/192], Time: 0.74, lr: [0.009953112769592761], Loss: 3.301002, Acc:0.269564, Semantic loss: 0.503424, BCE loss: 2.450202, SB loss: 0.347377
2025-01-07 22:53:45,166 Epoch: [0/20] Iter:[30/192], Time: 0.70, lr: [0.009929659955177281], Loss: 3.021109, Acc:0.303779, Semantic loss: 0.458953, BCE loss: 2.254836, SB loss: 0.307321
2025-01-07 22:53:51,464 Epoch: [0/20] Iter:[40/192], Time: 0.68, lr: [0.009906200984352154], Loss: 2.832753, Acc:0.328516, Semantic loss: 0.430541, BCE loss: 2.105875, SB loss: 0.296337
2025-01-07 22:53:57,757 Epoch: [0/20] Iter:[50/192], Time: 0.67, lr: [0.009882735839293803], Loss: 2.707886, Acc:0.338797, Semantic loss: 0.408537, BCE loss: 2.018180, SB loss: 0.281169
2025-01-07 22:54:03,894 Epoch: [0/20] Iter:[60/192], Time: 0.66, lr: [0.00985926450207989], Loss: 2.591892, Acc:0.351150, Semantic loss: 0.389027, BCE loss: 1.941973, SB loss: 0.260892
2025-01-07 22:54:10,101 Epoch: [0/20] Iter:[70/192], Time: 0.66, lr: [0.009835786954688485], Loss: 2.525784, Acc:0.360425, Semantic loss: 0.373465, BCE loss: 1.909697, SB loss: 0.242622
2025-01-07 22:54:16,185 Epoch: [0/20] Iter:[80/192], Time: 0.65, lr: [0.00981230317899726], Loss: 2.457468, Acc:0.362816, Semantic loss: 0.360998, BCE loss: 1.866501, SB loss: 0.229969
2025-01-07 22:54:22,356 Epoch: [0/20] Iter:[90/192], Time: 0.65, lr: [0.009788813156782662], Loss: 2.430234, Acc:0.374102, Semantic loss: 0.347457, BCE loss: 1.864921, SB loss: 0.217856
2025-01-07 22:54:28,601 Epoch: [0/20] Iter:[100/192], Time: 0.65, lr: [0.009765316869719067], Loss: 2.398584, Acc:0.379609, Semantic loss: 0.336564, BCE loss: 1.853923, SB loss: 0.208096
2025-01-07 22:54:34,763 Epoch: [0/20] Iter:[110/192], Time: 0.64, lr: [0.009741814299377942], Loss: 2.363446, Acc:0.384747, Semantic loss: 0.325026, BCE loss: 1.838939, SB loss: 0.199481
2025-01-07 22:54:40,910 Epoch: [0/20] Iter:[120/192], Time: 0.64, lr: [0.009718305427226986], Loss: 2.325493, Acc:0.390753, Semantic loss: 0.315214, BCE loss: 1.817289, SB loss: 0.192991
2025-01-07 22:54:46,989 Epoch: [0/20] Iter:[130/192], Time: 0.64, lr: [0.009694790234629266], Loss: 2.308782, Acc:0.390982, Semantic loss: 0.308865, BCE loss: 1.811336, SB loss: 0.188581
2025-01-07 22:54:53,099 Epoch: [0/20] Iter:[140/192], Time: 0.64, lr: [0.009671268702842338], Loss: 2.277563, Acc:0.398010, Semantic loss: 0.301133, BCE loss: 1.794094, SB loss: 0.182337
2025-01-07 22:54:59,330 Epoch: [0/20] Iter:[150/192], Time: 0.64, lr: [0.009647740813017376], Loss: 2.246969, Acc:0.401755, Semantic loss: 0.295403, BCE loss: 1.773506, SB loss: 0.178061
2025-01-07 22:55:05,416 Epoch: [0/20] Iter:[160/192], Time: 0.63, lr: [0.009624206546198262], Loss: 2.221783, Acc:0.404097, Semantic loss: 0.290258, BCE loss: 1.757079, SB loss: 0.174446
2025-01-07 22:55:11,622 Epoch: [0/20] Iter:[170/192], Time: 0.63, lr: [0.009600665883320689], Loss: 2.203987, Acc:0.407984, Semantic loss: 0.284594, BCE loss: 1.749033, SB loss: 0.170360
2025-01-07 22:55:17,942 Epoch: [0/20] Iter:[180/192], Time: 0.63, lr: [0.009577118805211254], Loss: 2.176170, Acc:0.414005, Semantic loss: 0.278908, BCE loss: 1.730687, SB loss: 0.166575
2025-01-07 22:55:24,152 Epoch: [0/20] Iter:[190/192], Time: 0.63, lr: [0.009553565292586523], Loss: 2.155618, Acc:0.415913, Semantic loss: 0.275153, BCE loss: 1.716319, SB loss: 0.164146
2025-01-07 22:57:38,544 0 [0.         0.23979228 0.22688883 0.06306158 0.31256091 0.0631251
 0.29397221 0.01844538] 0.15223078637083215
2025-01-07 22:57:38,544 1 [0.         0.28334823 0.16357469 0.02469754 0.23026652 0.07773448
 0.21704603 0.01076293] 0.12592880384459015
2025-01-07 22:57:38,545 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 22:57:38,717 Loss: 1.974, MeanIU:  0.1259, Best_mIoU:  0.1259
2025-01-07 22:57:38,717 [0.         0.28334823 0.16357469 0.02469754 0.23026652 0.07773448
 0.21704603 0.01076293]
2025-01-07 22:57:39,380 Epoch: [1/20] Iter:[0/192], Time: 0.57, lr: [0.009548853816214998], Loss: 1.906953, Acc:0.447922, Semantic loss: 0.217582, BCE loss: 1.592573, SB loss: 0.096799
2025-01-07 22:57:45,461 Epoch: [1/20] Iter:[10/192], Time: 0.60, lr: [0.009525292556561479], Loss: 1.781918, Acc:0.466107, Semantic loss: 0.196701, BCE loss: 1.471771, SB loss: 0.113446
2025-01-07 22:57:51,587 Epoch: [1/20] Iter:[20/192], Time: 0.61, lr: [0.00950172481957719], Loss: 1.773009, Acc:0.465421, Semantic loss: 0.194058, BCE loss: 1.464703, SB loss: 0.114248
2025-01-07 22:57:57,715 Epoch: [1/20] Iter:[30/192], Time: 0.61, lr: [0.009478150585620286], Loss: 1.767117, Acc:0.471655, Semantic loss: 0.191426, BCE loss: 1.460778, SB loss: 0.114913
2025-01-07 22:58:03,813 Epoch: [1/20] Iter:[40/192], Time: 0.61, lr: [0.009454569834934885], Loss: 1.748157, Acc:0.468704, Semantic loss: 0.187147, BCE loss: 1.450536, SB loss: 0.110474
2025-01-07 22:58:09,887 Epoch: [1/20] Iter:[50/192], Time: 0.61, lr: [0.009430982547650114], Loss: 1.771690, Acc:0.475132, Semantic loss: 0.188542, BCE loss: 1.472366, SB loss: 0.110782
2025-01-07 22:58:15,881 Epoch: [1/20] Iter:[60/192], Time: 0.61, lr: [0.009407388703779091], Loss: 1.776374, Acc:0.466626, Semantic loss: 0.191305, BCE loss: 1.471663, SB loss: 0.113406
2025-01-07 22:58:21,871 Epoch: [1/20] Iter:[70/192], Time: 0.61, lr: [0.009383788283217955], Loss: 1.752948, Acc:0.470204, Semantic loss: 0.190290, BCE loss: 1.450135, SB loss: 0.112522
2025-01-07 22:58:27,960 Epoch: [1/20] Iter:[80/192], Time: 0.61, lr: [0.00936018126574482], Loss: 1.757661, Acc:0.469461, Semantic loss: 0.191569, BCE loss: 1.451890, SB loss: 0.114202
2025-01-07 22:58:34,015 Epoch: [1/20] Iter:[90/192], Time: 0.61, lr: [0.009336567631018769], Loss: 1.762888, Acc:0.470990, Semantic loss: 0.191916, BCE loss: 1.455893, SB loss: 0.115079
2025-01-07 22:58:40,228 Epoch: [1/20] Iter:[100/192], Time: 0.61, lr: [0.009312947358578814], Loss: 1.776357, Acc:0.477585, Semantic loss: 0.190206, BCE loss: 1.472161, SB loss: 0.113990
2025-01-07 22:58:46,380 Epoch: [1/20] Iter:[110/192], Time: 0.61, lr: [0.009289320427842841], Loss: 1.783996, Acc:0.477839, Semantic loss: 0.189713, BCE loss: 1.480748, SB loss: 0.113535
2025-01-07 22:58:52,505 Epoch: [1/20] Iter:[120/192], Time: 0.61, lr: [0.009265686818106552], Loss: 1.786312, Acc:0.483415, Semantic loss: 0.187922, BCE loss: 1.485578, SB loss: 0.112812
2025-01-07 22:58:58,507 Epoch: [1/20] Iter:[130/192], Time: 0.61, lr: [0.009242046508542393], Loss: 1.788253, Acc:0.485336, Semantic loss: 0.187553, BCE loss: 1.487223, SB loss: 0.113478
2025-01-07 22:59:04,446 Epoch: [1/20] Iter:[140/192], Time: 0.61, lr: [0.009218399478198466], Loss: 1.780910, Acc:0.488044, Semantic loss: 0.186777, BCE loss: 1.480879, SB loss: 0.113254
2025-01-07 22:59:10,633 Epoch: [1/20] Iter:[150/192], Time: 0.61, lr: [0.009194745705997428], Loss: 1.779958, Acc:0.487289, Semantic loss: 0.186625, BCE loss: 1.480061, SB loss: 0.113272
2025-01-07 22:59:16,757 Epoch: [1/20] Iter:[160/192], Time: 0.61, lr: [0.00917108517073538], Loss: 1.791367, Acc:0.487780, Semantic loss: 0.185974, BCE loss: 1.492641, SB loss: 0.112751
2025-01-07 22:59:22,862 Epoch: [1/20] Iter:[170/192], Time: 0.61, lr: [0.00914741785108075], Loss: 1.799559, Acc:0.487481, Semantic loss: 0.185840, BCE loss: 1.501101, SB loss: 0.112618
2025-01-07 22:59:28,888 Epoch: [1/20] Iter:[180/192], Time: 0.61, lr: [0.00912374372557314], Loss: 1.804181, Acc:0.488660, Semantic loss: 0.184949, BCE loss: 1.507194, SB loss: 0.112038
2025-01-07 22:59:35,103 Epoch: [1/20] Iter:[190/192], Time: 0.61, lr: [0.009100062772622186], Loss: 1.799981, Acc:0.490669, Semantic loss: 0.183680, BCE loss: 1.504827, SB loss: 0.111475
2025-01-07 23:01:47,935 0 [0.         0.25616717 0.24568077 0.07620737 0.38775811 0.05710053
 0.33447859 0.01092249] 0.17103937921303364
2025-01-07 23:01:47,936 1 [0.         0.27103794 0.25093976 0.13328162 0.26817029 0.03414128
 0.2134181  0.00209465] 0.14663545423739116
2025-01-07 23:01:47,937 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:01:48,112 Loss: 1.903, MeanIU:  0.1466, Best_mIoU:  0.1466
2025-01-07 23:01:48,113 [0.         0.27103794 0.25093976 0.13328162 0.26817029 0.03414128
 0.2134181  0.00209465]
2025-01-07 23:01:48,777 Epoch: [2/20] Iter:[0/192], Time: 0.56, lr: [0.009095325760829623], Loss: 1.910602, Acc:0.485113, Semantic loss: 0.155784, BCE loss: 1.670077, SB loss: 0.084741
2025-01-07 23:01:54,799 Epoch: [2/20] Iter:[10/192], Time: 0.60, lr: [0.009071636586262652], Loss: 1.715508, Acc:0.513694, Semantic loss: 0.169455, BCE loss: 1.447251, SB loss: 0.098802
2025-01-07 23:02:00,900 Epoch: [2/20] Iter:[20/192], Time: 0.60, lr: [0.009047940536290279], Loss: 1.677117, Acc:0.501722, Semantic loss: 0.178506, BCE loss: 1.394563, SB loss: 0.104048
2025-01-07 23:02:06,997 Epoch: [2/20] Iter:[30/192], Time: 0.61, lr: [0.009024237588898336], Loss: 1.692691, Acc:0.494679, Semantic loss: 0.177373, BCE loss: 1.406619, SB loss: 0.108699
2025-01-07 23:02:13,166 Epoch: [2/20] Iter:[40/192], Time: 0.61, lr: [0.009000527721937697], Loss: 1.721928, Acc:0.498255, Semantic loss: 0.177411, BCE loss: 1.438174, SB loss: 0.106342
2025-01-07 23:02:19,164 Epoch: [2/20] Iter:[50/192], Time: 0.61, lr: [0.008976810913123051], Loss: 1.723739, Acc:0.506033, Semantic loss: 0.176131, BCE loss: 1.442980, SB loss: 0.104628
2025-01-07 23:02:25,283 Epoch: [2/20] Iter:[60/192], Time: 0.61, lr: [0.008953087140031669], Loss: 1.718500, Acc:0.506103, Semantic loss: 0.174503, BCE loss: 1.440314, SB loss: 0.103683
2025-01-07 23:02:31,384 Epoch: [2/20] Iter:[70/192], Time: 0.61, lr: [0.008929356380102142], Loss: 1.731223, Acc:0.510888, Semantic loss: 0.173244, BCE loss: 1.452690, SB loss: 0.105288
2025-01-07 23:02:37,543 Epoch: [2/20] Iter:[80/192], Time: 0.61, lr: [0.008905618610633112], Loss: 1.749154, Acc:0.516330, Semantic loss: 0.171350, BCE loss: 1.472564, SB loss: 0.105240
2025-01-07 23:02:43,723 Epoch: [2/20] Iter:[90/192], Time: 0.61, lr: [0.008881873808781991], Loss: 1.753215, Acc:0.518560, Semantic loss: 0.170394, BCE loss: 1.477438, SB loss: 0.105383
2025-01-07 23:02:49,902 Epoch: [2/20] Iter:[100/192], Time: 0.61, lr: [0.008858121951563658], Loss: 1.756390, Acc:0.522046, Semantic loss: 0.169787, BCE loss: 1.481581, SB loss: 0.105021
2025-01-07 23:02:55,945 Epoch: [2/20] Iter:[110/192], Time: 0.61, lr: [0.008834363015849136], Loss: 1.763666, Acc:0.522682, Semantic loss: 0.170100, BCE loss: 1.488257, SB loss: 0.105309
2025-01-07 23:03:02,020 Epoch: [2/20] Iter:[120/192], Time: 0.61, lr: [0.008810596978364274], Loss: 1.775596, Acc:0.525295, Semantic loss: 0.169357, BCE loss: 1.501139, SB loss: 0.105100
2025-01-07 23:03:08,153 Epoch: [2/20] Iter:[130/192], Time: 0.61, lr: [0.008786823815688379], Loss: 1.774169, Acc:0.523612, Semantic loss: 0.170750, BCE loss: 1.497194, SB loss: 0.106225
2025-01-07 23:03:14,199 Epoch: [2/20] Iter:[140/192], Time: 0.61, lr: [0.008763043504252865], Loss: 1.756148, Acc:0.520721, Semantic loss: 0.171104, BCE loss: 1.479081, SB loss: 0.105963
2025-01-07 23:03:20,468 Epoch: [2/20] Iter:[150/192], Time: 0.61, lr: [0.008739256020339866], Loss: 1.764316, Acc:0.520999, Semantic loss: 0.171398, BCE loss: 1.487008, SB loss: 0.105910
2025-01-07 23:03:26,590 Epoch: [2/20] Iter:[160/192], Time: 0.61, lr: [0.00871546134008083], Loss: 1.765507, Acc:0.521990, Semantic loss: 0.170631, BCE loss: 1.489590, SB loss: 0.105287
2025-01-07 23:03:32,666 Epoch: [2/20] Iter:[170/192], Time: 0.61, lr: [0.008691659439455107], Loss: 1.752244, Acc:0.522637, Semantic loss: 0.169300, BCE loss: 1.478350, SB loss: 0.104594
2025-01-07 23:03:38,824 Epoch: [2/20] Iter:[180/192], Time: 0.61, lr: [0.008667850294288517], Loss: 1.743106, Acc:0.522559, Semantic loss: 0.169687, BCE loss: 1.468404, SB loss: 0.105015
2025-01-07 23:03:44,990 Epoch: [2/20] Iter:[190/192], Time: 0.61, lr: [0.008644033880251888], Loss: 1.748969, Acc:0.523244, Semantic loss: 0.169573, BCE loss: 1.474728, SB loss: 0.104668
2025-01-07 23:05:57,937 0 [0.         0.26489516 0.26119104 0.14413036 0.39606353 0.07575845
 0.33086183 0.00565589] 0.18481953173910426
2025-01-07 23:05:57,938 1 [0.00000000e+00 2.89246076e-01 2.92274389e-01 2.26449752e-01
 4.04154137e-01 1.16561945e-01 2.69920181e-01 1.34654258e-04] 0.19984264174989386
2025-01-07 23:05:57,938 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:05:58,109 Loss: 1.865, MeanIU:  0.1998, Best_mIoU:  0.1998
2025-01-07 23:05:58,109 [0.00000000e+00 2.89246076e-01 2.92274389e-01 2.26449752e-01
 4.04154137e-01 1.16561945e-01 2.69920181e-01 1.34654258e-04]
2025-01-07 23:05:58,710 Epoch: [3/20] Iter:[0/192], Time: 0.51, lr: [0.008639269723028191], Loss: 2.103963, Acc:0.562471, Semantic loss: 0.141959, BCE loss: 1.856949, SB loss: 0.105055
2025-01-07 23:06:04,681 Epoch: [3/20] Iter:[10/192], Time: 0.59, lr: [0.008615444554012613], Loss: 1.787303, Acc:0.559477, Semantic loss: 0.154229, BCE loss: 1.536700, SB loss: 0.096374
2025-01-07 23:06:10,729 Epoch: [3/20] Iter:[20/192], Time: 0.60, lr: [0.008591612062049989], Loss: 1.805213, Acc:0.557011, Semantic loss: 0.159225, BCE loss: 1.545627, SB loss: 0.100361
2025-01-07 23:06:16,830 Epoch: [3/20] Iter:[30/192], Time: 0.60, lr: [0.008567772222305215], Loss: 1.760832, Acc:0.555510, Semantic loss: 0.159428, BCE loss: 1.501996, SB loss: 0.099409
2025-01-07 23:06:23,024 Epoch: [3/20] Iter:[40/192], Time: 0.61, lr: [0.008543925009781886], Loss: 1.734353, Acc:0.551640, Semantic loss: 0.159991, BCE loss: 1.475824, SB loss: 0.098537
2025-01-07 23:06:29,150 Epoch: [3/20] Iter:[50/192], Time: 0.61, lr: [0.00852007039932076], Loss: 1.784550, Acc:0.548859, Semantic loss: 0.161340, BCE loss: 1.524771, SB loss: 0.098439
2025-01-07 23:06:35,261 Epoch: [3/20] Iter:[60/192], Time: 0.61, lr: [0.00849620836559818], Loss: 1.729323, Acc:0.547298, Semantic loss: 0.161051, BCE loss: 1.469917, SB loss: 0.098355
2025-01-07 23:06:41,429 Epoch: [3/20] Iter:[70/192], Time: 0.61, lr: [0.008472338883124477], Loss: 1.731759, Acc:0.541517, Semantic loss: 0.161720, BCE loss: 1.471241, SB loss: 0.098798
2025-01-07 23:06:47,539 Epoch: [3/20] Iter:[80/192], Time: 0.61, lr: [0.008448461926242374], Loss: 1.760321, Acc:0.542109, Semantic loss: 0.162315, BCE loss: 1.498347, SB loss: 0.099658
2025-01-07 23:06:53,769 Epoch: [3/20] Iter:[90/192], Time: 0.61, lr: [0.008424577469125337], Loss: 1.760142, Acc:0.540910, Semantic loss: 0.162864, BCE loss: 1.496500, SB loss: 0.100778
2025-01-07 23:06:59,843 Epoch: [3/20] Iter:[100/192], Time: 0.61, lr: [0.008400685485775935], Loss: 1.769636, Acc:0.541793, Semantic loss: 0.162266, BCE loss: 1.506580, SB loss: 0.100789
2025-01-07 23:07:05,991 Epoch: [3/20] Iter:[110/192], Time: 0.61, lr: [0.008376785950024154], Loss: 1.793311, Acc:0.543747, Semantic loss: 0.161899, BCE loss: 1.530285, SB loss: 0.101126
2025-01-07 23:07:12,101 Epoch: [3/20] Iter:[120/192], Time: 0.61, lr: [0.00835287883552571], Loss: 1.793173, Acc:0.540925, Semantic loss: 0.162779, BCE loss: 1.528575, SB loss: 0.101819
2025-01-07 23:07:18,172 Epoch: [3/20] Iter:[130/192], Time: 0.61, lr: [0.008328964115760324], Loss: 1.791442, Acc:0.542030, Semantic loss: 0.162098, BCE loss: 1.527996, SB loss: 0.101348
2025-01-07 23:07:24,205 Epoch: [3/20] Iter:[140/192], Time: 0.61, lr: [0.008305041764029988], Loss: 1.793593, Acc:0.542586, Semantic loss: 0.161923, BCE loss: 1.530586, SB loss: 0.101083
2025-01-07 23:07:30,349 Epoch: [3/20] Iter:[150/192], Time: 0.61, lr: [0.008281111753457188], Loss: 1.783732, Acc:0.541367, Semantic loss: 0.162625, BCE loss: 1.519635, SB loss: 0.101471
2025-01-07 23:07:36,438 Epoch: [3/20] Iter:[160/192], Time: 0.61, lr: [0.008257174056983133], Loss: 1.778962, Acc:0.537921, Semantic loss: 0.163739, BCE loss: 1.512726, SB loss: 0.102498
2025-01-07 23:07:42,422 Epoch: [3/20] Iter:[170/192], Time: 0.61, lr: [0.00823322864736593], Loss: 1.777204, Acc:0.537563, Semantic loss: 0.163182, BCE loss: 1.512053, SB loss: 0.101969
2025-01-07 23:07:48,439 Epoch: [3/20] Iter:[180/192], Time: 0.61, lr: [0.008209275497178764], Loss: 1.770625, Acc:0.534569, Semantic loss: 0.163843, BCE loss: 1.504095, SB loss: 0.102686
2025-01-07 23:07:54,396 Epoch: [3/20] Iter:[190/192], Time: 0.61, lr: [0.008185314578808021], Loss: 1.764231, Acc:0.533871, Semantic loss: 0.163587, BCE loss: 1.497745, SB loss: 0.102899
2025-01-07 23:10:05,836 0 [0.         0.26863631 0.26811903 0.13252243 0.479527   0.07716887
 0.34711558 0.00201942] 0.1968885796824713
2025-01-07 23:10:05,837 1 [0.         0.29224361 0.22127813 0.20382253 0.37754638 0.0718394
 0.29262715 0.02527659] 0.1855792243480919
2025-01-07 23:10:05,838 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:10:05,944 Loss: 1.840, MeanIU:  0.1856, Best_mIoU:  0.1998
2025-01-07 23:10:05,944 [0.         0.29224361 0.22127813 0.20382253 0.37754638 0.0718394
 0.29262715 0.02527659]
2025-01-07 23:10:06,557 Epoch: [4/20] Iter:[0/192], Time: 0.52, lr: [0.008180521460508584], Loss: 1.814814, Acc:0.539299, Semantic loss: 0.160771, BCE loss: 1.558028, SB loss: 0.096015
2025-01-07 23:10:12,665 Epoch: [4/20] Iter:[10/192], Time: 0.60, lr: [0.008156551183601795], Loss: 1.567887, Acc:0.565249, Semantic loss: 0.140911, BCE loss: 1.329838, SB loss: 0.097138
2025-01-07 23:10:18,759 Epoch: [4/20] Iter:[20/192], Time: 0.61, lr: [0.008132573077094668], Loss: 1.678648, Acc:0.562865, Semantic loss: 0.151045, BCE loss: 1.430268, SB loss: 0.097334
2025-01-07 23:10:24,802 Epoch: [4/20] Iter:[30/192], Time: 0.61, lr: [0.008108587112763079], Loss: 1.703081, Acc:0.547974, Semantic loss: 0.156017, BCE loss: 1.444945, SB loss: 0.102120
2025-01-07 23:10:30,888 Epoch: [4/20] Iter:[40/192], Time: 0.61, lr: [0.008084593262188028], Loss: 1.698305, Acc:0.544964, Semantic loss: 0.157369, BCE loss: 1.438775, SB loss: 0.102160
2025-01-07 23:10:37,006 Epoch: [4/20] Iter:[50/192], Time: 0.61, lr: [0.008060591496753653], Loss: 1.709417, Acc:0.545205, Semantic loss: 0.158470, BCE loss: 1.446096, SB loss: 0.104851
2025-01-07 23:10:43,072 Epoch: [4/20] Iter:[60/192], Time: 0.61, lr: [0.008036581787645204], Loss: 1.728414, Acc:0.546078, Semantic loss: 0.157911, BCE loss: 1.466547, SB loss: 0.103956
2025-01-07 23:10:49,189 Epoch: [4/20] Iter:[70/192], Time: 0.61, lr: [0.008012564105846994], Loss: 1.741329, Acc:0.538503, Semantic loss: 0.159648, BCE loss: 1.476957, SB loss: 0.104724
2025-01-07 23:10:55,249 Epoch: [4/20] Iter:[80/192], Time: 0.61, lr: [0.007988538422140333], Loss: 1.721330, Acc:0.541375, Semantic loss: 0.159171, BCE loss: 1.458667, SB loss: 0.103492
2025-01-07 23:11:01,349 Epoch: [4/20] Iter:[90/192], Time: 0.61, lr: [0.007964504707101411], Loss: 1.718584, Acc:0.542709, Semantic loss: 0.158174, BCE loss: 1.458024, SB loss: 0.102386
2025-01-07 23:11:07,545 Epoch: [4/20] Iter:[100/192], Time: 0.61, lr: [0.007940462931099176], Loss: 1.708921, Acc:0.544529, Semantic loss: 0.158256, BCE loss: 1.448776, SB loss: 0.101888
2025-01-07 23:11:13,641 Epoch: [4/20] Iter:[110/192], Time: 0.61, lr: [0.007916413064293163], Loss: 1.727931, Acc:0.546720, Semantic loss: 0.157723, BCE loss: 1.468980, SB loss: 0.101228
2025-01-07 23:11:19,721 Epoch: [4/20] Iter:[120/192], Time: 0.61, lr: [0.007892355076631318], Loss: 1.731028, Acc:0.545382, Semantic loss: 0.159448, BCE loss: 1.470000, SB loss: 0.101580
2025-01-07 23:11:25,931 Epoch: [4/20] Iter:[130/192], Time: 0.61, lr: [0.007868288937847752], Loss: 1.738393, Acc:0.546136, Semantic loss: 0.159684, BCE loss: 1.477176, SB loss: 0.101533
2025-01-07 23:11:32,020 Epoch: [4/20] Iter:[140/192], Time: 0.61, lr: [0.007844214617460508], Loss: 1.745622, Acc:0.547517, Semantic loss: 0.159633, BCE loss: 1.484553, SB loss: 0.101435
2025-01-07 23:11:38,179 Epoch: [4/20] Iter:[150/192], Time: 0.61, lr: [0.007820132084769268], Loss: 1.747089, Acc:0.547964, Semantic loss: 0.159553, BCE loss: 1.486713, SB loss: 0.100823
2025-01-07 23:11:44,212 Epoch: [4/20] Iter:[160/192], Time: 0.61, lr: [0.00779604130885303], Loss: 1.739406, Acc:0.546976, Semantic loss: 0.159308, BCE loss: 1.478867, SB loss: 0.101231
2025-01-07 23:11:50,332 Epoch: [4/20] Iter:[170/192], Time: 0.61, lr: [0.007771942258567773], Loss: 1.733750, Acc:0.546500, Semantic loss: 0.159297, BCE loss: 1.473570, SB loss: 0.100883
2025-01-07 23:11:56,328 Epoch: [4/20] Iter:[180/192], Time: 0.61, lr: [0.007747834902544056], Loss: 1.743901, Acc:0.548056, Semantic loss: 0.158798, BCE loss: 1.484092, SB loss: 0.101011
2025-01-07 23:12:02,510 Epoch: [4/20] Iter:[190/192], Time: 0.61, lr: [0.0077237192091846206], Loss: 1.748595, Acc:0.550163, Semantic loss: 0.158161, BCE loss: 1.489607, SB loss: 0.100827
2025-01-07 23:14:14,551 0 [0.         0.27893482 0.28223777 0.17771936 0.48780399 0.0603733
 0.34892089 0.00237199] 0.20479526397256434
2025-01-07 23:14:14,552 1 [0.         0.28027853 0.36141645 0.24070449 0.28170321 0.02437846
 0.32712695 0.00882139] 0.19055368559961167
2025-01-07 23:14:14,552 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:14:14,662 Loss: 1.858, MeanIU:  0.1906, Best_mIoU:  0.1998
2025-01-07 23:14:14,662 [0.         0.28027853 0.36141645 0.24070449 0.28170321 0.02437846
 0.32712695 0.00882139]
2025-01-07 23:14:15,256 Epoch: [5/20] Iter:[0/192], Time: 0.50, lr: [0.007718895067235705], Loss: 1.884956, Acc:0.632487, Semantic loss: 0.139199, BCE loss: 1.662322, SB loss: 0.083435
2025-01-07 23:14:21,321 Epoch: [5/20] Iter:[10/192], Time: 0.60, lr: [0.007694769327040611], Loss: 1.872362, Acc:0.593620, Semantic loss: 0.150229, BCE loss: 1.615534, SB loss: 0.106598
2025-01-07 23:14:27,438 Epoch: [5/20] Iter:[20/192], Time: 0.60, lr: [0.00767063517918174], Loss: 1.818514, Acc:0.584165, Semantic loss: 0.150802, BCE loss: 1.562067, SB loss: 0.105645
2025-01-07 23:14:33,519 Epoch: [5/20] Iter:[30/192], Time: 0.61, lr: [0.007646492591316132], Loss: 1.773192, Acc:0.582400, Semantic loss: 0.150637, BCE loss: 1.516927, SB loss: 0.105629
2025-01-07 23:14:39,474 Epoch: [5/20] Iter:[40/192], Time: 0.60, lr: [0.007622341530862476], Loss: 1.712954, Acc:0.570522, Semantic loss: 0.154821, BCE loss: 1.449412, SB loss: 0.108721
2025-01-07 23:14:45,560 Epoch: [5/20] Iter:[50/192], Time: 0.60, lr: [0.0075981819649984985], Loss: 1.728245, Acc:0.572285, Semantic loss: 0.155061, BCE loss: 1.468453, SB loss: 0.104731
2025-01-07 23:14:51,712 Epoch: [5/20] Iter:[60/192], Time: 0.61, lr: [0.007574013860658317], Loss: 1.728068, Acc:0.572260, Semantic loss: 0.155818, BCE loss: 1.467002, SB loss: 0.105248
2025-01-07 23:14:57,825 Epoch: [5/20] Iter:[70/192], Time: 0.61, lr: [0.007549837184529776], Loss: 1.718583, Acc:0.572645, Semantic loss: 0.154994, BCE loss: 1.459521, SB loss: 0.104068
2025-01-07 23:15:03,908 Epoch: [5/20] Iter:[80/192], Time: 0.61, lr: [0.0075256519030517215], Loss: 1.737960, Acc:0.567168, Semantic loss: 0.156084, BCE loss: 1.477868, SB loss: 0.104008
2025-01-07 23:15:10,027 Epoch: [5/20] Iter:[90/192], Time: 0.61, lr: [0.007501457982411236], Loss: 1.723359, Acc:0.569806, Semantic loss: 0.154899, BCE loss: 1.465849, SB loss: 0.102611
2025-01-07 23:15:16,059 Epoch: [5/20] Iter:[100/192], Time: 0.61, lr: [0.0074772553885408604], Loss: 1.721893, Acc:0.568579, Semantic loss: 0.155100, BCE loss: 1.463483, SB loss: 0.103310
2025-01-07 23:15:22,065 Epoch: [5/20] Iter:[110/192], Time: 0.61, lr: [0.007453044087115737], Loss: 1.711408, Acc:0.567624, Semantic loss: 0.154906, BCE loss: 1.454514, SB loss: 0.101988
2025-01-07 23:15:28,274 Epoch: [5/20] Iter:[120/192], Time: 0.61, lr: [0.007428824043550734], Loss: 1.718704, Acc:0.565569, Semantic loss: 0.155044, BCE loss: 1.461134, SB loss: 0.102526
2025-01-07 23:15:34,439 Epoch: [5/20] Iter:[130/192], Time: 0.61, lr: [0.007404595222997526], Loss: 1.724562, Acc:0.562612, Semantic loss: 0.155796, BCE loss: 1.466302, SB loss: 0.102465
2025-01-07 23:15:40,482 Epoch: [5/20] Iter:[140/192], Time: 0.61, lr: [0.007380357590341623], Loss: 1.728064, Acc:0.561213, Semantic loss: 0.156349, BCE loss: 1.469775, SB loss: 0.101940
2025-01-07 23:15:46,476 Epoch: [5/20] Iter:[150/192], Time: 0.61, lr: [0.007356111110199354], Loss: 1.727712, Acc:0.561967, Semantic loss: 0.155922, BCE loss: 1.470148, SB loss: 0.101642
2025-01-07 23:15:52,622 Epoch: [5/20] Iter:[160/192], Time: 0.61, lr: [0.00733185574691482], Loss: 1.723096, Acc:0.563923, Semantic loss: 0.155380, BCE loss: 1.466513, SB loss: 0.101203
2025-01-07 23:15:58,727 Epoch: [5/20] Iter:[170/192], Time: 0.61, lr: [0.007307591464556783], Loss: 1.713123, Acc:0.563025, Semantic loss: 0.155976, BCE loss: 1.455851, SB loss: 0.101296
2025-01-07 23:16:04,771 Epoch: [5/20] Iter:[180/192], Time: 0.61, lr: [0.007283318226915514], Loss: 1.702489, Acc:0.560158, Semantic loss: 0.156675, BCE loss: 1.444266, SB loss: 0.101548
2025-01-07 23:16:10,907 Epoch: [5/20] Iter:[190/192], Time: 0.61, lr: [0.007259035997499604], Loss: 1.710840, Acc:0.559913, Semantic loss: 0.156780, BCE loss: 1.452340, SB loss: 0.101720
2025-01-07 23:18:23,082 0 [0.         0.28790149 0.2721152  0.17823247 0.5111346  0.06896144
 0.36405208 0.00355695] 0.2107442787409458
2025-01-07 23:18:23,083 1 [0.         0.3004687  0.28849583 0.38386525 0.489462   0.05055316
 0.31695936 0.12812749] 0.24474147390119233
2025-01-07 23:18:23,084 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:18:23,259 Loss: 1.779, MeanIU:  0.2447, Best_mIoU:  0.2447
2025-01-07 23:18:23,261 [0.         0.3004687  0.28849583 0.38386525 0.489462   0.05055316
 0.31695936 0.12812749]
2025-01-07 23:18:23,870 Epoch: [6/20] Iter:[0/192], Time: 0.52, lr: [0.007254178469372199], Loss: 1.483713, Acc:0.406900, Semantic loss: 0.260154, BCE loss: 1.038552, SB loss: 0.185006
2025-01-07 23:18:29,860 Epoch: [6/20] Iter:[10/192], Time: 0.59, lr: [0.007229885401256642], Loss: 1.650729, Acc:0.572032, Semantic loss: 0.162482, BCE loss: 1.382397, SB loss: 0.105850
2025-01-07 23:18:36,150 Epoch: [6/20] Iter:[20/192], Time: 0.61, lr: [0.0072055832600777334], Loss: 1.728475, Acc:0.562509, Semantic loss: 0.161082, BCE loss: 1.461711, SB loss: 0.105682
2025-01-07 23:18:42,316 Epoch: [6/20] Iter:[30/192], Time: 0.61, lr: [0.007181272008420604], Loss: 1.715028, Acc:0.567360, Semantic loss: 0.155344, BCE loss: 1.458641, SB loss: 0.101042
2025-01-07 23:18:48,344 Epoch: [6/20] Iter:[40/192], Time: 0.61, lr: [0.007156951608574727], Loss: 1.720256, Acc:0.567527, Semantic loss: 0.155846, BCE loss: 1.462456, SB loss: 0.101954
2025-01-07 23:18:54,385 Epoch: [6/20] Iter:[50/192], Time: 0.61, lr: [0.007132622022530447], Loss: 1.687856, Acc:0.556737, Semantic loss: 0.156905, BCE loss: 1.427555, SB loss: 0.103396
2025-01-07 23:19:00,583 Epoch: [6/20] Iter:[60/192], Time: 0.61, lr: [0.007108283211975477], Loss: 1.678265, Acc:0.561134, Semantic loss: 0.155471, BCE loss: 1.421985, SB loss: 0.100809
2025-01-07 23:19:06,682 Epoch: [6/20] Iter:[70/192], Time: 0.61, lr: [0.007083935138291319], Loss: 1.693496, Acc:0.562566, Semantic loss: 0.154810, BCE loss: 1.437927, SB loss: 0.100758
2025-01-07 23:19:12,815 Epoch: [6/20] Iter:[80/192], Time: 0.61, lr: [0.007059577762549636], Loss: 1.702459, Acc:0.561614, Semantic loss: 0.154803, BCE loss: 1.447025, SB loss: 0.100631
2025-01-07 23:19:18,943 Epoch: [6/20] Iter:[90/192], Time: 0.61, lr: [0.007035211045508576], Loss: 1.706444, Acc:0.564029, Semantic loss: 0.153883, BCE loss: 1.452741, SB loss: 0.099820
2025-01-07 23:19:25,034 Epoch: [6/20] Iter:[100/192], Time: 0.61, lr: [0.0070108349476090265], Loss: 1.710165, Acc:0.563113, Semantic loss: 0.154229, BCE loss: 1.456214, SB loss: 0.099722
2025-01-07 23:19:31,180 Epoch: [6/20] Iter:[110/192], Time: 0.61, lr: [0.00698644942897081], Loss: 1.710076, Acc:0.563325, Semantic loss: 0.153697, BCE loss: 1.456829, SB loss: 0.099550
2025-01-07 23:19:37,253 Epoch: [6/20] Iter:[120/192], Time: 0.61, lr: [0.006962054449388827], Loss: 1.706579, Acc:0.561832, Semantic loss: 0.154412, BCE loss: 1.452360, SB loss: 0.099807
2025-01-07 23:19:43,309 Epoch: [6/20] Iter:[130/192], Time: 0.61, lr: [0.006937649968329135], Loss: 1.713531, Acc:0.562499, Semantic loss: 0.154207, BCE loss: 1.459626, SB loss: 0.099697
2025-01-07 23:19:49,496 Epoch: [6/20] Iter:[140/192], Time: 0.61, lr: [0.006913235944924954], Loss: 1.710423, Acc:0.563977, Semantic loss: 0.154105, BCE loss: 1.456141, SB loss: 0.100176
2025-01-07 23:19:55,612 Epoch: [6/20] Iter:[150/192], Time: 0.61, lr: [0.006888812337972621], Loss: 1.702004, Acc:0.565697, Semantic loss: 0.154133, BCE loss: 1.447681, SB loss: 0.100189
2025-01-07 23:20:01,587 Epoch: [6/20] Iter:[160/192], Time: 0.61, lr: [0.00686437910592748], Loss: 1.687106, Acc:0.566741, Semantic loss: 0.153832, BCE loss: 1.432934, SB loss: 0.100340
2025-01-07 23:20:07,617 Epoch: [6/20] Iter:[170/192], Time: 0.61, lr: [0.00683993620689969], Loss: 1.685653, Acc:0.566210, Semantic loss: 0.153776, BCE loss: 1.431763, SB loss: 0.100114
2025-01-07 23:20:13,677 Epoch: [6/20] Iter:[180/192], Time: 0.61, lr: [0.0068154835986499775], Loss: 1.695514, Acc:0.568280, Semantic loss: 0.153304, BCE loss: 1.442142, SB loss: 0.100068
2025-01-07 23:20:19,656 Epoch: [6/20] Iter:[190/192], Time: 0.61, lr: [0.006791021238585323], Loss: 1.706224, Acc:0.567365, Semantic loss: 0.153312, BCE loss: 1.452333, SB loss: 0.100579
2025-01-07 23:22:31,473 0 [0.         0.28747674 0.32010839 0.19087115 0.52451433 0.04382931
 0.33858753 0.00109051] 0.21330974500318478
2025-01-07 23:22:31,474 1 [0.         0.29229267 0.39491773 0.26325453 0.25357628 0.05675379
 0.32359429 0.18712505] 0.22143929350461877
2025-01-07 23:22:31,474 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:22:31,582 Loss: 1.803, MeanIU:  0.2214, Best_mIoU:  0.2447
2025-01-07 23:22:31,582 [0.         0.29229267 0.39491773 0.26325453 0.25357628 0.05675379
 0.32359429 0.18712505]
2025-01-07 23:22:32,178 Epoch: [7/20] Iter:[0/192], Time: 0.50, lr: [0.006786127592581251], Loss: 1.800447, Acc:0.677764, Semantic loss: 0.127314, BCE loss: 1.586301, SB loss: 0.086832
2025-01-07 23:22:38,237 Epoch: [7/20] Iter:[10/192], Time: 0.60, lr: [0.006761653473611295], Loss: 1.832279, Acc:0.604652, Semantic loss: 0.144922, BCE loss: 1.591627, SB loss: 0.095729
2025-01-07 23:22:44,298 Epoch: [7/20] Iter:[20/192], Time: 0.60, lr: [0.006737169507854796], Loss: 1.732561, Acc:0.611174, Semantic loss: 0.141124, BCE loss: 1.496000, SB loss: 0.095437
2025-01-07 23:22:50,411 Epoch: [7/20] Iter:[30/192], Time: 0.60, lr: [0.006712675651556858], Loss: 1.716524, Acc:0.582553, Semantic loss: 0.148862, BCE loss: 1.464415, SB loss: 0.103248
2025-01-07 23:22:56,514 Epoch: [7/20] Iter:[40/192], Time: 0.61, lr: [0.006688171860589905], Loss: 1.709396, Acc:0.581562, Semantic loss: 0.150133, BCE loss: 1.455153, SB loss: 0.104110
2025-01-07 23:23:02,607 Epoch: [7/20] Iter:[50/192], Time: 0.61, lr: [0.0066636580904489585], Loss: 1.743852, Acc:0.583625, Semantic loss: 0.149847, BCE loss: 1.492227, SB loss: 0.101777
2025-01-07 23:23:08,710 Epoch: [7/20] Iter:[60/192], Time: 0.61, lr: [0.006639134296246874], Loss: 1.755534, Acc:0.583741, Semantic loss: 0.151166, BCE loss: 1.502333, SB loss: 0.102035
2025-01-07 23:23:14,749 Epoch: [7/20] Iter:[70/192], Time: 0.61, lr: [0.0066146004327094585], Loss: 1.760094, Acc:0.580775, Semantic loss: 0.152285, BCE loss: 1.505835, SB loss: 0.101975
2025-01-07 23:23:20,714 Epoch: [7/20] Iter:[80/192], Time: 0.61, lr: [0.006590056454170537], Loss: 1.754430, Acc:0.573131, Semantic loss: 0.153725, BCE loss: 1.496832, SB loss: 0.103873
2025-01-07 23:23:26,863 Epoch: [7/20] Iter:[90/192], Time: 0.61, lr: [0.006565502314566912], Loss: 1.748504, Acc:0.574148, Semantic loss: 0.153023, BCE loss: 1.492309, SB loss: 0.103172
2025-01-07 23:23:33,067 Epoch: [7/20] Iter:[100/192], Time: 0.61, lr: [0.006540937967433255], Loss: 1.750735, Acc:0.575308, Semantic loss: 0.152641, BCE loss: 1.495477, SB loss: 0.102616
2025-01-07 23:23:39,162 Epoch: [7/20] Iter:[110/192], Time: 0.61, lr: [0.006516363365896894], Loss: 1.732839, Acc:0.577270, Semantic loss: 0.151559, BCE loss: 1.479569, SB loss: 0.101711
2025-01-07 23:23:45,143 Epoch: [7/20] Iter:[120/192], Time: 0.61, lr: [0.006491778462672531], Loss: 1.725607, Acc:0.577345, Semantic loss: 0.151683, BCE loss: 1.472890, SB loss: 0.101034
2025-01-07 23:23:51,536 Epoch: [7/20] Iter:[130/192], Time: 0.61, lr: [0.006467183210056843], Loss: 1.730400, Acc:0.579643, Semantic loss: 0.150493, BCE loss: 1.479666, SB loss: 0.100241
2025-01-07 23:23:57,563 Epoch: [7/20] Iter:[140/192], Time: 0.61, lr: [0.006442577559923017], Loss: 1.721780, Acc:0.580758, Semantic loss: 0.150366, BCE loss: 1.471053, SB loss: 0.100362
2025-01-07 23:24:03,752 Epoch: [7/20] Iter:[150/192], Time: 0.61, lr: [0.006417961463715172], Loss: 1.718648, Acc:0.577729, Semantic loss: 0.151297, BCE loss: 1.466381, SB loss: 0.100970
2025-01-07 23:24:09,834 Epoch: [7/20] Iter:[160/192], Time: 0.61, lr: [0.006393334872442681], Loss: 1.709468, Acc:0.576956, Semantic loss: 0.150695, BCE loss: 1.457797, SB loss: 0.100975
2025-01-07 23:24:15,780 Epoch: [7/20] Iter:[170/192], Time: 0.61, lr: [0.006368697736674411], Loss: 1.703600, Acc:0.576252, Semantic loss: 0.150964, BCE loss: 1.451613, SB loss: 0.101022
2025-01-07 23:24:21,852 Epoch: [7/20] Iter:[180/192], Time: 0.61, lr: [0.006344050006532848], Loss: 1.709793, Acc:0.578603, Semantic loss: 0.150267, BCE loss: 1.459076, SB loss: 0.100450
2025-01-07 23:24:27,979 Epoch: [7/20] Iter:[190/192], Time: 0.61, lr: [0.006319391631688114], Loss: 1.702675, Acc:0.577383, Semantic loss: 0.151140, BCE loss: 1.450510, SB loss: 0.101025
2025-01-07 23:26:41,190 0 [0.         0.27757016 0.32717128 0.2040975  0.52513891 0.09670884
 0.36239531 0.0047922 ] 0.22473427513150446
2025-01-07 23:26:41,190 1 [0.         0.30080028 0.38140918 0.32171465 0.50650066 0.21333812
 0.2976335  0.10106384] 0.26530753090265485
2025-01-07 23:26:41,191 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:26:41,365 Loss: 1.803, MeanIU:  0.2653, Best_mIoU:  0.2653
2025-01-07 23:26:41,365 [0.         0.30080028 0.38140918 0.32171465 0.50650066 0.21333812
 0.2976335  0.10106384]
2025-01-07 23:26:41,979 Epoch: [8/20] Iter:[0/192], Time: 0.52, lr: [0.006314458674893553], Loss: 2.034857, Acc:0.645697, Semantic loss: 0.139339, BCE loss: 1.812444, SB loss: 0.083074
2025-01-07 23:26:47,977 Epoch: [8/20] Iter:[10/192], Time: 0.59, lr: [0.006289787459323657], Loss: 1.771897, Acc:0.605726, Semantic loss: 0.138377, BCE loss: 1.543862, SB loss: 0.089658
2025-01-07 23:26:54,143 Epoch: [8/20] Iter:[20/192], Time: 0.60, lr: [0.006265105486702424], Loss: 1.707884, Acc:0.589573, Semantic loss: 0.142415, BCE loss: 1.471755, SB loss: 0.093715
2025-01-07 23:27:00,285 Epoch: [8/20] Iter:[30/192], Time: 0.61, lr: [0.006240412705211017], Loss: 1.701566, Acc:0.590156, Semantic loss: 0.142179, BCE loss: 1.464920, SB loss: 0.094467
2025-01-07 23:27:06,377 Epoch: [8/20] Iter:[40/192], Time: 0.61, lr: [0.006215709062551949], Loss: 1.711657, Acc:0.582894, Semantic loss: 0.147513, BCE loss: 1.465242, SB loss: 0.098901
2025-01-07 23:27:12,529 Epoch: [8/20] Iter:[50/192], Time: 0.61, lr: [0.006190994505942529], Loss: 1.695386, Acc:0.586885, Semantic loss: 0.145711, BCE loss: 1.452105, SB loss: 0.097570
2025-01-07 23:27:18,583 Epoch: [8/20] Iter:[60/192], Time: 0.61, lr: [0.006166268982108192], Loss: 1.738819, Acc:0.587615, Semantic loss: 0.146459, BCE loss: 1.493348, SB loss: 0.099012
2025-01-07 23:27:24,649 Epoch: [8/20] Iter:[70/192], Time: 0.61, lr: [0.006141532437275693], Loss: 1.719355, Acc:0.585178, Semantic loss: 0.147254, BCE loss: 1.473588, SB loss: 0.098514
2025-01-07 23:27:30,762 Epoch: [8/20] Iter:[80/192], Time: 0.61, lr: [0.006116784817166194], Loss: 1.748586, Acc:0.583545, Semantic loss: 0.147593, BCE loss: 1.502816, SB loss: 0.098177
2025-01-07 23:27:36,898 Epoch: [8/20] Iter:[90/192], Time: 0.61, lr: [0.006092026066988214], Loss: 1.736359, Acc:0.582701, Semantic loss: 0.148671, BCE loss: 1.488934, SB loss: 0.098753
2025-01-07 23:27:42,942 Epoch: [8/20] Iter:[100/192], Time: 0.61, lr: [0.006067256131430442], Loss: 1.731914, Acc:0.582434, Semantic loss: 0.148695, BCE loss: 1.483623, SB loss: 0.099596
2025-01-07 23:27:49,054 Epoch: [8/20] Iter:[110/192], Time: 0.61, lr: [0.00604247495465443], Loss: 1.696559, Acc:0.583259, Semantic loss: 0.147772, BCE loss: 1.449587, SB loss: 0.099201
2025-01-07 23:27:55,200 Epoch: [8/20] Iter:[120/192], Time: 0.61, lr: [0.006017682480287143], Loss: 1.705095, Acc:0.584197, Semantic loss: 0.147990, BCE loss: 1.458541, SB loss: 0.098564
2025-01-07 23:28:01,540 Epoch: [8/20] Iter:[130/192], Time: 0.61, lr: [0.00599287865141337], Loss: 1.700952, Acc:0.582710, Semantic loss: 0.148530, BCE loss: 1.453876, SB loss: 0.098546
2025-01-07 23:28:07,716 Epoch: [8/20] Iter:[140/192], Time: 0.61, lr: [0.005968063410567983], Loss: 1.700352, Acc:0.582336, Semantic loss: 0.148328, BCE loss: 1.453303, SB loss: 0.098721
2025-01-07 23:28:13,782 Epoch: [8/20] Iter:[150/192], Time: 0.61, lr: [0.00594323669972807], Loss: 1.703674, Acc:0.585404, Semantic loss: 0.147515, BCE loss: 1.457846, SB loss: 0.098313
2025-01-07 23:28:19,791 Epoch: [8/20] Iter:[160/192], Time: 0.61, lr: [0.005918398460304892], Loss: 1.707919, Acc:0.585305, Semantic loss: 0.147390, BCE loss: 1.462098, SB loss: 0.098430
2025-01-07 23:28:25,846 Epoch: [8/20] Iter:[170/192], Time: 0.61, lr: [0.0058935486331357125], Loss: 1.700934, Acc:0.584801, Semantic loss: 0.147552, BCE loss: 1.454859, SB loss: 0.098523
2025-01-07 23:28:31,926 Epoch: [8/20] Iter:[180/192], Time: 0.61, lr: [0.005868687158475447], Loss: 1.699016, Acc:0.585706, Semantic loss: 0.147041, BCE loss: 1.453397, SB loss: 0.098578
2025-01-07 23:28:38,006 Epoch: [8/20] Iter:[190/192], Time: 0.61, lr: [0.005843813975988169], Loss: 1.697831, Acc:0.586046, Semantic loss: 0.146969, BCE loss: 1.452568, SB loss: 0.098295
2025-01-07 23:30:51,056 0 [0.         0.29028513 0.32865862 0.21140815 0.51141979 0.07973787
 0.37713025 0.00238512] 0.2251281159692049
2025-01-07 23:30:51,057 1 [0.         0.3134387  0.3840665  0.36103205 0.55219298 0.20038766
 0.35367124 0.11514541] 0.2849918158362088
2025-01-07 23:30:51,057 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:30:51,227 Loss: 1.770, MeanIU:  0.2850, Best_mIoU:  0.2850
2025-01-07 23:30:51,231 [0.         0.3134387  0.3840665  0.36103205 0.55219298 0.20038766
 0.35367124 0.11514541]
2025-01-07 23:30:51,872 Epoch: [9/20] Iter:[0/192], Time: 0.55, lr: [0.0058388379291998025], Loss: 2.099867, Acc:0.590406, Semantic loss: 0.151201, BCE loss: 1.859182, SB loss: 0.089483
2025-01-07 23:30:57,960 Epoch: [9/20] Iter:[10/192], Time: 0.60, lr: [0.0058139506168319475], Loss: 1.706348, Acc:0.584670, Semantic loss: 0.149781, BCE loss: 1.464918, SB loss: 0.091650
2025-01-07 23:31:04,034 Epoch: [9/20] Iter:[20/192], Time: 0.61, lr: [0.005789051461775303], Loss: 1.766429, Acc:0.620704, Semantic loss: 0.140388, BCE loss: 1.534141, SB loss: 0.091899
2025-01-07 23:31:10,237 Epoch: [9/20] Iter:[30/192], Time: 0.61, lr: [0.005764140401744157], Loss: 1.789786, Acc:0.616213, Semantic loss: 0.142256, BCE loss: 1.556212, SB loss: 0.091317
2025-01-07 23:31:16,395 Epoch: [9/20] Iter:[40/192], Time: 0.61, lr: [0.005739217373824397], Loss: 1.762433, Acc:0.608573, Semantic loss: 0.143739, BCE loss: 1.525809, SB loss: 0.092886
2025-01-07 23:31:22,489 Epoch: [9/20] Iter:[50/192], Time: 0.61, lr: [0.005714282314464109], Loss: 1.760960, Acc:0.610535, Semantic loss: 0.142917, BCE loss: 1.524477, SB loss: 0.093566
2025-01-07 23:31:28,603 Epoch: [9/20] Iter:[60/192], Time: 0.61, lr: [0.005689335159463984], Loss: 1.781242, Acc:0.604507, Semantic loss: 0.146582, BCE loss: 1.535932, SB loss: 0.098728
2025-01-07 23:31:34,734 Epoch: [9/20] Iter:[70/192], Time: 0.61, lr: [0.0056643758439675305], Loss: 1.757150, Acc:0.602188, Semantic loss: 0.145879, BCE loss: 1.514022, SB loss: 0.097250
2025-01-07 23:31:40,734 Epoch: [9/20] Iter:[80/192], Time: 0.61, lr: [0.005639404302451105], Loss: 1.746742, Acc:0.604078, Semantic loss: 0.144363, BCE loss: 1.506640, SB loss: 0.095739
2025-01-07 23:31:46,811 Epoch: [9/20] Iter:[90/192], Time: 0.61, lr: [0.005614420468713722], Loss: 1.730783, Acc:0.598456, Semantic loss: 0.146489, BCE loss: 1.488723, SB loss: 0.095571
2025-01-07 23:31:52,865 Epoch: [9/20] Iter:[100/192], Time: 0.61, lr: [0.005589424275866668], Loss: 1.712729, Acc:0.596457, Semantic loss: 0.147117, BCE loss: 1.469496, SB loss: 0.096117
2025-01-07 23:31:59,111 Epoch: [9/20] Iter:[110/192], Time: 0.61, lr: [0.005564415656322913], Loss: 1.726176, Acc:0.598479, Semantic loss: 0.146430, BCE loss: 1.484170, SB loss: 0.095577
2025-01-07 23:32:05,306 Epoch: [9/20] Iter:[120/192], Time: 0.61, lr: [0.00553939454178628], Loss: 1.728043, Acc:0.598957, Semantic loss: 0.145860, BCE loss: 1.487271, SB loss: 0.094912
2025-01-07 23:32:11,444 Epoch: [9/20] Iter:[130/192], Time: 0.61, lr: [0.005514360863240413], Loss: 1.739856, Acc:0.597852, Semantic loss: 0.146137, BCE loss: 1.497196, SB loss: 0.096523
2025-01-07 23:32:17,570 Epoch: [9/20] Iter:[140/192], Time: 0.61, lr: [0.005489314550937511], Loss: 1.725437, Acc:0.600733, Semantic loss: 0.145769, BCE loss: 1.483371, SB loss: 0.096297
2025-01-07 23:32:23,693 Epoch: [9/20] Iter:[150/192], Time: 0.61, lr: [0.005464255534386825], Loss: 1.714223, Acc:0.599179, Semantic loss: 0.146204, BCE loss: 1.471826, SB loss: 0.096192
2025-01-07 23:32:29,776 Epoch: [9/20] Iter:[160/192], Time: 0.61, lr: [0.005439183742342914], Loss: 1.708949, Acc:0.596756, Semantic loss: 0.146578, BCE loss: 1.465599, SB loss: 0.096772
2025-01-07 23:32:35,900 Epoch: [9/20] Iter:[170/192], Time: 0.61, lr: [0.00541409910279366], Loss: 1.706271, Acc:0.596848, Semantic loss: 0.146507, BCE loss: 1.463111, SB loss: 0.096654
2025-01-07 23:32:41,935 Epoch: [9/20] Iter:[180/192], Time: 0.61, lr: [0.005389001542948025], Loss: 1.701142, Acc:0.597660, Semantic loss: 0.145925, BCE loss: 1.458652, SB loss: 0.096565
2025-01-07 23:32:48,080 Epoch: [9/20] Iter:[190/192], Time: 0.61, lr: [0.005363890989223547], Loss: 1.696159, Acc:0.598492, Semantic loss: 0.145614, BCE loss: 1.454243, SB loss: 0.096303
2025-01-07 23:34:59,411 0 [0.         0.29006533 0.34538064 0.19883018 0.51373483 0.13054393
 0.37333267 0.00122365] 0.23163890553140803
2025-01-07 23:34:59,411 1 [0.         0.3096962  0.42811857 0.37126809 0.55903865 0.15437136
 0.3416208  0.24575009] 0.3012329696563102
2025-01-07 23:34:59,412 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:34:59,586 Loss: 1.749, MeanIU:  0.3012, Best_mIoU:  0.3012
2025-01-07 23:34:59,587 [0.         0.3096962  0.42811857 0.37126809 0.55903865 0.15437136
 0.3416208  0.24575009]
2025-01-07 23:35:00,220 Epoch: [10/20] Iter:[0/192], Time: 0.54, lr: [0.005358867312681466], Loss: 0.627180, Acc:0.684147, Semantic loss: 0.123052, BCE loss: 0.432791, SB loss: 0.071337
2025-01-07 23:35:06,225 Epoch: [10/20] Iter:[10/192], Time: 0.60, lr: [0.005333741068040314], Loss: 1.670268, Acc:0.639344, Semantic loss: 0.134053, BCE loss: 1.445115, SB loss: 0.091100
2025-01-07 23:35:12,363 Epoch: [10/20] Iter:[20/192], Time: 0.60, lr: [0.0053086016647897714], Loss: 1.654096, Acc:0.628324, Semantic loss: 0.139495, BCE loss: 1.418223, SB loss: 0.096379
2025-01-07 23:35:18,621 Epoch: [10/20] Iter:[30/192], Time: 0.61, lr: [0.005283449026727663], Loss: 1.646343, Acc:0.620887, Semantic loss: 0.142401, BCE loss: 1.407894, SB loss: 0.096048
2025-01-07 23:35:24,643 Epoch: [10/20] Iter:[40/192], Time: 0.61, lr: [0.005258283076804885], Loss: 1.661416, Acc:0.618783, Semantic loss: 0.142077, BCE loss: 1.422661, SB loss: 0.096678
2025-01-07 23:35:30,699 Epoch: [10/20] Iter:[50/192], Time: 0.61, lr: [0.00523310373711144], Loss: 1.688463, Acc:0.616054, Semantic loss: 0.140882, BCE loss: 1.451301, SB loss: 0.096280
2025-01-07 23:35:36,770 Epoch: [10/20] Iter:[60/192], Time: 0.61, lr: [0.005207910928862159], Loss: 1.661988, Acc:0.607489, Semantic loss: 0.143366, BCE loss: 1.421195, SB loss: 0.097426
2025-01-07 23:35:42,846 Epoch: [10/20] Iter:[70/192], Time: 0.61, lr: [0.005182704572382109], Loss: 1.664388, Acc:0.602874, Semantic loss: 0.143285, BCE loss: 1.424743, SB loss: 0.096360
2025-01-07 23:35:49,164 Epoch: [10/20] Iter:[80/192], Time: 0.61, lr: [0.005157484587091684], Loss: 1.659611, Acc:0.603662, Semantic loss: 0.143327, BCE loss: 1.420249, SB loss: 0.096035
2025-01-07 23:35:55,247 Epoch: [10/20] Iter:[90/192], Time: 0.61, lr: [0.005132250891491357], Loss: 1.673346, Acc:0.603224, Semantic loss: 0.143860, BCE loss: 1.433425, SB loss: 0.096061
2025-01-07 23:36:01,344 Epoch: [10/20] Iter:[100/192], Time: 0.61, lr: [0.005107003403146087], Loss: 1.677528, Acc:0.601394, Semantic loss: 0.144338, BCE loss: 1.436928, SB loss: 0.096261
2025-01-07 23:36:07,551 Epoch: [10/20] Iter:[110/192], Time: 0.61, lr: [0.005081742038669388], Loss: 1.659767, Acc:0.601407, Semantic loss: 0.144221, BCE loss: 1.419406, SB loss: 0.096141
2025-01-07 23:36:13,643 Epoch: [10/20] Iter:[120/192], Time: 0.61, lr: [0.005056466713707024], Loss: 1.662321, Acc:0.600576, Semantic loss: 0.144028, BCE loss: 1.422429, SB loss: 0.095865
2025-01-07 23:36:19,717 Epoch: [10/20] Iter:[130/192], Time: 0.61, lr: [0.005031177342920337], Loss: 1.679280, Acc:0.601329, Semantic loss: 0.144304, BCE loss: 1.438776, SB loss: 0.096200
2025-01-07 23:36:25,832 Epoch: [10/20] Iter:[140/192], Time: 0.61, lr: [0.005005873839969192], Loss: 1.684626, Acc:0.602042, Semantic loss: 0.144350, BCE loss: 1.443712, SB loss: 0.096564
2025-01-07 23:36:31,993 Epoch: [10/20] Iter:[150/192], Time: 0.61, lr: [0.00498055611749454], Loss: 1.694493, Acc:0.602840, Semantic loss: 0.144267, BCE loss: 1.453341, SB loss: 0.096885
2025-01-07 23:36:37,921 Epoch: [10/20] Iter:[160/192], Time: 0.61, lr: [0.004955224087100552], Loss: 1.675947, Acc:0.601934, Semantic loss: 0.144049, BCE loss: 1.435419, SB loss: 0.096479
2025-01-07 23:36:43,964 Epoch: [10/20] Iter:[170/192], Time: 0.61, lr: [0.004929877659336362], Loss: 1.671029, Acc:0.601584, Semantic loss: 0.143428, BCE loss: 1.431065, SB loss: 0.096537
2025-01-07 23:36:50,058 Epoch: [10/20] Iter:[180/192], Time: 0.61, lr: [0.004904516743677371], Loss: 1.668093, Acc:0.601509, Semantic loss: 0.143624, BCE loss: 1.428027, SB loss: 0.096442
2025-01-07 23:36:56,120 Epoch: [10/20] Iter:[190/192], Time: 0.61, lr: [0.00487914124850611], Loss: 1.672888, Acc:0.601711, Semantic loss: 0.143431, BCE loss: 1.433051, SB loss: 0.096406
2025-01-07 23:39:09,958 0 [0.         0.29120343 0.34797208 0.21892538 0.51385355 0.1049558
 0.38232642 0.00106381] 0.2325375599902761
2025-01-07 23:39:09,959 1 [0.         0.3040122  0.41784891 0.38949552 0.56029035 0.0794877
 0.32847373 0.13787325] 0.2771852073402643
2025-01-07 23:39:09,960 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:39:10,069 Loss: 1.764, MeanIU:  0.2772, Best_mIoU:  0.3012
2025-01-07 23:39:10,069 [0.         0.3040122  0.41784891 0.38949552 0.56029035 0.0794877
 0.32847373 0.13787325]
2025-01-07 23:39:10,679 Epoch: [11/20] Iter:[0/192], Time: 0.52, lr: [0.004874064391789954], Loss: 1.247404, Acc:0.452161, Semantic loss: 0.179309, BCE loss: 0.902755, SB loss: 0.165340
2025-01-07 23:39:16,894 Epoch: [11/20] Iter:[10/192], Time: 0.61, lr: [0.004848671278701159], Loss: 1.580432, Acc:0.568404, Semantic loss: 0.152866, BCE loss: 1.321203, SB loss: 0.106363
2025-01-07 23:39:23,053 Epoch: [11/20] Iter:[20/192], Time: 0.61, lr: [0.0048232633805975296], Loss: 1.620265, Acc:0.599180, Semantic loss: 0.148346, BCE loss: 1.375285, SB loss: 0.096633
2025-01-07 23:39:29,209 Epoch: [11/20] Iter:[30/192], Time: 0.61, lr: [0.00479784060223045], Loss: 1.624278, Acc:0.592068, Semantic loss: 0.152381, BCE loss: 1.369083, SB loss: 0.102814
2025-01-07 23:39:35,206 Epoch: [11/20] Iter:[40/192], Time: 0.61, lr: [0.004772402847172951], Loss: 1.633711, Acc:0.594393, Semantic loss: 0.150396, BCE loss: 1.380830, SB loss: 0.102486
2025-01-07 23:39:41,269 Epoch: [11/20] Iter:[50/192], Time: 0.61, lr: [0.004746950017798064], Loss: 1.645606, Acc:0.593803, Semantic loss: 0.148605, BCE loss: 1.395696, SB loss: 0.101304
2025-01-07 23:39:47,223 Epoch: [11/20] Iter:[60/192], Time: 0.61, lr: [0.004721482015256639], Loss: 1.676334, Acc:0.594353, Semantic loss: 0.146662, BCE loss: 1.429547, SB loss: 0.100126
2025-01-07 23:39:53,283 Epoch: [11/20] Iter:[70/192], Time: 0.61, lr: [0.004695998739454633], Loss: 1.688158, Acc:0.596241, Semantic loss: 0.145506, BCE loss: 1.442035, SB loss: 0.100617
2025-01-07 23:39:59,407 Epoch: [11/20] Iter:[80/192], Time: 0.61, lr: [0.004670500089029817], Loss: 1.681204, Acc:0.599912, Semantic loss: 0.144892, BCE loss: 1.436367, SB loss: 0.099946
2025-01-07 23:40:05,539 Epoch: [11/20] Iter:[90/192], Time: 0.61, lr: [0.004644985961327915], Loss: 1.682703, Acc:0.605726, Semantic loss: 0.143084, BCE loss: 1.441447, SB loss: 0.098171
2025-01-07 23:40:11,707 Epoch: [11/20] Iter:[100/192], Time: 0.61, lr: [0.004619456252378151], Loss: 1.676130, Acc:0.599792, Semantic loss: 0.144759, BCE loss: 1.433260, SB loss: 0.098111
2025-01-07 23:40:17,777 Epoch: [11/20] Iter:[110/192], Time: 0.61, lr: [0.004593910856868159], Loss: 1.683004, Acc:0.599988, Semantic loss: 0.144638, BCE loss: 1.440367, SB loss: 0.097999
2025-01-07 23:40:23,797 Epoch: [11/20] Iter:[120/192], Time: 0.61, lr: [0.004568349668118281], Loss: 1.677596, Acc:0.599791, Semantic loss: 0.145019, BCE loss: 1.435269, SB loss: 0.097309
2025-01-07 23:40:29,880 Epoch: [11/20] Iter:[130/192], Time: 0.61, lr: [0.004542772578055196], Loss: 1.661739, Acc:0.602044, Semantic loss: 0.144256, BCE loss: 1.421154, SB loss: 0.096329
2025-01-07 23:40:35,994 Epoch: [11/20] Iter:[140/192], Time: 0.61, lr: [0.00451717947718487], Loss: 1.668993, Acc:0.603480, Semantic loss: 0.143981, BCE loss: 1.428691, SB loss: 0.096321
2025-01-07 23:40:42,138 Epoch: [11/20] Iter:[150/192], Time: 0.61, lr: [0.004491570254564817], Loss: 1.666603, Acc:0.603457, Semantic loss: 0.143408, BCE loss: 1.426628, SB loss: 0.096567
2025-01-07 23:40:48,273 Epoch: [11/20] Iter:[160/192], Time: 0.61, lr: [0.004465944797775636], Loss: 1.676852, Acc:0.603197, Semantic loss: 0.143640, BCE loss: 1.436639, SB loss: 0.096572
2025-01-07 23:40:54,318 Epoch: [11/20] Iter:[170/192], Time: 0.61, lr: [0.004440302992891796], Loss: 1.667536, Acc:0.606429, Semantic loss: 0.142736, BCE loss: 1.428672, SB loss: 0.096128
2025-01-07 23:41:00,554 Epoch: [11/20] Iter:[180/192], Time: 0.61, lr: [0.004414644724451659], Loss: 1.675621, Acc:0.605315, Semantic loss: 0.143325, BCE loss: 1.435623, SB loss: 0.096673
2025-01-07 23:41:06,668 Epoch: [11/20] Iter:[190/192], Time: 0.61, lr: [0.004388969875426714], Loss: 1.665644, Acc:0.604773, Semantic loss: 0.143263, BCE loss: 1.425789, SB loss: 0.096591
2025-01-07 23:43:18,223 0 [0.         0.29557083 0.33975851 0.21834064 0.49802702 0.09303378
 0.38690997 0.00198554] 0.22920328645130053
2025-01-07 23:43:18,223 1 [0.         0.31535732 0.44946423 0.39571043 0.59086005 0.13587229
 0.35745022 0.32436965] 0.3211355257928379
2025-01-07 23:43:18,224 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:43:18,400 Loss: 1.734, MeanIU:  0.3211, Best_mIoU:  0.3211
2025-01-07 23:43:18,401 [0.         0.31535732 0.44946423 0.39571043 0.59086005 0.13587229
 0.35745022 0.32436965]
2025-01-07 23:43:19,020 Epoch: [12/20] Iter:[0/192], Time: 0.53, lr: [0.0043838329055408696], Loss: 1.360592, Acc:0.664087, Semantic loss: 0.114415, BCE loss: 1.161052, SB loss: 0.085126
2025-01-07 23:43:25,131 Epoch: [12/20] Iter:[10/192], Time: 0.60, lr: [0.004358138003083593], Loss: 1.610926, Acc:0.613511, Semantic loss: 0.134619, BCE loss: 1.385985, SB loss: 0.090321
2025-01-07 23:43:31,153 Epoch: [12/20] Iter:[20/192], Time: 0.60, lr: [0.0043324262569064236], Loss: 1.690526, Acc:0.613964, Semantic loss: 0.139276, BCE loss: 1.457189, SB loss: 0.094061
2025-01-07 23:43:37,182 Epoch: [12/20] Iter:[30/192], Time: 0.60, lr: [0.0043066975447503065], Loss: 1.701771, Acc:0.614763, Semantic loss: 0.140378, BCE loss: 1.466534, SB loss: 0.094858
2025-01-07 23:43:43,331 Epoch: [12/20] Iter:[40/192], Time: 0.61, lr: [0.00428095174265078], Loss: 1.738062, Acc:0.616424, Semantic loss: 0.139564, BCE loss: 1.502973, SB loss: 0.095526
2025-01-07 23:43:49,498 Epoch: [12/20] Iter:[50/192], Time: 0.61, lr: [0.004255188724902623], Loss: 1.701688, Acc:0.609889, Semantic loss: 0.141219, BCE loss: 1.461943, SB loss: 0.098527
2025-01-07 23:43:55,650 Epoch: [12/20] Iter:[60/192], Time: 0.61, lr: [0.004229408364023519], Loss: 1.684613, Acc:0.606819, Semantic loss: 0.140507, BCE loss: 1.446726, SB loss: 0.097379
2025-01-07 23:44:01,785 Epoch: [12/20] Iter:[70/192], Time: 0.61, lr: [0.004203610530716726], Loss: 1.676175, Acc:0.609468, Semantic loss: 0.140985, BCE loss: 1.436540, SB loss: 0.098650
2025-01-07 23:44:07,793 Epoch: [12/20] Iter:[80/192], Time: 0.61, lr: [0.004177795093832693], Loss: 1.661592, Acc:0.608677, Semantic loss: 0.141033, BCE loss: 1.423182, SB loss: 0.097377
2025-01-07 23:44:13,882 Epoch: [12/20] Iter:[90/192], Time: 0.61, lr: [0.004151961920329593], Loss: 1.658257, Acc:0.613649, Semantic loss: 0.139703, BCE loss: 1.422394, SB loss: 0.096160
2025-01-07 23:44:19,930 Epoch: [12/20] Iter:[100/192], Time: 0.61, lr: [0.004126110875232744], Loss: 1.639289, Acc:0.612352, Semantic loss: 0.140429, BCE loss: 1.402309, SB loss: 0.096552
2025-01-07 23:44:25,962 Epoch: [12/20] Iter:[110/192], Time: 0.61, lr: [0.004100241821592866], Loss: 1.641852, Acc:0.616230, Semantic loss: 0.140440, BCE loss: 1.405785, SB loss: 0.095626
2025-01-07 23:44:32,004 Epoch: [12/20] Iter:[120/192], Time: 0.61, lr: [0.00407435462044314], Loss: 1.649608, Acc:0.614922, Semantic loss: 0.141040, BCE loss: 1.413179, SB loss: 0.095389
2025-01-07 23:44:38,001 Epoch: [12/20] Iter:[130/192], Time: 0.61, lr: [0.004048449130755016], Loss: 1.662171, Acc:0.611619, Semantic loss: 0.142105, BCE loss: 1.424237, SB loss: 0.095829
2025-01-07 23:44:44,089 Epoch: [12/20] Iter:[140/192], Time: 0.61, lr: [0.004022525209392749], Loss: 1.653322, Acc:0.612205, Semantic loss: 0.141907, BCE loss: 1.416211, SB loss: 0.095204
2025-01-07 23:44:50,210 Epoch: [12/20] Iter:[150/192], Time: 0.61, lr: [0.003996582711066572], Loss: 1.655276, Acc:0.612428, Semantic loss: 0.141900, BCE loss: 1.417929, SB loss: 0.095447
2025-01-07 23:44:56,349 Epoch: [12/20] Iter:[160/192], Time: 0.61, lr: [0.003970621488284514], Loss: 1.658296, Acc:0.610351, Semantic loss: 0.143066, BCE loss: 1.418900, SB loss: 0.096330
2025-01-07 23:45:02,472 Epoch: [12/20] Iter:[170/192], Time: 0.61, lr: [0.003944641391302759], Loss: 1.656770, Acc:0.611471, Semantic loss: 0.142828, BCE loss: 1.418101, SB loss: 0.095841
2025-01-07 23:45:08,699 Epoch: [12/20] Iter:[180/192], Time: 0.61, lr: [0.003918642268074522], Loss: 1.647003, Acc:0.609657, Semantic loss: 0.142389, BCE loss: 1.408744, SB loss: 0.095869
2025-01-07 23:45:14,858 Epoch: [12/20] Iter:[190/192], Time: 0.61, lr: [0.003892623964197378], Loss: 1.649199, Acc:0.607174, Semantic loss: 0.142521, BCE loss: 1.410586, SB loss: 0.096092
2025-01-07 23:47:27,178 0 [0.         0.29405707 0.3473747  0.21622657 0.5047176  0.08279063
 0.3729888  0.00192046] 0.22750947798704002
2025-01-07 23:47:27,179 1 [0.         0.31597102 0.44904628 0.43813779 0.58549116 0.14161245
 0.32647537 0.28163259] 0.3172958312997375
2025-01-07 23:47:27,180 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:47:27,288 Loss: 1.736, MeanIU:  0.3173, Best_mIoU:  0.3211
2025-01-07 23:47:27,288 [0.         0.31597102 0.44904628 0.43813779 0.58549116 0.14161245
 0.32647537 0.28163259]
2025-01-07 23:47:27,884 Epoch: [13/20] Iter:[0/192], Time: 0.50, lr: [0.0038874179879876346], Loss: 1.195169, Acc:0.595148, Semantic loss: 0.141056, BCE loss: 0.960467, SB loss: 0.093646
2025-01-07 23:47:33,791 Epoch: [13/20] Iter:[10/192], Time: 0.58, lr: [0.003861376460123532], Loss: 1.482968, Acc:0.602690, Semantic loss: 0.147066, BCE loss: 1.234997, SB loss: 0.100905
2025-01-07 23:47:39,860 Epoch: [13/20] Iter:[20/192], Time: 0.59, lr: [0.003835315403363414], Loss: 1.536746, Acc:0.603721, Semantic loss: 0.143443, BCE loss: 1.294447, SB loss: 0.098856
2025-01-07 23:47:45,955 Epoch: [13/20] Iter:[30/192], Time: 0.60, lr: [0.00380923465539378], Loss: 1.584384, Acc:0.613271, Semantic loss: 0.142012, BCE loss: 1.343228, SB loss: 0.099143
2025-01-07 23:47:52,046 Epoch: [13/20] Iter:[40/192], Time: 0.60, lr: [0.0037831340513060203], Loss: 1.591806, Acc:0.610916, Semantic loss: 0.143707, BCE loss: 1.350159, SB loss: 0.097940
2025-01-07 23:47:58,212 Epoch: [13/20] Iter:[50/192], Time: 0.60, lr: [0.003757013423534689], Loss: 1.598661, Acc:0.604270, Semantic loss: 0.145787, BCE loss: 1.352383, SB loss: 0.100491
2025-01-07 23:48:04,439 Epoch: [13/20] Iter:[60/192], Time: 0.61, lr: [0.003730872601793835], Loss: 1.636340, Acc:0.604456, Semantic loss: 0.145332, BCE loss: 1.390993, SB loss: 0.100014
2025-01-07 23:48:10,557 Epoch: [13/20] Iter:[70/192], Time: 0.61, lr: [0.0037047114130112895], Loss: 1.662958, Acc:0.612360, Semantic loss: 0.143705, BCE loss: 1.420098, SB loss: 0.099155
2025-01-07 23:48:16,573 Epoch: [13/20] Iter:[80/192], Time: 0.61, lr: [0.0036785296812608405], Loss: 1.656919, Acc:0.610525, Semantic loss: 0.144199, BCE loss: 1.414324, SB loss: 0.098396
2025-01-07 23:48:22,559 Epoch: [13/20] Iter:[90/192], Time: 0.61, lr: [0.003652327227692201], Loss: 1.644546, Acc:0.609745, Semantic loss: 0.143371, BCE loss: 1.403211, SB loss: 0.097965
2025-01-07 23:48:28,724 Epoch: [13/20] Iter:[100/192], Time: 0.61, lr: [0.0036261038704587037], Loss: 1.633836, Acc:0.609272, Semantic loss: 0.143251, BCE loss: 1.392714, SB loss: 0.097871
2025-01-07 23:48:34,840 Epoch: [13/20] Iter:[110/192], Time: 0.61, lr: [0.003599859424642584], Loss: 1.633118, Acc:0.609560, Semantic loss: 0.143861, BCE loss: 1.390968, SB loss: 0.098289
2025-01-07 23:48:40,797 Epoch: [13/20] Iter:[120/192], Time: 0.61, lr: [0.0035735937021778063], Loss: 1.642027, Acc:0.609923, Semantic loss: 0.142971, BCE loss: 1.401259, SB loss: 0.097797
2025-01-07 23:48:46,879 Epoch: [13/20] Iter:[130/192], Time: 0.61, lr: [0.0035473065117702885], Loss: 1.635554, Acc:0.611139, Semantic loss: 0.142851, BCE loss: 1.395305, SB loss: 0.097398
2025-01-07 23:48:52,972 Epoch: [13/20] Iter:[140/192], Time: 0.61, lr: [0.003520997658815433], Loss: 1.643326, Acc:0.612013, Semantic loss: 0.142369, BCE loss: 1.403842, SB loss: 0.097115
2025-01-07 23:48:59,136 Epoch: [13/20] Iter:[150/192], Time: 0.61, lr: [0.003494666945312851], Loss: 1.652047, Acc:0.611875, Semantic loss: 0.142303, BCE loss: 1.413264, SB loss: 0.096480
2025-01-07 23:49:05,134 Epoch: [13/20] Iter:[160/192], Time: 0.61, lr: [0.00346831416977816], Loss: 1.647496, Acc:0.609527, Semantic loss: 0.142088, BCE loss: 1.409012, SB loss: 0.096396
2025-01-07 23:49:11,221 Epoch: [13/20] Iter:[170/192], Time: 0.61, lr: [0.003441939127151716], Loss: 1.660398, Acc:0.609668, Semantic loss: 0.142080, BCE loss: 1.422139, SB loss: 0.096179
2025-01-07 23:49:17,205 Epoch: [13/20] Iter:[180/192], Time: 0.61, lr: [0.0034155416087041646], Loss: 1.654024, Acc:0.611530, Semantic loss: 0.141385, BCE loss: 1.416663, SB loss: 0.095977
2025-01-07 23:49:23,286 Epoch: [13/20] Iter:[190/192], Time: 0.61, lr: [0.0033891214019386652], Loss: 1.655598, Acc:0.610537, Semantic loss: 0.141841, BCE loss: 1.417485, SB loss: 0.096272
2025-01-07 23:51:36,085 0 [0.         0.2979293  0.36122376 0.23615508 0.52200916 0.07200099
 0.38138389 0.00060539] 0.23391344719970664
2025-01-07 23:51:36,086 1 [0.         0.32534587 0.4431477  0.43783092 0.60031041 0.11142592
 0.33851773 0.2794661 ] 0.31700558062845974
2025-01-07 23:51:36,086 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:51:36,196 Loss: 1.730, MeanIU:  0.3170, Best_mIoU:  0.3211
2025-01-07 23:51:36,196 [0.         0.32534587 0.4431477  0.43783092 0.60031041 0.11142592
 0.33851773 0.2794661 ]
2025-01-07 23:51:36,787 Epoch: [14/20] Iter:[0/192], Time: 0.50, lr: [0.0033838346190164987], Loss: 1.386257, Acc:0.745650, Semantic loss: 0.094070, BCE loss: 1.221385, SB loss: 0.070802
2025-01-07 23:51:42,878 Epoch: [14/20] Iter:[10/192], Time: 0.60, lr: [0.0033573869003190927], Loss: 1.515977, Acc:0.602805, Semantic loss: 0.145087, BCE loss: 1.267652, SB loss: 0.103239
2025-01-07 23:51:49,164 Epoch: [14/20] Iter:[20/192], Time: 0.61, lr: [0.0033309160120447595], Loss: 1.617785, Acc:0.599529, Semantic loss: 0.145569, BCE loss: 1.371568, SB loss: 0.100649
2025-01-07 23:51:55,411 Epoch: [14/20] Iter:[30/192], Time: 0.62, lr: [0.0033044217289421207], Loss: 1.605379, Acc:0.602800, Semantic loss: 0.144804, BCE loss: 1.361512, SB loss: 0.099063
2025-01-07 23:52:01,521 Epoch: [14/20] Iter:[40/192], Time: 0.62, lr: [0.0032779038215418194], Loss: 1.541555, Acc:0.613033, Semantic loss: 0.140939, BCE loss: 1.302630, SB loss: 0.097985
2025-01-07 23:52:07,623 Epoch: [14/20] Iter:[50/192], Time: 0.61, lr: [0.0032513620560388795], Loss: 1.536265, Acc:0.606629, Semantic loss: 0.141507, BCE loss: 1.299501, SB loss: 0.095258
2025-01-07 23:52:13,684 Epoch: [14/20] Iter:[60/192], Time: 0.61, lr: [0.003224796194170671], Loss: 1.572189, Acc:0.615718, Semantic loss: 0.138167, BCE loss: 1.340264, SB loss: 0.093758
2025-01-07 23:52:19,818 Epoch: [14/20] Iter:[70/192], Time: 0.61, lr: [0.003198205993090303], Loss: 1.575889, Acc:0.615496, Semantic loss: 0.138293, BCE loss: 1.343348, SB loss: 0.094248
2025-01-07 23:52:26,017 Epoch: [14/20] Iter:[80/192], Time: 0.61, lr: [0.0031715912052352204], Loss: 1.578138, Acc:0.617278, Semantic loss: 0.138502, BCE loss: 1.345263, SB loss: 0.094373
2025-01-07 23:52:32,197 Epoch: [14/20] Iter:[90/192], Time: 0.61, lr: [0.0031449515781907557], Loss: 1.580338, Acc:0.613975, Semantic loss: 0.138815, BCE loss: 1.347101, SB loss: 0.094422
2025-01-07 23:52:38,279 Epoch: [14/20] Iter:[100/192], Time: 0.61, lr: [0.003118286854548424], Loss: 1.602545, Acc:0.612929, Semantic loss: 0.139473, BCE loss: 1.368104, SB loss: 0.094968
2025-01-07 23:52:44,500 Epoch: [14/20] Iter:[110/192], Time: 0.61, lr: [0.003091596771758693], Loss: 1.612035, Acc:0.612615, Semantic loss: 0.139874, BCE loss: 1.377194, SB loss: 0.094966
2025-01-07 23:52:50,582 Epoch: [14/20] Iter:[120/192], Time: 0.61, lr: [0.0030648810619779434], Loss: 1.623326, Acc:0.611468, Semantic loss: 0.140204, BCE loss: 1.388388, SB loss: 0.094734
2025-01-07 23:52:56,696 Epoch: [14/20] Iter:[130/192], Time: 0.61, lr: [0.0030381394519093602], Loss: 1.621766, Acc:0.613245, Semantic loss: 0.140078, BCE loss: 1.386832, SB loss: 0.094856
2025-01-07 23:53:02,868 Epoch: [14/20] Iter:[140/192], Time: 0.61, lr: [0.0030113716626374316], Loss: 1.619815, Acc:0.612493, Semantic loss: 0.139837, BCE loss: 1.385238, SB loss: 0.094741
2025-01-07 23:53:08,908 Epoch: [14/20] Iter:[150/192], Time: 0.61, lr: [0.0029845774094557345], Loss: 1.623841, Acc:0.612002, Semantic loss: 0.139726, BCE loss: 1.389667, SB loss: 0.094447
2025-01-07 23:53:14,920 Epoch: [14/20] Iter:[160/192], Time: 0.61, lr: [0.002957756401687678], Loss: 1.621962, Acc:0.613211, Semantic loss: 0.139621, BCE loss: 1.387799, SB loss: 0.094541
2025-01-07 23:53:21,155 Epoch: [14/20] Iter:[170/192], Time: 0.61, lr: [0.002930908342499829], Loss: 1.621663, Acc:0.611559, Semantic loss: 0.139724, BCE loss: 1.387434, SB loss: 0.094505
2025-01-07 23:53:27,350 Epoch: [14/20] Iter:[180/192], Time: 0.61, lr: [0.002904032928707437], Loss: 1.626755, Acc:0.611709, Semantic loss: 0.140178, BCE loss: 1.392126, SB loss: 0.094451
2025-01-07 23:53:33,489 Epoch: [14/20] Iter:[190/192], Time: 0.61, lr: [0.0028771298505717554], Loss: 1.635514, Acc:0.611428, Semantic loss: 0.139974, BCE loss: 1.400998, SB loss: 0.094541
2025-01-07 23:55:44,872 0 [0.         0.30028269 0.36128924 0.24331318 0.51839356 0.12967845
 0.37891669 0.00174507] 0.2417023600004854
2025-01-07 23:55:44,873 1 [0.         0.31683966 0.46555431 0.40569568 0.56058574 0.17928012
 0.31839172 0.2452821 ] 0.3114536652203056
2025-01-07 23:55:44,873 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:55:44,985 Loss: 1.761, MeanIU:  0.3115, Best_mIoU:  0.3211
2025-01-07 23:55:44,985 [0.         0.31683966 0.46555431 0.40569568 0.56058574 0.17928012
 0.31839172 0.2452821 ]
2025-01-07 23:55:45,594 Epoch: [15/20] Iter:[0/192], Time: 0.52, lr: [0.0028717458874925874], Loss: 1.277720, Acc:0.704233, Semantic loss: 0.104299, BCE loss: 1.094386, SB loss: 0.079035
2025-01-07 23:55:51,643 Epoch: [15/20] Iter:[10/192], Time: 0.60, lr: [0.0028448091937488323], Loss: 1.641534, Acc:0.650416, Semantic loss: 0.135177, BCE loss: 1.414512, SB loss: 0.091845
2025-01-07 23:55:57,796 Epoch: [15/20] Iter:[20/192], Time: 0.61, lr: [0.002817844130111582], Loss: 1.703259, Acc:0.649761, Semantic loss: 0.134027, BCE loss: 1.477741, SB loss: 0.091491
2025-01-07 23:56:03,804 Epoch: [15/20] Iter:[30/192], Time: 0.60, lr: [0.0027908503644035994], Loss: 1.698670, Acc:0.639929, Semantic loss: 0.136479, BCE loss: 1.471977, SB loss: 0.090214
2025-01-07 23:56:10,068 Epoch: [15/20] Iter:[40/192], Time: 0.61, lr: [0.0027638275569424097], Loss: 1.733970, Acc:0.625862, Semantic loss: 0.140800, BCE loss: 1.499113, SB loss: 0.094057
2025-01-07 23:56:16,143 Epoch: [15/20] Iter:[50/192], Time: 0.61, lr: [0.002736775360287257], Loss: 1.701574, Acc:0.613737, Semantic loss: 0.144188, BCE loss: 1.461797, SB loss: 0.095590
2025-01-07 23:56:22,380 Epoch: [15/20] Iter:[60/192], Time: 0.61, lr: [0.002709693418974644], Loss: 1.701862, Acc:0.626848, Semantic loss: 0.142334, BCE loss: 1.465016, SB loss: 0.094511
2025-01-07 23:56:28,401 Epoch: [15/20] Iter:[70/192], Time: 0.61, lr: [0.0026825813692418162], Loss: 1.687768, Acc:0.623544, Semantic loss: 0.143341, BCE loss: 1.449483, SB loss: 0.094944
2025-01-07 23:56:34,465 Epoch: [15/20] Iter:[80/192], Time: 0.61, lr: [0.0026554388387375], Loss: 1.659976, Acc:0.622196, Semantic loss: 0.143807, BCE loss: 1.420551, SB loss: 0.095618
2025-01-07 23:56:40,642 Epoch: [15/20] Iter:[90/192], Time: 0.61, lr: [0.002628265446219161], Loss: 1.646718, Acc:0.618491, Semantic loss: 0.143961, BCE loss: 1.406565, SB loss: 0.096192
2025-01-07 23:56:46,763 Epoch: [15/20] Iter:[100/192], Time: 0.61, lr: [0.002601060801235972], Loss: 1.648793, Acc:0.619242, Semantic loss: 0.143749, BCE loss: 1.409917, SB loss: 0.095127
2025-01-07 23:56:52,877 Epoch: [15/20] Iter:[110/192], Time: 0.61, lr: [0.002573824503796671], Loss: 1.661269, Acc:0.620966, Semantic loss: 0.143097, BCE loss: 1.423662, SB loss: 0.094510
2025-01-07 23:56:59,034 Epoch: [15/20] Iter:[120/192], Time: 0.61, lr: [0.00254655614402138], Loss: 1.661433, Acc:0.618849, Semantic loss: 0.142482, BCE loss: 1.425122, SB loss: 0.093829
2025-01-07 23:57:05,182 Epoch: [15/20] Iter:[130/192], Time: 0.61, lr: [0.0025192553017764118], Loss: 1.659818, Acc:0.616403, Semantic loss: 0.142111, BCE loss: 1.424231, SB loss: 0.093476
2025-01-07 23:57:11,276 Epoch: [15/20] Iter:[140/192], Time: 0.61, lr: [0.002491921546291034], Loss: 1.657114, Acc:0.619087, Semantic loss: 0.141672, BCE loss: 1.421602, SB loss: 0.093839
2025-01-07 23:57:17,498 Epoch: [15/20] Iter:[150/192], Time: 0.61, lr: [0.0024645544357550573], Loss: 1.658467, Acc:0.616834, Semantic loss: 0.142104, BCE loss: 1.421828, SB loss: 0.094535
2025-01-07 23:57:23,538 Epoch: [15/20] Iter:[160/192], Time: 0.61, lr: [0.002437153516896028], Loss: 1.649859, Acc:0.617580, Semantic loss: 0.141344, BCE loss: 1.414300, SB loss: 0.094215
2025-01-07 23:57:29,576 Epoch: [15/20] Iter:[170/192], Time: 0.61, lr: [0.0024097183245347467], Loss: 1.642879, Acc:0.617718, Semantic loss: 0.140985, BCE loss: 1.407706, SB loss: 0.094187
2025-01-07 23:57:35,672 Epoch: [15/20] Iter:[180/192], Time: 0.61, lr: [0.0023822483811177], Loss: 1.645760, Acc:0.618246, Semantic loss: 0.140663, BCE loss: 1.411105, SB loss: 0.093993
2025-01-07 23:57:41,801 Epoch: [15/20] Iter:[190/192], Time: 0.61, lr: [0.002354743196224891], Loss: 1.642915, Acc:0.620523, Semantic loss: 0.140471, BCE loss: 1.408261, SB loss: 0.094183
2025-01-07 23:59:54,216 0 [0.         0.30384522 0.36287137 0.2410367  0.53034856 0.12818263
 0.37951282 0.00120407] 0.24337517180696233
2025-01-07 23:59:54,216 1 [0.         0.33305132 0.43726928 0.45304051 0.5994139  0.22836518
 0.33920259 0.20513726] 0.32443500385296997
2025-01-07 23:59:54,217 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-07 23:59:54,387 Loss: 1.731, MeanIU:  0.3244, Best_mIoU:  0.3244
2025-01-07 23:59:54,388 [0.         0.33305132 0.43726928 0.45304051 0.5994139  0.22836518
 0.33920259 0.20513726]
2025-01-07 23:59:55,034 Epoch: [16/20] Iter:[0/192], Time: 0.55, lr: [0.0023492378861760376], Loss: 1.013595, Acc:0.547179, Semantic loss: 0.150280, BCE loss: 0.743293, SB loss: 0.120022
2025-01-08 00:00:01,133 Epoch: [16/20] Iter:[10/192], Time: 0.60, lr: [0.0023216897452738748], Loss: 1.506590, Acc:0.630357, Semantic loss: 0.141843, BCE loss: 1.267159, SB loss: 0.097589
2025-01-08 00:00:07,331 Epoch: [16/20] Iter:[20/192], Time: 0.61, lr: [0.0022941052360858027], Loss: 1.586502, Acc:0.614508, Semantic loss: 0.143042, BCE loss: 1.347272, SB loss: 0.096188
2025-01-08 00:00:13,425 Epoch: [16/20] Iter:[30/192], Time: 0.61, lr: [0.0022664838233947228], Loss: 1.623879, Acc:0.624185, Semantic loss: 0.142930, BCE loss: 1.387120, SB loss: 0.093830
2025-01-08 00:00:19,546 Epoch: [16/20] Iter:[40/192], Time: 0.61, lr: [0.0022388249567421017], Loss: 1.622473, Acc:0.624999, Semantic loss: 0.140959, BCE loss: 1.388785, SB loss: 0.092729
2025-01-08 00:00:25,583 Epoch: [16/20] Iter:[50/192], Time: 0.61, lr: [0.002211128069778427], Loss: 1.640324, Acc:0.624421, Semantic loss: 0.140865, BCE loss: 1.406970, SB loss: 0.092489
2025-01-08 00:00:31,726 Epoch: [16/20] Iter:[60/192], Time: 0.61, lr: [0.0021833925795765436], Loss: 1.635189, Acc:0.621842, Semantic loss: 0.141194, BCE loss: 1.401096, SB loss: 0.092900
2025-01-08 00:00:37,908 Epoch: [16/20] Iter:[70/192], Time: 0.61, lr: [0.0021556178859051967], Loss: 1.619995, Acc:0.623562, Semantic loss: 0.139738, BCE loss: 1.387814, SB loss: 0.092443
2025-01-08 00:00:43,868 Epoch: [16/20] Iter:[80/192], Time: 0.61, lr: [0.002127803370459852], Loss: 1.618962, Acc:0.622784, Semantic loss: 0.139614, BCE loss: 1.386425, SB loss: 0.092923
2025-01-08 00:00:50,027 Epoch: [16/20] Iter:[90/192], Time: 0.61, lr: [0.0020999483960476508], Loss: 1.616413, Acc:0.625079, Semantic loss: 0.138429, BCE loss: 1.385276, SB loss: 0.092708
2025-01-08 00:00:56,093 Epoch: [16/20] Iter:[100/192], Time: 0.61, lr: [0.0020720523057230294], Loss: 1.620527, Acc:0.625578, Semantic loss: 0.138611, BCE loss: 1.388904, SB loss: 0.093012
2025-01-08 00:01:02,307 Epoch: [16/20] Iter:[110/192], Time: 0.61, lr: [0.002044114421870228], Loss: 1.626617, Acc:0.625356, Semantic loss: 0.138155, BCE loss: 1.396184, SB loss: 0.092277
2025-01-08 00:01:08,241 Epoch: [16/20] Iter:[120/192], Time: 0.61, lr: [0.0020161340452285867], Loss: 1.629153, Acc:0.621827, Semantic loss: 0.139496, BCE loss: 1.396409, SB loss: 0.093248
2025-01-08 00:01:14,507 Epoch: [16/20] Iter:[130/192], Time: 0.61, lr: [0.001988110453856112], Loss: 1.609648, Acc:0.623531, Semantic loss: 0.139282, BCE loss: 1.377038, SB loss: 0.093328
2025-01-08 00:01:20,645 Epoch: [16/20] Iter:[140/192], Time: 0.61, lr: [0.0019600429020263607], Loss: 1.614826, Acc:0.623576, Semantic loss: 0.139090, BCE loss: 1.382457, SB loss: 0.093280
2025-01-08 00:01:26,697 Epoch: [16/20] Iter:[150/192], Time: 0.61, lr: [0.0019319306190532476], Loss: 1.623679, Acc:0.622956, Semantic loss: 0.139504, BCE loss: 1.390697, SB loss: 0.093478
2025-01-08 00:01:32,755 Epoch: [16/20] Iter:[160/192], Time: 0.61, lr: [0.0019037728080378042], Loss: 1.616847, Acc:0.621902, Semantic loss: 0.139510, BCE loss: 1.383984, SB loss: 0.093353
2025-01-08 00:01:38,845 Epoch: [16/20] Iter:[170/192], Time: 0.61, lr: [0.0018755686445303314], Loss: 1.629670, Acc:0.622130, Semantic loss: 0.139455, BCE loss: 1.396586, SB loss: 0.093629
2025-01-08 00:01:45,036 Epoch: [16/20] Iter:[180/192], Time: 0.61, lr: [0.001847317275100738], Loss: 1.619855, Acc:0.621611, Semantic loss: 0.139073, BCE loss: 1.387438, SB loss: 0.093344
2025-01-08 00:01:51,080 Epoch: [16/20] Iter:[190/192], Time: 0.61, lr: [0.0018190178158090872], Loss: 1.621472, Acc:0.620359, Semantic loss: 0.138642, BCE loss: 1.389757, SB loss: 0.093072
2025-01-08 00:04:02,780 0 [0.         0.29956128 0.36039847 0.2326982  0.52823336 0.11422968
 0.38510602 0.00157514] 0.24022526883813755
2025-01-08 00:04:02,782 1 [0.         0.32203922 0.46277613 0.43827222 0.59663948 0.204395
 0.33908771 0.34451758] 0.33846591873776044
2025-01-08 00:04:02,782 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-08 00:04:02,950 Loss: 1.726, MeanIU:  0.3385, Best_mIoU:  0.3385
2025-01-08 00:04:02,951 [0.         0.32203922 0.46277613 0.43827222 0.59663948 0.204395
 0.33908771 0.34451758]
2025-01-08 00:04:03,567 Epoch: [17/20] Iter:[0/192], Time: 0.52, lr: [0.0018133520731367456], Loss: 1.531292, Acc:0.656370, Semantic loss: 0.110863, BCE loss: 1.334797, SB loss: 0.085632
2025-01-08 00:04:09,657 Epoch: [17/20] Iter:[10/192], Time: 0.60, lr: [0.001784993693823396], Loss: 1.507878, Acc:0.671003, Semantic loss: 0.123713, BCE loss: 1.300608, SB loss: 0.083557
2025-01-08 00:04:15,918 Epoch: [17/20] Iter:[20/192], Time: 0.61, lr: [0.0017565851643374117], Loss: 1.477727, Acc:0.650114, Semantic loss: 0.133683, BCE loss: 1.256965, SB loss: 0.087079
2025-01-08 00:04:21,995 Epoch: [17/20] Iter:[30/192], Time: 0.61, lr: [0.0017281254915000803], Loss: 1.464524, Acc:0.634681, Semantic loss: 0.136160, BCE loss: 1.237964, SB loss: 0.090400
2025-01-08 00:04:28,152 Epoch: [17/20] Iter:[40/192], Time: 0.61, lr: [0.0016996136438923032], Loss: 1.511569, Acc:0.617296, Semantic loss: 0.140236, BCE loss: 1.277497, SB loss: 0.093837
2025-01-08 00:04:34,161 Epoch: [17/20] Iter:[50/192], Time: 0.61, lr: [0.0016710485496403851], Loss: 1.506106, Acc:0.611064, Semantic loss: 0.140475, BCE loss: 1.270391, SB loss: 0.095239
2025-01-08 00:04:40,219 Epoch: [17/20] Iter:[60/192], Time: 0.61, lr: [0.0016424290940290059], Loss: 1.556061, Acc:0.610933, Semantic loss: 0.140856, BCE loss: 1.319564, SB loss: 0.095640
2025-01-08 00:04:46,444 Epoch: [17/20] Iter:[70/192], Time: 0.61, lr: [0.0016137541169242964], Loss: 1.564332, Acc:0.613410, Semantic loss: 0.139678, BCE loss: 1.329597, SB loss: 0.095057
2025-01-08 00:04:52,460 Epoch: [17/20] Iter:[80/192], Time: 0.61, lr: [0.0015850224099878446], Loss: 1.583700, Acc:0.618081, Semantic loss: 0.138121, BCE loss: 1.351034, SB loss: 0.094545
2025-01-08 00:04:58,643 Epoch: [17/20] Iter:[90/192], Time: 0.61, lr: [0.001556232713660091], Loss: 1.616009, Acc:0.620817, Semantic loss: 0.137904, BCE loss: 1.384346, SB loss: 0.093759
2025-01-08 00:05:04,676 Epoch: [17/20] Iter:[100/192], Time: 0.61, lr: [0.0015273837138888994], Loss: 1.622701, Acc:0.619499, Semantic loss: 0.138773, BCE loss: 1.389704, SB loss: 0.094224
2025-01-08 00:05:10,691 Epoch: [17/20] Iter:[110/192], Time: 0.61, lr: [0.0014984740385759552], Loss: 1.616250, Acc:0.623278, Semantic loss: 0.137360, BCE loss: 1.385039, SB loss: 0.093850
2025-01-08 00:05:16,795 Epoch: [17/20] Iter:[120/192], Time: 0.61, lr: [0.0014695022537100475], Loss: 1.620280, Acc:0.625443, Semantic loss: 0.137451, BCE loss: 1.388252, SB loss: 0.094578
2025-01-08 00:05:22,911 Epoch: [17/20] Iter:[130/192], Time: 0.61, lr: [0.0014404668591521846], Loss: 1.617498, Acc:0.623042, Semantic loss: 0.138667, BCE loss: 1.383107, SB loss: 0.095725
2025-01-08 00:05:29,003 Epoch: [17/20] Iter:[140/192], Time: 0.61, lr: [0.0014113662840326588], Loss: 1.630575, Acc:0.623965, Semantic loss: 0.138256, BCE loss: 1.397308, SB loss: 0.095011
2025-01-08 00:05:35,149 Epoch: [17/20] Iter:[150/192], Time: 0.61, lr: [0.0013821988817145786], Loss: 1.627718, Acc:0.625565, Semantic loss: 0.138113, BCE loss: 1.395102, SB loss: 0.094503
2025-01-08 00:05:41,270 Epoch: [17/20] Iter:[160/192], Time: 0.61, lr: [0.0013529629242719093], Loss: 1.629403, Acc:0.624161, Semantic loss: 0.138932, BCE loss: 1.395516, SB loss: 0.094954
2025-01-08 00:05:47,376 Epoch: [17/20] Iter:[170/192], Time: 0.61, lr: [0.0013236565964223801], Loss: 1.633645, Acc:0.624586, Semantic loss: 0.138667, BCE loss: 1.399946, SB loss: 0.095032
2025-01-08 00:05:53,507 Epoch: [17/20] Iter:[180/192], Time: 0.61, lr: [0.0012942779888466518], Loss: 1.630551, Acc:0.620777, Semantic loss: 0.138516, BCE loss: 1.397168, SB loss: 0.094867
2025-01-08 00:05:59,622 Epoch: [17/20] Iter:[190/192], Time: 0.61, lr: [0.0012648250908145803], Loss: 1.626724, Acc:0.621908, Semantic loss: 0.138396, BCE loss: 1.393705, SB loss: 0.094623
2025-01-08 00:08:10,966 0 [0.         0.29946624 0.34716072 0.23898505 0.49888763 0.08834725
 0.39639174 0.00095047] 0.2337736382275952
2025-01-08 00:08:10,967 1 [0.         0.31512658 0.45307572 0.46121282 0.590588   0.16921425
 0.33510646 0.18792803] 0.3140314828049756
2025-01-08 00:08:10,968 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-08 00:08:11,078 Loss: 1.740, MeanIU:  0.3140, Best_mIoU:  0.3385
2025-01-08 00:08:11,078 [0.         0.31512658 0.45307572 0.46121282 0.590588   0.16921425
 0.33510646 0.18792803]
2025-01-08 00:08:11,680 Epoch: [18/20] Iter:[0/192], Time: 0.51, lr: [0.001258925411794167], Loss: 1.622876, Acc:0.632121, Semantic loss: 0.156334, BCE loss: 1.356965, SB loss: 0.109577
2025-01-08 00:08:17,760 Epoch: [18/20] Iter:[10/192], Time: 0.60, lr: [0.0012293805561511607], Loss: 1.587036, Acc:0.624401, Semantic loss: 0.138462, BCE loss: 1.352455, SB loss: 0.096120
2025-01-08 00:08:23,837 Epoch: [18/20] Iter:[20/192], Time: 0.60, lr: [0.0011997565879500753], Loss: 1.521057, Acc:0.608235, Semantic loss: 0.143017, BCE loss: 1.284894, SB loss: 0.093147
2025-01-08 00:08:29,921 Epoch: [18/20] Iter:[30/192], Time: 0.60, lr: [0.0011700511125444005], Loss: 1.559239, Acc:0.609076, Semantic loss: 0.142887, BCE loss: 1.321726, SB loss: 0.094625
2025-01-08 00:08:35,950 Epoch: [18/20] Iter:[40/192], Time: 0.60, lr: [0.0011402615929770753], Loss: 1.593611, Acc:0.617699, Semantic loss: 0.139484, BCE loss: 1.360503, SB loss: 0.093624
2025-01-08 00:08:41,978 Epoch: [18/20] Iter:[50/192], Time: 0.60, lr: [0.0011103853371305413], Loss: 1.629154, Acc:0.609185, Semantic loss: 0.141356, BCE loss: 1.392649, SB loss: 0.095150
2025-01-08 00:08:48,153 Epoch: [18/20] Iter:[60/192], Time: 0.61, lr: [0.001080419483295973], Loss: 1.650827, Acc:0.606622, Semantic loss: 0.143528, BCE loss: 1.411203, SB loss: 0.096097
2025-01-08 00:08:54,293 Epoch: [18/20] Iter:[70/192], Time: 0.61, lr: [0.0010503609839122385], Loss: 1.632109, Acc:0.610001, Semantic loss: 0.141931, BCE loss: 1.395431, SB loss: 0.094746
2025-01-08 00:09:00,391 Epoch: [18/20] Iter:[80/192], Time: 0.61, lr: [0.0010202065871765603], Loss: 1.641937, Acc:0.612989, Semantic loss: 0.141000, BCE loss: 1.406933, SB loss: 0.094004
2025-01-08 00:09:06,546 Epoch: [18/20] Iter:[90/192], Time: 0.61, lr: [0.000989952816168914], Loss: 1.652350, Acc:0.610999, Semantic loss: 0.141039, BCE loss: 1.417232, SB loss: 0.094079
2025-01-08 00:09:12,736 Epoch: [18/20] Iter:[100/192], Time: 0.61, lr: [0.0009595959450576828], Loss: 1.655178, Acc:0.616555, Semantic loss: 0.140611, BCE loss: 1.420175, SB loss: 0.094392
2025-01-08 00:09:18,770 Epoch: [18/20] Iter:[110/192], Time: 0.61, lr: [0.000929131971860904], Loss: 1.673907, Acc:0.615497, Semantic loss: 0.139916, BCE loss: 1.439872, SB loss: 0.094119
2025-01-08 00:09:24,699 Epoch: [18/20] Iter:[120/192], Time: 0.61, lr: [0.0008985565871200918], Loss: 1.652019, Acc:0.616041, Semantic loss: 0.139379, BCE loss: 1.418527, SB loss: 0.094113
2025-01-08 00:09:30,764 Epoch: [18/20] Iter:[130/192], Time: 0.61, lr: [0.0008678651376944955], Loss: 1.649792, Acc:0.616064, Semantic loss: 0.140256, BCE loss: 1.415031, SB loss: 0.094505
2025-01-08 00:09:36,967 Epoch: [18/20] Iter:[140/192], Time: 0.61, lr: [0.000837052584692747], Loss: 1.649526, Acc:0.616594, Semantic loss: 0.141073, BCE loss: 1.413212, SB loss: 0.095240
2025-01-08 00:09:43,031 Epoch: [18/20] Iter:[150/192], Time: 0.61, lr: [0.000806113454312208], Loss: 1.644614, Acc:0.617099, Semantic loss: 0.141482, BCE loss: 1.407750, SB loss: 0.095382
2025-01-08 00:09:49,162 Epoch: [18/20] Iter:[160/192], Time: 0.61, lr: [0.0007750417800344363], Loss: 1.639610, Acc:0.618630, Semantic loss: 0.141099, BCE loss: 1.403343, SB loss: 0.095168
2025-01-08 00:09:55,266 Epoch: [18/20] Iter:[170/192], Time: 0.61, lr: [0.0007438310342008949], Loss: 1.635889, Acc:0.616819, Semantic loss: 0.141578, BCE loss: 1.398743, SB loss: 0.095568
2025-01-08 00:10:01,490 Epoch: [18/20] Iter:[180/192], Time: 0.61, lr: [0.0007124740464272778], Loss: 1.641310, Acc:0.619207, Semantic loss: 0.140703, BCE loss: 1.405361, SB loss: 0.095246
2025-01-08 00:10:07,498 Epoch: [18/20] Iter:[190/192], Time: 0.61, lr: [0.0006809629055511224], Loss: 1.637857, Acc:0.619753, Semantic loss: 0.140309, BCE loss: 1.402499, SB loss: 0.095049
2025-01-08 00:12:19,364 0 [0.         0.29988301 0.35965118 0.24174663 0.51198533 0.10484188
 0.39074269 0.00159865] 0.2388061704092397
2025-01-08 00:12:19,365 1 [0.         0.32328391 0.46421008 0.44734094 0.60663046 0.17542861
 0.35827844 0.29693033] 0.3340128453032666
2025-01-08 00:12:19,365 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-08 00:12:19,474 Loss: 1.723, MeanIU:  0.3340, Best_mIoU:  0.3385
2025-01-08 00:12:19,474 [0.         0.32328391 0.46421008 0.44734094 0.60663046 0.17542861
 0.35827844 0.29693033]
2025-01-08 00:12:20,105 Epoch: [19/20] Iter:[0/192], Time: 0.54, lr: [0.0006746414238367822], Loss: 2.266213, Acc:0.652201, Semantic loss: 0.129038, BCE loss: 2.053730, SB loss: 0.083446
2025-01-08 00:12:26,188 Epoch: [19/20] Iter:[10/192], Time: 0.60, lr: [0.0006429336362339898], Loss: 1.685392, Acc:0.629876, Semantic loss: 0.131378, BCE loss: 1.464826, SB loss: 0.089187
2025-01-08 00:12:32,250 Epoch: [19/20] Iter:[20/192], Time: 0.60, lr: [0.0006110510578510947], Loss: 1.744684, Acc:0.629923, Semantic loss: 0.133699, BCE loss: 1.521795, SB loss: 0.089190
2025-01-08 00:12:38,343 Epoch: [19/20] Iter:[30/192], Time: 0.61, lr: [0.0005789824653018995], Loss: 1.636365, Acc:0.614622, Semantic loss: 0.138887, BCE loss: 1.405289, SB loss: 0.092189
2025-01-08 00:12:44,418 Epoch: [19/20] Iter:[40/192], Time: 0.61, lr: [0.0005467151732202777], Loss: 1.632847, Acc:0.617428, Semantic loss: 0.137320, BCE loss: 1.403102, SB loss: 0.092425
2025-01-08 00:12:50,450 Epoch: [19/20] Iter:[50/192], Time: 0.61, lr: [0.0005142347343351296], Loss: 1.653202, Acc:0.615408, Semantic loss: 0.138680, BCE loss: 1.421215, SB loss: 0.093306
2025-01-08 00:12:56,599 Epoch: [19/20] Iter:[60/192], Time: 0.61, lr: [0.0004815245523312483], Loss: 1.675188, Acc:0.614393, Semantic loss: 0.140586, BCE loss: 1.440676, SB loss: 0.093926
2025-01-08 00:13:02,679 Epoch: [19/20] Iter:[70/192], Time: 0.61, lr: [0.000448565373510549], Loss: 1.682782, Acc:0.616817, Semantic loss: 0.139471, BCE loss: 1.450175, SB loss: 0.093135
2025-01-08 00:13:08,771 Epoch: [19/20] Iter:[80/192], Time: 0.61, lr: [0.00041533460609889997], Loss: 1.673055, Acc:0.611902, Semantic loss: 0.140478, BCE loss: 1.439361, SB loss: 0.093216
2025-01-08 00:13:14,878 Epoch: [19/20] Iter:[90/192], Time: 0.61, lr: [0.00038180538785330436], Loss: 1.668584, Acc:0.615973, Semantic loss: 0.139584, BCE loss: 1.435973, SB loss: 0.093026
2025-01-08 00:13:21,001 Epoch: [19/20] Iter:[100/192], Time: 0.61, lr: [0.00034794527452517863], Loss: 1.679573, Acc:0.617838, Semantic loss: 0.139859, BCE loss: 1.446314, SB loss: 0.093400
2025-01-08 00:13:27,041 Epoch: [19/20] Iter:[110/192], Time: 0.61, lr: [0.00031371433588231993], Loss: 1.674197, Acc:0.620354, Semantic loss: 0.138598, BCE loss: 1.442914, SB loss: 0.092684
2025-01-08 00:13:33,133 Epoch: [19/20] Iter:[120/192], Time: 0.61, lr: [0.0002790622842837163], Loss: 1.676127, Acc:0.619910, Semantic loss: 0.139610, BCE loss: 1.442874, SB loss: 0.093643
2025-01-08 00:13:39,199 Epoch: [19/20] Iter:[130/192], Time: 0.61, lr: [0.0002439239356353784], Loss: 1.661687, Acc:0.624069, Semantic loss: 0.138761, BCE loss: 1.430240, SB loss: 0.092686
2025-01-08 00:13:45,367 Epoch: [19/20] Iter:[140/192], Time: 0.61, lr: [0.00020821159321002047], Loss: 1.655429, Acc:0.626190, Semantic loss: 0.138143, BCE loss: 1.424691, SB loss: 0.092594
2025-01-08 00:13:51,450 Epoch: [19/20] Iter:[150/192], Time: 0.61, lr: [0.00017180122628828942], Loss: 1.640302, Acc:0.625720, Semantic loss: 0.138524, BCE loss: 1.408972, SB loss: 0.092806
2025-01-08 00:13:57,587 Epoch: [19/20] Iter:[160/192], Time: 0.61, lr: [0.00013450451987183818], Loss: 1.629445, Acc:0.626572, Semantic loss: 0.138689, BCE loss: 1.397564, SB loss: 0.093192
2025-01-08 00:14:03,616 Epoch: [19/20] Iter:[170/192], Time: 0.61, lr: [9.600244875192536e-05], Loss: 1.621340, Acc:0.626140, Semantic loss: 0.138276, BCE loss: 1.390024, SB loss: 0.093040
2025-01-08 00:14:09,748 Epoch: [19/20] Iter:[180/192], Time: 0.61, lr: [5.563716848048335e-05], Loss: 1.626507, Acc:0.626476, Semantic loss: 0.138387, BCE loss: 1.394945, SB loss: 0.093174
2025-01-08 00:14:15,824 Epoch: [19/20] Iter:[190/192], Time: 0.61, lr: [1.1092486125349496e-05], Loss: 1.629223, Acc:0.624533, Semantic loss: 0.138596, BCE loss: 1.397298, SB loss: 0.093329
2025-01-08 00:16:27,769 0 [0.         0.30139214 0.36569836 0.2389493  0.51962377 0.10818478
 0.38534741 0.00129887] 0.24006182770920892
2025-01-08 00:16:27,770 1 [0.         0.31496327 0.48065188 0.45128885 0.60783748 0.17692641
 0.32953216 0.30416164] 0.33317021149007
2025-01-08 00:16:27,771 => saving checkpoint to output\loveDa\pidnet_small_loveda_traincheckpoint.pth.tar
2025-01-08 00:16:27,881 Loss: 1.731, MeanIU:  0.3332, Best_mIoU:  0.3385
2025-01-08 00:16:27,882 [0.         0.31496327 0.48065188 0.45128885 0.60783748 0.17692641
 0.32953216 0.30416164]
2025-01-08 00:16:27,945 Hours: 1
2025-01-08 00:16:27,946 Done
