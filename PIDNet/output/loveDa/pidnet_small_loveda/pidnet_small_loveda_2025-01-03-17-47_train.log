2025-01-03 17:47:14,057 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 17:47:14,059 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.9
  SB_WEIGHTS: 1.0
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 17:47:14,259 Attention!!!
2025-01-03 17:47:14,259 Loaded 302 parameters!
2025-01-03 17:47:14,260 Over!!!
2025-01-03 17:47:34,450 Epoch: [0/20] Iter:[0/227], Time: 18.63, lr: [0.01], Loss: 2.237046, Acc:0.139805, Semantic loss: 0.000000, BCE loss: 2.237047, SB loss: -0.000001
2025-01-03 17:47:41,137 Epoch: [0/20] Iter:[10/227], Time: 2.27, lr: [0.009980174026451818], Loss: 1.714776, Acc:0.100572, Semantic loss: 0.000000, BCE loss: 1.714776, SB loss: 0.000000
2025-01-03 17:47:47,674 Epoch: [0/20] Iter:[20/227], Time: 1.50, lr: [0.009960343675822199], Loss: 1.387854, Acc:0.111431, Semantic loss: 0.000000, BCE loss: 1.387854, SB loss: 0.000000
2025-01-03 17:47:54,005 Epoch: [0/20] Iter:[30/227], Time: 1.22, lr: [0.009940508937457754], Loss: 1.236042, Acc:0.115515, Semantic loss: 0.000000, BCE loss: 1.236042, SB loss: 0.000000
2025-01-03 17:48:00,256 Epoch: [0/20] Iter:[40/227], Time: 1.08, lr: [0.009920669800655492], Loss: 1.186552, Acc:0.120141, Semantic loss: 0.000000, BCE loss: 1.186552, SB loss: 0.000000
2025-01-03 17:48:06,451 Epoch: [0/20] Iter:[50/227], Time: 0.99, lr: [0.009900826254662456], Loss: 1.141373, Acc:0.122684, Semantic loss: 0.000000, BCE loss: 1.141373, SB loss: 0.000000
2025-01-03 17:48:13,135 Epoch: [0/20] Iter:[60/227], Time: 0.94, lr: [0.009880978288675405], Loss: 1.119482, Acc:0.122183, Semantic loss: 0.000000, BCE loss: 1.119482, SB loss: 0.000000
2025-01-03 17:48:19,479 Epoch: [0/20] Iter:[70/227], Time: 0.89, lr: [0.009861125891840445], Loss: 1.111834, Acc:0.123860, Semantic loss: 0.000000, BCE loss: 1.111834, SB loss: 0.000000
2025-01-03 17:48:26,418 Epoch: [0/20] Iter:[80/227], Time: 0.87, lr: [0.009841269053252683], Loss: 1.103306, Acc:0.126063, Semantic loss: 0.000000, BCE loss: 1.103306, SB loss: 0.000000
2025-01-03 17:48:32,686 Epoch: [0/20] Iter:[90/227], Time: 0.84, lr: [0.00982140776195588], Loss: 1.102497, Acc:0.127002, Semantic loss: 0.000000, BCE loss: 1.102497, SB loss: 0.000000
2025-01-03 17:48:38,834 Epoch: [0/20] Iter:[100/227], Time: 0.82, lr: [0.009801542006942082], Loss: 1.089334, Acc:0.128050, Semantic loss: 0.000000, BCE loss: 1.089334, SB loss: 0.000000
2025-01-03 17:48:45,163 Epoch: [0/20] Iter:[110/227], Time: 0.80, lr: [0.009781671777151266], Loss: 1.074719, Acc:0.127364, Semantic loss: 0.000000, BCE loss: 1.074719, SB loss: 0.000000
2025-01-03 17:48:51,399 Epoch: [0/20] Iter:[120/227], Time: 0.79, lr: [0.009761797061470977], Loss: 1.061286, Acc:0.129470, Semantic loss: 0.000000, BCE loss: 1.061286, SB loss: 0.000000
2025-01-03 17:48:57,740 Epoch: [0/20] Iter:[130/227], Time: 0.78, lr: [0.009741917848735952], Loss: 1.055202, Acc:0.129166, Semantic loss: 0.000000, BCE loss: 1.055202, SB loss: 0.000000
2025-01-03 17:49:04,094 Epoch: [0/20] Iter:[140/227], Time: 0.77, lr: [0.009722034127727758], Loss: 1.047992, Acc:0.128155, Semantic loss: 0.000000, BCE loss: 1.047992, SB loss: 0.000000
2025-01-03 17:49:10,441 Epoch: [0/20] Iter:[150/227], Time: 0.76, lr: [0.009702145887174411], Loss: 1.039556, Acc:0.128716, Semantic loss: 0.000000, BCE loss: 1.039556, SB loss: 0.000000
2025-01-03 17:49:16,876 Epoch: [0/20] Iter:[160/227], Time: 0.75, lr: [0.009682253115750003], Loss: 1.041378, Acc:0.129218, Semantic loss: 0.000000, BCE loss: 1.041378, SB loss: 0.000000
2025-01-03 17:49:23,054 Epoch: [0/20] Iter:[170/227], Time: 0.74, lr: [0.009662355802074312], Loss: 1.048533, Acc:0.128509, Semantic loss: 0.000000, BCE loss: 1.048533, SB loss: 0.000000
2025-01-03 17:49:29,280 Epoch: [0/20] Iter:[180/227], Time: 0.74, lr: [0.00964245393471243], Loss: 1.046217, Acc:0.127847, Semantic loss: 0.000000, BCE loss: 1.046217, SB loss: 0.000000
2025-01-03 17:49:35,570 Epoch: [0/20] Iter:[190/227], Time: 0.73, lr: [0.009622547502174356], Loss: 1.043422, Acc:0.128091, Semantic loss: 0.000000, BCE loss: 1.043422, SB loss: 0.000000
2025-01-03 17:49:41,972 Epoch: [0/20] Iter:[200/227], Time: 0.73, lr: [0.009602636492914622], Loss: 1.035686, Acc:0.128561, Semantic loss: 0.000000, BCE loss: 1.035686, SB loss: 0.000000
2025-01-03 17:49:48,529 Epoch: [0/20] Iter:[210/227], Time: 0.72, lr: [0.009582720895331883], Loss: 1.030505, Acc:0.129374, Semantic loss: 0.000000, BCE loss: 1.030505, SB loss: 0.000000
2025-01-03 17:49:54,765 Epoch: [0/20] Iter:[220/227], Time: 0.72, lr: [0.009562800697768528], Loss: 1.026242, Acc:0.129952, Semantic loss: 0.000000, BCE loss: 1.026242, SB loss: 0.000000
2025-01-03 17:52:36,996 0 [0.00000000e+00 4.16730133e-01 0.00000000e+00 3.44137616e-03
 5.29511593e-06 1.25439717e-02 1.15420534e-02 6.46466958e-03] 0.056340937421265995
2025-01-03 17:52:36,996 1 [0.00000000e+00 8.50856831e-03 2.87287646e-05 2.03819956e-02
 1.14609565e-01 1.74538454e-02 7.78679057e-04 1.04314769e-03] 0.020350566266956513
2025-01-03 17:52:37,002 => saving checkpoint to output\loveDa\pidnet_small_lovedacheckpoint.pth.tar
2025-01-03 17:52:37,254 Loss: 1.046, MeanIU:  0.0204, Best_mIoU:  0.0204
2025-01-03 17:52:37,254 [0.00000000e+00 8.50856831e-03 2.87287646e-05 2.03819956e-02
 1.14609565e-01 1.74538454e-02 7.78679057e-04 1.04314769e-03]
2025-01-03 17:52:53,455 Epoch: [1/20] Iter:[0/227], Time: 15.40, lr: [0.009548853816214998], Loss: 1.274575, Acc:0.148606, Semantic loss: 0.000000, BCE loss: 1.274575, SB loss: -0.000000
2025-01-03 17:53:02,182 Epoch: [1/20] Iter:[10/227], Time: 2.23, lr: [0.009528925771769746], Loss: 0.875617, Acc:0.134128, Semantic loss: 0.000000, BCE loss: 0.875617, SB loss: -0.000000
2025-01-03 17:53:10,414 Epoch: [1/20] Iter:[20/227], Time: 1.56, lr: [0.009508993095584064], Loss: 0.930633, Acc:0.130343, Semantic loss: 0.000000, BCE loss: 0.930633, SB loss: 0.000000
2025-01-03 17:53:18,694 Epoch: [1/20] Iter:[30/227], Time: 1.32, lr: [0.009489055775788591], Loss: 0.962532, Acc:0.136077, Semantic loss: 0.000000, BCE loss: 0.962532, SB loss: 0.000000
2025-01-03 17:53:26,949 Epoch: [1/20] Iter:[40/227], Time: 1.20, lr: [0.009469113800455762], Loss: 0.982422, Acc:0.130555, Semantic loss: 0.000000, BCE loss: 0.982422, SB loss: 0.000000
2025-01-03 17:53:35,239 Epoch: [1/20] Iter:[50/227], Time: 1.13, lr: [0.009449167157599386], Loss: 0.980576, Acc:0.128901, Semantic loss: 0.000000, BCE loss: 0.980576, SB loss: 0.000000
2025-01-03 17:53:43,497 Epoch: [1/20] Iter:[60/227], Time: 1.08, lr: [0.00942921583517422], Loss: 0.990269, Acc:0.128992, Semantic loss: 0.000000, BCE loss: 0.990268, SB loss: 0.000000
2025-01-03 17:53:51,738 Epoch: [1/20] Iter:[70/227], Time: 1.04, lr: [0.009409259821075532], Loss: 0.986805, Acc:0.129313, Semantic loss: 0.000000, BCE loss: 0.986805, SB loss: 0.000000
