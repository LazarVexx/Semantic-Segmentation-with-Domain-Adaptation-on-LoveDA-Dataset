2025-01-03 19:01:50,172 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 19:01:50,172 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 65536
  OHEMTHRES: 0.7
  SB_WEIGHTS: 1.0
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 19:01:50,323 Attention!!!
2025-01-03 19:01:50,324 Loaded 302 parameters!
2025-01-03 19:01:50,324 Over!!!
2025-01-03 19:02:15,125 Epoch: [0/20] Iter:[0/227], Time: 23.41, lr: [0.01], Loss: 1.907756, Acc:0.155077, Semantic loss: 0.000000, BCE loss: 1.907756, SB loss: 0.000001
2025-01-03 19:02:49,790 Epoch: [0/20] Iter:[10/227], Time: 5.14, lr: [0.009980174026451818], Loss: 1.866259, Acc:0.088682, Semantic loss: 0.000000, BCE loss: 1.866259, SB loss: 0.000000
2025-01-03 19:03:27,752 Epoch: [0/20] Iter:[20/227], Time: 4.47, lr: [0.009960343675822199], Loss: 1.500786, Acc:0.087745, Semantic loss: 0.000000, BCE loss: 1.500786, SB loss: -0.000000
2025-01-03 19:04:07,741 Epoch: [0/20] Iter:[30/227], Time: 4.31, lr: [0.009940508937457754], Loss: 1.376741, Acc:0.084730, Semantic loss: 0.000000, BCE loss: 1.376741, SB loss: 0.000000
2025-01-03 19:04:47,390 Epoch: [0/20] Iter:[40/227], Time: 4.22, lr: [0.009920669800655492], Loss: 1.330698, Acc:0.083875, Semantic loss: 0.000000, BCE loss: 1.330698, SB loss: 0.000000
2025-01-03 19:05:25,607 Epoch: [0/20] Iter:[50/227], Time: 4.16, lr: [0.009900826254662456], Loss: 1.293532, Acc:0.086235, Semantic loss: 0.000000, BCE loss: 1.293532, SB loss: 0.000000
2025-01-03 19:06:05,604 Epoch: [0/20] Iter:[60/227], Time: 4.13, lr: [0.009880978288675405], Loss: 1.275636, Acc:0.085361, Semantic loss: 0.000000, BCE loss: 1.275636, SB loss: 0.000000
2025-01-03 19:06:43,032 Epoch: [0/20] Iter:[70/227], Time: 4.07, lr: [0.009861125891840445], Loss: 1.261230, Acc:0.088235, Semantic loss: 0.000000, BCE loss: 1.261230, SB loss: 0.000000
2025-01-03 19:07:52,581 Epoch: [0/20] Iter:[80/227], Time: 4.37, lr: [0.009841269053252683], Loss: 1.268217, Acc:0.088611, Semantic loss: 0.000000, BCE loss: 1.268217, SB loss: 0.000000
2025-01-03 19:09:04,529 Epoch: [0/20] Iter:[90/227], Time: 4.68, lr: [0.00982140776195588], Loss: 1.245516, Acc:0.087139, Semantic loss: 0.000000, BCE loss: 1.245516, SB loss: 0.000000
2025-01-03 19:10:06,075 Epoch: [0/20] Iter:[100/227], Time: 4.88, lr: [0.009801542006942082], Loss: 1.237176, Acc:0.087271, Semantic loss: 0.000000, BCE loss: 1.237176, SB loss: 0.000000
2025-01-03 19:11:09,508 Epoch: [0/20] Iter:[110/227], Time: 5.01, lr: [0.009781671777151266], Loss: 1.229452, Acc:0.086498, Semantic loss: 0.000000, BCE loss: 1.229451, SB loss: 0.000000
2025-01-03 19:12:13,644 Epoch: [0/20] Iter:[120/227], Time: 5.12, lr: [0.009761797061470977], Loss: 1.226670, Acc:0.087324, Semantic loss: 0.000000, BCE loss: 1.226670, SB loss: 0.000000
2025-01-03 19:13:26,491 Epoch: [0/20] Iter:[130/227], Time: 5.26, lr: [0.009741917848735952], Loss: 1.217927, Acc:0.086970, Semantic loss: 0.000000, BCE loss: 1.217927, SB loss: 0.000000
2025-01-03 19:14:31,186 Epoch: [0/20] Iter:[140/227], Time: 5.34, lr: [0.009722034127727758], Loss: 1.210667, Acc:0.086518, Semantic loss: 0.000000, BCE loss: 1.210667, SB loss: 0.000000
2025-01-03 19:15:36,150 Epoch: [0/20] Iter:[150/227], Time: 5.45, lr: [0.009702145887174411], Loss: 1.200058, Acc:0.087052, Semantic loss: 0.000000, BCE loss: 1.200058, SB loss: 0.000000
2025-01-03 19:16:37,202 Epoch: [0/20] Iter:[160/227], Time: 5.49, lr: [0.009682253115750003], Loss: 1.197488, Acc:0.087273, Semantic loss: 0.000000, BCE loss: 1.197488, SB loss: 0.000000
2025-01-03 19:17:48,875 Epoch: [0/20] Iter:[170/227], Time: 5.56, lr: [0.009662355802074312], Loss: 1.196817, Acc:0.086920, Semantic loss: 0.000000, BCE loss: 1.196817, SB loss: 0.000000
2025-01-03 19:18:55,881 Epoch: [0/20] Iter:[180/227], Time: 5.65, lr: [0.00964245393471243], Loss: 1.194171, Acc:0.086905, Semantic loss: 0.000000, BCE loss: 1.194171, SB loss: 0.000000
2025-01-03 19:19:52,889 Epoch: [0/20] Iter:[190/227], Time: 5.62, lr: [0.009622547502174356], Loss: 1.187937, Acc:0.086531, Semantic loss: 0.000000, BCE loss: 1.187937, SB loss: 0.000000
2025-01-03 19:20:49,609 Epoch: [0/20] Iter:[200/227], Time: 5.63, lr: [0.009602636492914622], Loss: 1.183763, Acc:0.086676, Semantic loss: 0.000000, BCE loss: 1.183763, SB loss: 0.000000
2025-01-03 19:21:49,581 Epoch: [0/20] Iter:[210/227], Time: 5.67, lr: [0.009582720895331883], Loss: 1.176558, Acc:0.087177, Semantic loss: 0.000000, BCE loss: 1.176557, SB loss: 0.000000
2025-01-03 19:22:58,377 Epoch: [0/20] Iter:[220/227], Time: 5.70, lr: [0.009562800697768528], Loss: 1.174752, Acc:0.087333, Semantic loss: 0.000000, BCE loss: 1.174751, SB loss: 0.000000
2025-01-03 19:26:55,410 0 [0.00000000e+00 4.16913286e-01 0.00000000e+00 5.44007817e-03
 3.17711402e-06 1.25439378e-02 1.15377155e-02 2.78623267e-03] 0.05615305343971358
2025-01-03 19:26:55,411 1 [0.         0.00031759 0.02833179 0.02435863 0.09983362 0.02724624
 0.00156614 0.        ] 0.02270674941430909
2025-01-03 19:26:55,416 => saving checkpoint to output\loveDa\pidnet_small_lovedacheckpoint.pth.tar
2025-01-03 19:26:55,833 Loss: 1.058, MeanIU:  0.0227, Best_mIoU:  0.0227
2025-01-03 19:26:55,833 [0.         0.00031759 0.02833179 0.02435863 0.09983362 0.02724624
 0.00156614 0.        ]
2025-01-03 19:27:12,881 Epoch: [1/20] Iter:[0/227], Time: 16.74, lr: [0.009548853816214998], Loss: 1.406818, Acc:0.100116, Semantic loss: 0.000000, BCE loss: 1.406819, SB loss: -0.000000
2025-01-03 19:27:23,401 Epoch: [1/20] Iter:[10/227], Time: 2.45, lr: [0.009528925771769746], Loss: 1.179841, Acc:0.083247, Semantic loss: 0.000000, BCE loss: 1.179841, SB loss: 0.000000
2025-01-03 19:27:33,897 Epoch: [1/20] Iter:[20/227], Time: 1.79, lr: [0.009508993095584064], Loss: 1.173294, Acc:0.088005, Semantic loss: 0.000000, BCE loss: 1.173293, SB loss: 0.000000
2025-01-03 19:27:44,408 Epoch: [1/20] Iter:[30/227], Time: 1.55, lr: [0.009489055775788591], Loss: 1.193782, Acc:0.093437, Semantic loss: 0.000000, BCE loss: 1.193782, SB loss: 0.000000
2025-01-03 19:27:54,790 Epoch: [1/20] Iter:[40/227], Time: 1.43, lr: [0.009469113800455762], Loss: 1.200184, Acc:0.092961, Semantic loss: 0.000000, BCE loss: 1.200184, SB loss: 0.000000
2025-01-03 19:28:05,288 Epoch: [1/20] Iter:[50/227], Time: 1.35, lr: [0.009449167157599386], Loss: 1.192880, Acc:0.091024, Semantic loss: 0.000000, BCE loss: 1.192880, SB loss: 0.000000
2025-01-03 19:28:15,721 Epoch: [1/20] Iter:[60/227], Time: 1.30, lr: [0.00942921583517422], Loss: 1.183819, Acc:0.091107, Semantic loss: 0.000000, BCE loss: 1.183819, SB loss: 0.000000
2025-01-03 19:28:26,413 Epoch: [1/20] Iter:[70/227], Time: 1.27, lr: [0.009409259821075532], Loss: 1.187115, Acc:0.092259, Semantic loss: 0.000000, BCE loss: 1.187115, SB loss: 0.000000
2025-01-03 19:28:37,144 Epoch: [1/20] Iter:[80/227], Time: 1.24, lr: [0.009389299103138684], Loss: 1.165623, Acc:0.089201, Semantic loss: 0.000000, BCE loss: 1.165623, SB loss: 0.000000
