2025-01-03 19:51:43,448 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 19:51:43,448 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 19:51:43,686 Attention!!!
2025-01-03 19:51:43,686 Loaded 302 parameters!
2025-01-03 19:51:43,686 Over!!!
2025-01-03 19:52:05,885 Epoch: [0/20] Iter:[0/192], Time: 20.10, lr: [0.01], Loss: 2.769941, Acc:0.174589, Semantic loss: 0.000000, BCE loss: 2.769940, SB loss: 0.000001
2025-01-03 19:52:24,974 Epoch: [0/20] Iter:[10/192], Time: 3.51, lr: [0.009976559445324192], Loss: 2.068751, Acc:0.207033, Semantic loss: 0.000000, BCE loss: 2.068751, SB loss: 0.000000
2025-01-03 19:52:40,281 Epoch: [0/20] Iter:[20/192], Time: 2.58, lr: [0.009953112769592761], Loss: 1.684163, Acc:0.197661, Semantic loss: 0.000000, BCE loss: 1.684162, SB loss: 0.000000
2025-01-03 19:53:01,686 Epoch: [0/20] Iter:[30/192], Time: 2.44, lr: [0.009929659955177281], Loss: 1.615812, Acc:0.196128, Semantic loss: 0.000000, BCE loss: 1.615812, SB loss: 0.000000
2025-01-03 19:53:16,623 Epoch: [0/20] Iter:[40/192], Time: 2.21, lr: [0.009906200984352154], Loss: 1.573022, Acc:0.193802, Semantic loss: 0.000000, BCE loss: 1.573022, SB loss: 0.000000
2025-01-03 19:53:31,490 Epoch: [0/20] Iter:[50/192], Time: 2.07, lr: [0.009882735839293803], Loss: 1.528372, Acc:0.192315, Semantic loss: 0.000000, BCE loss: 1.528372, SB loss: 0.000000
2025-01-03 19:53:45,658 Epoch: [0/20] Iter:[60/192], Time: 1.96, lr: [0.00985926450207989], Loss: 1.490782, Acc:0.188160, Semantic loss: 0.000000, BCE loss: 1.490781, SB loss: 0.000000
2025-01-03 19:54:01,707 Epoch: [0/20] Iter:[70/192], Time: 1.90, lr: [0.009835786954688485], Loss: 1.484293, Acc:0.185924, Semantic loss: 0.000000, BCE loss: 1.484293, SB loss: 0.000000
2025-01-03 19:54:17,466 Epoch: [0/20] Iter:[80/192], Time: 1.87, lr: [0.00981230317899726], Loss: 1.481583, Acc:0.186868, Semantic loss: 0.000000, BCE loss: 1.481583, SB loss: 0.000000
2025-01-03 19:54:32,537 Epoch: [0/20] Iter:[90/192], Time: 1.83, lr: [0.009788813156782662], Loss: 1.502064, Acc:0.188004, Semantic loss: 0.000000, BCE loss: 1.502064, SB loss: 0.000000
2025-01-03 19:54:48,622 Epoch: [0/20] Iter:[100/192], Time: 1.80, lr: [0.009765316869719067], Loss: 1.505588, Acc:0.187446, Semantic loss: 0.000000, BCE loss: 1.505588, SB loss: 0.000000
2025-01-03 19:55:06,226 Epoch: [0/20] Iter:[110/192], Time: 1.79, lr: [0.009741814299377942], Loss: 1.492243, Acc:0.186748, Semantic loss: 0.000000, BCE loss: 1.492242, SB loss: 0.000000
2025-01-03 19:55:26,774 Epoch: [0/20] Iter:[120/192], Time: 1.82, lr: [0.009718305427226986], Loss: 1.484178, Acc:0.186781, Semantic loss: 0.000000, BCE loss: 1.484178, SB loss: 0.000000
2025-01-03 19:55:53,793 Epoch: [0/20] Iter:[130/192], Time: 1.88, lr: [0.009694790234629266], Loss: 1.489222, Acc:0.186078, Semantic loss: 0.000000, BCE loss: 1.489222, SB loss: 0.000000
2025-01-03 19:56:21,804 Epoch: [0/20] Iter:[140/192], Time: 1.94, lr: [0.009671268702842338], Loss: 1.477696, Acc:0.184840, Semantic loss: 0.000000, BCE loss: 1.477696, SB loss: 0.000000
2025-01-03 19:56:58,688 Epoch: [0/20] Iter:[150/192], Time: 2.07, lr: [0.009647740813017376], Loss: 1.471555, Acc:0.183639, Semantic loss: 0.000000, BCE loss: 1.471555, SB loss: 0.000000
2025-01-03 19:57:28,868 Epoch: [0/20] Iter:[160/192], Time: 2.12, lr: [0.009624206546198262], Loss: 1.470285, Acc:0.183657, Semantic loss: 0.000000, BCE loss: 1.470285, SB loss: 0.000000
2025-01-03 19:58:08,000 Epoch: [0/20] Iter:[170/192], Time: 2.22, lr: [0.009600665883320689], Loss: 1.473298, Acc:0.183600, Semantic loss: 0.000000, BCE loss: 1.473298, SB loss: 0.000000
2025-01-03 19:58:35,171 Epoch: [0/20] Iter:[180/192], Time: 2.26, lr: [0.009577118805211254], Loss: 1.468875, Acc:0.183060, Semantic loss: 0.000000, BCE loss: 1.468875, SB loss: 0.000000
2025-01-03 19:59:02,816 Epoch: [0/20] Iter:[190/192], Time: 2.28, lr: [0.009553565292586523], Loss: 1.465115, Acc:0.183185, Semantic loss: 0.000000, BCE loss: 1.465115, SB loss: 0.000000
2025-01-03 20:01:05,086 0 [0.00000000e+00 2.54714115e-01 0.00000000e+00 7.27623819e-03
 1.76188523e-06 1.87701190e-02 1.40019868e-02 2.79488741e-03] 0.0371948885565577
2025-01-03 20:01:05,086 1 [0.00000000e+00 7.21956707e-02 9.85783888e-02 1.17534949e-02
 9.90673435e-02 3.25361752e-02 2.36474544e-02 4.67118844e-05] 0.04222815493042502
2025-01-03 20:01:05,098 => saving checkpoint to output\loveDa\pidnet_small_lovedacheckpoint.pth.tar
2025-01-03 20:01:05,305 Loss: 1.671, MeanIU:  0.0422, Best_mIoU:  0.0422
2025-01-03 20:01:05,305 [0.00000000e+00 7.21956707e-02 9.85783888e-02 1.17534949e-02
 9.90673435e-02 3.25361752e-02 2.36474544e-02 4.67118844e-05]
2025-01-03 20:01:21,470 Epoch: [1/20] Iter:[0/192], Time: 15.42, lr: [0.009548853816214998], Loss: 1.725514, Acc:0.195904, Semantic loss: 0.000000, BCE loss: 1.725515, SB loss: -0.000000
2025-01-03 20:01:35,667 Epoch: [1/20] Iter:[10/192], Time: 2.69, lr: [0.009525292556561479], Loss: 1.452354, Acc:0.176899, Semantic loss: 0.000000, BCE loss: 1.452354, SB loss: -0.000000
2025-01-03 20:01:49,633 Epoch: [1/20] Iter:[20/192], Time: 2.09, lr: [0.00950172481957719], Loss: 1.373199, Acc:0.171503, Semantic loss: 0.000000, BCE loss: 1.373199, SB loss: 0.000000
2025-01-03 20:02:04,779 Epoch: [1/20] Iter:[30/192], Time: 1.90, lr: [0.009478150585620286], Loss: 1.343391, Acc:0.171439, Semantic loss: 0.000000, BCE loss: 1.343391, SB loss: 0.000000
2025-01-03 20:02:19,157 Epoch: [1/20] Iter:[40/192], Time: 1.77, lr: [0.009454569834934885], Loss: 1.363690, Acc:0.173341, Semantic loss: 0.000000, BCE loss: 1.363690, SB loss: 0.000000
2025-01-03 20:02:33,597 Epoch: [1/20] Iter:[50/192], Time: 1.71, lr: [0.009430982547650114], Loss: 1.373343, Acc:0.174392, Semantic loss: 0.000000, BCE loss: 1.373343, SB loss: 0.000000
2025-01-03 20:02:47,853 Epoch: [1/20] Iter:[60/192], Time: 1.67, lr: [0.009407388703779091], Loss: 1.386425, Acc:0.174079, Semantic loss: 0.000000, BCE loss: 1.386425, SB loss: 0.000000
2025-01-03 20:03:01,941 Epoch: [1/20] Iter:[70/192], Time: 1.64, lr: [0.009383788283217955], Loss: 1.355386, Acc:0.172135, Semantic loss: 0.000000, BCE loss: 1.355386, SB loss: 0.000000
2025-01-03 20:03:15,900 Epoch: [1/20] Iter:[80/192], Time: 1.61, lr: [0.00936018126574482], Loss: 1.361011, Acc:0.170232, Semantic loss: 0.000000, BCE loss: 1.361011, SB loss: 0.000000
2025-01-03 20:03:29,084 Epoch: [1/20] Iter:[90/192], Time: 1.57, lr: [0.009336567631018769], Loss: 1.367740, Acc:0.170809, Semantic loss: 0.000000, BCE loss: 1.367740, SB loss: 0.000000
2025-01-03 20:03:42,575 Epoch: [1/20] Iter:[100/192], Time: 1.55, lr: [0.009312947358578814], Loss: 1.385062, Acc:0.172437, Semantic loss: 0.000000, BCE loss: 1.385062, SB loss: 0.000000
2025-01-03 20:03:56,741 Epoch: [1/20] Iter:[110/192], Time: 1.53, lr: [0.009289320427842841], Loss: 1.390068, Acc:0.174290, Semantic loss: 0.000000, BCE loss: 1.390068, SB loss: 0.000000
2025-01-03 20:04:11,499 Epoch: [1/20] Iter:[120/192], Time: 1.53, lr: [0.009265686818106552], Loss: 1.389457, Acc:0.173774, Semantic loss: 0.000000, BCE loss: 1.389457, SB loss: 0.000000
