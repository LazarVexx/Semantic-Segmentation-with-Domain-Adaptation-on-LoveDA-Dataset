2025-01-03 20:41:53,645 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 20:41:53,645 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 2
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 20:41:53,798 Attention!!!
2025-01-03 20:41:53,798 Loaded 302 parameters!
2025-01-03 20:41:53,798 Over!!!
2025-01-03 20:42:14,030 Epoch: [0/20] Iter:[0/192], Time: 18.88, lr: [0.01], Loss: 2.769938, Acc:0.174611, Semantic loss: 0.000000, BCE loss: 2.769938, SB loss: -0.000000
2025-01-03 20:42:20,606 Epoch: [0/20] Iter:[10/192], Time: 2.29, lr: [0.009976559445324192], Loss: 2.088495, Acc:0.208179, Semantic loss: 0.000000, BCE loss: 2.088495, SB loss: -0.000000
2025-01-03 20:42:27,066 Epoch: [0/20] Iter:[20/192], Time: 1.51, lr: [0.009953112769592761], Loss: 1.707585, Acc:0.201980, Semantic loss: 0.000000, BCE loss: 1.707585, SB loss: -0.000000
2025-01-03 20:42:33,484 Epoch: [0/20] Iter:[30/192], Time: 1.23, lr: [0.009929659955177281], Loss: 1.631643, Acc:0.204456, Semantic loss: 0.000000, BCE loss: 1.631643, SB loss: -0.000000
2025-01-03 20:42:39,723 Epoch: [0/20] Iter:[40/192], Time: 1.08, lr: [0.009906200984352154], Loss: 1.585417, Acc:0.199124, Semantic loss: 0.000000, BCE loss: 1.585417, SB loss: -0.000000
2025-01-03 20:42:46,035 Epoch: [0/20] Iter:[50/192], Time: 0.99, lr: [0.009882735839293803], Loss: 1.537307, Acc:0.197321, Semantic loss: 0.000000, BCE loss: 1.537307, SB loss: -0.000000
2025-01-03 20:42:52,260 Epoch: [0/20] Iter:[60/192], Time: 0.93, lr: [0.00985926450207989], Loss: 1.497775, Acc:0.191987, Semantic loss: 0.000000, BCE loss: 1.497775, SB loss: -0.000000
2025-01-03 20:42:58,578 Epoch: [0/20] Iter:[70/192], Time: 0.89, lr: [0.009835786954688485], Loss: 1.489797, Acc:0.189502, Semantic loss: 0.000000, BCE loss: 1.489797, SB loss: 0.000000
2025-01-03 20:43:05,000 Epoch: [0/20] Iter:[80/192], Time: 0.86, lr: [0.00981230317899726], Loss: 1.486300, Acc:0.189964, Semantic loss: 0.000000, BCE loss: 1.486300, SB loss: 0.000000
2025-01-03 20:43:11,226 Epoch: [0/20] Iter:[90/192], Time: 0.83, lr: [0.009788813156782662], Loss: 1.506049, Acc:0.190457, Semantic loss: 0.000000, BCE loss: 1.506049, SB loss: 0.000000
2025-01-03 20:43:17,490 Epoch: [0/20] Iter:[100/192], Time: 0.81, lr: [0.009765316869719067], Loss: 1.509085, Acc:0.188874, Semantic loss: 0.000000, BCE loss: 1.509085, SB loss: 0.000000
2025-01-03 20:43:23,801 Epoch: [0/20] Iter:[110/192], Time: 0.80, lr: [0.009741814299377942], Loss: 1.495127, Acc:0.187495, Semantic loss: 0.000000, BCE loss: 1.495127, SB loss: 0.000000
2025-01-03 20:43:30,181 Epoch: [0/20] Iter:[120/192], Time: 0.78, lr: [0.009718305427226986], Loss: 1.486903, Acc:0.187242, Semantic loss: 0.000000, BCE loss: 1.486903, SB loss: 0.000000
2025-01-03 20:43:36,487 Epoch: [0/20] Iter:[130/192], Time: 0.77, lr: [0.009694790234629266], Loss: 1.491654, Acc:0.185624, Semantic loss: 0.000000, BCE loss: 1.491654, SB loss: 0.000000
2025-01-03 20:43:42,697 Epoch: [0/20] Iter:[140/192], Time: 0.76, lr: [0.009671268702842338], Loss: 1.479991, Acc:0.183978, Semantic loss: 0.000000, BCE loss: 1.479991, SB loss: -0.000000
2025-01-03 20:43:48,968 Epoch: [0/20] Iter:[150/192], Time: 0.75, lr: [0.009647740813017376], Loss: 1.473682, Acc:0.182168, Semantic loss: 0.000000, BCE loss: 1.473682, SB loss: -0.000000
2025-01-03 20:43:55,409 Epoch: [0/20] Iter:[160/192], Time: 0.75, lr: [0.009624206546198262], Loss: 1.472196, Acc:0.181413, Semantic loss: 0.000000, BCE loss: 1.472196, SB loss: -0.000000
2025-01-03 20:44:01,426 Epoch: [0/20] Iter:[170/192], Time: 0.74, lr: [0.009600665883320689], Loss: 1.474916, Acc:0.180701, Semantic loss: 0.000000, BCE loss: 1.474916, SB loss: -0.000000
2025-01-03 20:44:07,687 Epoch: [0/20] Iter:[180/192], Time: 0.73, lr: [0.009577118805211254], Loss: 1.470399, Acc:0.179487, Semantic loss: 0.000000, BCE loss: 1.470399, SB loss: -0.000000
2025-01-03 20:44:13,417 Epoch: [0/20] Iter:[190/192], Time: 0.72, lr: [0.009553565292586523], Loss: 1.466436, Acc:0.179273, Semantic loss: 0.000000, BCE loss: 1.466436, SB loss: 0.000000
