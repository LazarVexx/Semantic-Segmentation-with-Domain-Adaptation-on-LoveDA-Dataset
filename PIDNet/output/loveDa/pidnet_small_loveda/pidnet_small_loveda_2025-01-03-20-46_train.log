2025-01-03 20:46:58,431 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 20:46:58,432 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 10
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 20:46:58,618 Attention!!!
2025-01-03 20:46:58,618 Loaded 302 parameters!
2025-01-03 20:46:58,618 Over!!!
2025-01-03 20:47:18,891 Epoch: [0/20] Iter:[0/192], Time: 18.81, lr: [0.01], Loss: 2.769941, Acc:0.174589, Semantic loss: 0.000000, BCE loss: 2.769940, SB loss: 0.000001
2025-01-03 20:47:25,401 Epoch: [0/20] Iter:[10/192], Time: 2.28, lr: [0.009976559445324192], Loss: 2.078034, Acc:0.195961, Semantic loss: 0.000000, BCE loss: 2.078035, SB loss: -0.000000
2025-01-03 20:47:31,717 Epoch: [0/20] Iter:[20/192], Time: 1.49, lr: [0.009953112769592761], Loss: 1.689609, Acc:0.185772, Semantic loss: 0.000000, BCE loss: 1.689609, SB loss: 0.000000
2025-01-03 20:47:38,123 Epoch: [0/20] Iter:[30/192], Time: 1.22, lr: [0.009929659955177281], Loss: 1.620499, Acc:0.183448, Semantic loss: 0.000000, BCE loss: 1.620499, SB loss: 0.000000
2025-01-03 20:47:44,641 Epoch: [0/20] Iter:[40/192], Time: 1.08, lr: [0.009906200984352154], Loss: 1.576066, Acc:0.181863, Semantic loss: 0.000000, BCE loss: 1.576066, SB loss: 0.000000
2025-01-03 20:47:51,113 Epoch: [0/20] Iter:[50/192], Time: 1.00, lr: [0.009882735839293803], Loss: 1.530216, Acc:0.181746, Semantic loss: 0.000000, BCE loss: 1.530216, SB loss: 0.000000
2025-01-03 20:47:57,626 Epoch: [0/20] Iter:[60/192], Time: 0.94, lr: [0.00985926450207989], Loss: 1.491789, Acc:0.178083, Semantic loss: 0.000000, BCE loss: 1.491789, SB loss: 0.000000
2025-01-03 20:48:04,220 Epoch: [0/20] Iter:[70/192], Time: 0.90, lr: [0.009835786954688485], Loss: 1.484606, Acc:0.176205, Semantic loss: 0.000000, BCE loss: 1.484606, SB loss: 0.000000
2025-01-03 20:48:10,921 Epoch: [0/20] Iter:[80/192], Time: 0.87, lr: [0.00981230317899726], Loss: 1.481547, Acc:0.176355, Semantic loss: 0.000000, BCE loss: 1.481547, SB loss: 0.000000
2025-01-03 20:48:17,298 Epoch: [0/20] Iter:[90/192], Time: 0.85, lr: [0.009788813156782662], Loss: 1.501812, Acc:0.177701, Semantic loss: 0.000000, BCE loss: 1.501812, SB loss: 0.000000
2025-01-03 20:48:23,695 Epoch: [0/20] Iter:[100/192], Time: 0.82, lr: [0.009765316869719067], Loss: 1.505415, Acc:0.177448, Semantic loss: 0.000000, BCE loss: 1.505415, SB loss: 0.000000
