2025-01-03 20:48:58,147 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 20:48:58,148 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 4
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 20:48:58,305 Attention!!!
2025-01-03 20:48:58,306 Loaded 302 parameters!
2025-01-03 20:48:58,306 Over!!!
2025-01-03 20:49:18,138 Epoch: [0/20] Iter:[0/192], Time: 18.51, lr: [0.01], Loss: 2.769941, Acc:0.174614, Semantic loss: 0.000000, BCE loss: 2.769940, SB loss: 0.000001
2025-01-03 20:49:24,836 Epoch: [0/20] Iter:[10/192], Time: 2.26, lr: [0.009976559445324192], Loss: 2.085966, Acc:0.196990, Semantic loss: 0.000000, BCE loss: 2.085967, SB loss: -0.000000
2025-01-03 20:49:31,369 Epoch: [0/20] Iter:[20/192], Time: 1.50, lr: [0.009953112769592761], Loss: 1.695591, Acc:0.191641, Semantic loss: 0.000000, BCE loss: 1.695591, SB loss: -0.000000
2025-01-03 20:49:37,951 Epoch: [0/20] Iter:[30/192], Time: 1.23, lr: [0.009929659955177281], Loss: 1.621737, Acc:0.191295, Semantic loss: 0.000000, BCE loss: 1.621737, SB loss: -0.000000
2025-01-03 20:49:44,432 Epoch: [0/20] Iter:[40/192], Time: 1.09, lr: [0.009906200984352154], Loss: 1.575919, Acc:0.189913, Semantic loss: 0.000000, BCE loss: 1.575919, SB loss: 0.000000
2025-01-03 20:49:50,965 Epoch: [0/20] Iter:[50/192], Time: 1.00, lr: [0.009882735839293803], Loss: 1.529466, Acc:0.189936, Semantic loss: 0.000000, BCE loss: 1.529466, SB loss: 0.000000
2025-01-03 20:49:57,408 Epoch: [0/20] Iter:[60/192], Time: 0.94, lr: [0.00985926450207989], Loss: 1.491028, Acc:0.187579, Semantic loss: 0.000000, BCE loss: 1.491028, SB loss: 0.000000
2025-01-03 20:50:03,995 Epoch: [0/20] Iter:[70/192], Time: 0.90, lr: [0.009835786954688485], Loss: 1.484216, Acc:0.186258, Semantic loss: 0.000000, BCE loss: 1.484216, SB loss: 0.000000
2025-01-03 20:50:10,633 Epoch: [0/20] Iter:[80/192], Time: 0.87, lr: [0.00981230317899726], Loss: 1.481353, Acc:0.187332, Semantic loss: 0.000000, BCE loss: 1.481353, SB loss: 0.000000
2025-01-03 20:50:17,189 Epoch: [0/20] Iter:[90/192], Time: 0.85, lr: [0.009788813156782662], Loss: 1.501502, Acc:0.189890, Semantic loss: 0.000000, BCE loss: 1.501502, SB loss: 0.000000
2025-01-03 20:50:23,739 Epoch: [0/20] Iter:[100/192], Time: 0.83, lr: [0.009765316869719067], Loss: 1.504984, Acc:0.190452, Semantic loss: 0.000000, BCE loss: 1.504984, SB loss: 0.000000
2025-01-03 20:50:30,220 Epoch: [0/20] Iter:[110/192], Time: 0.81, lr: [0.009741814299377942], Loss: 1.491262, Acc:0.189896, Semantic loss: 0.000000, BCE loss: 1.491262, SB loss: 0.000000
2025-01-03 20:50:36,766 Epoch: [0/20] Iter:[120/192], Time: 0.80, lr: [0.009718305427226986], Loss: 1.483260, Acc:0.191245, Semantic loss: 0.000000, BCE loss: 1.483260, SB loss: 0.000000
2025-01-03 20:50:43,251 Epoch: [0/20] Iter:[130/192], Time: 0.79, lr: [0.009694790234629266], Loss: 1.488190, Acc:0.191180, Semantic loss: 0.000000, BCE loss: 1.488190, SB loss: 0.000000
2025-01-03 20:50:49,776 Epoch: [0/20] Iter:[140/192], Time: 0.78, lr: [0.009671268702842338], Loss: 1.476624, Acc:0.190455, Semantic loss: 0.000000, BCE loss: 1.476624, SB loss: 0.000000
2025-01-03 20:50:56,248 Epoch: [0/20] Iter:[150/192], Time: 0.77, lr: [0.009647740813017376], Loss: 1.470422, Acc:0.189500, Semantic loss: 0.000000, BCE loss: 1.470421, SB loss: 0.000000
2025-01-03 20:51:02,772 Epoch: [0/20] Iter:[160/192], Time: 0.76, lr: [0.009624206546198262], Loss: 1.469081, Acc:0.189351, Semantic loss: 0.000000, BCE loss: 1.469081, SB loss: 0.000000
2025-01-03 20:51:09,340 Epoch: [0/20] Iter:[170/192], Time: 0.76, lr: [0.009600665883320689], Loss: 1.472180, Acc:0.189240, Semantic loss: 0.000000, BCE loss: 1.472180, SB loss: 0.000000
2025-01-03 20:51:15,940 Epoch: [0/20] Iter:[180/192], Time: 0.75, lr: [0.009577118805211254], Loss: 1.468003, Acc:0.188678, Semantic loss: 0.000000, BCE loss: 1.468003, SB loss: 0.000000
2025-01-03 20:51:22,301 Epoch: [0/20] Iter:[190/192], Time: 0.75, lr: [0.009553565292586523], Loss: 1.464359, Acc:0.188795, Semantic loss: 0.000000, BCE loss: 1.464359, SB loss: 0.000000
2025-01-03 20:53:18,619 0 [0.00000000e+00 2.54790097e-01 0.00000000e+00 1.09574232e-04
 1.76188523e-06 1.79298020e-02 1.40022833e-02 1.00297591e-02] 0.03710790968506838
2025-01-03 20:53:18,619 1 [0.         0.02749154 0.04363545 0.01220139 0.10054151 0.04686358
 0.05177959 0.00137108] 0.03548551736715887
2025-01-03 20:53:18,622 => saving checkpoint to output\loveDa\pidnet_small_lovedacheckpoint.pth.tar
2025-01-03 20:53:18,805 Loss: 1.668, MeanIU:  0.0355, Best_mIoU:  0.0355
2025-01-03 20:53:18,805 [0.         0.02749154 0.04363545 0.01220139 0.10054151 0.04686358
 0.05177959 0.00137108]
2025-01-03 20:53:34,270 Epoch: [1/20] Iter:[0/192], Time: 15.16, lr: [0.009548853816214998], Loss: 1.727490, Acc:0.196823, Semantic loss: 0.000000, BCE loss: 1.727491, SB loss: -0.000000
2025-01-03 20:53:40,680 Epoch: [1/20] Iter:[10/192], Time: 1.96, lr: [0.009525292556561479], Loss: 1.452470, Acc:0.184800, Semantic loss: 0.000000, BCE loss: 1.452470, SB loss: -0.000000
2025-01-03 20:53:47,199 Epoch: [1/20] Iter:[20/192], Time: 1.34, lr: [0.00950172481957719], Loss: 1.372852, Acc:0.180797, Semantic loss: 0.000000, BCE loss: 1.372852, SB loss: -0.000000
2025-01-03 20:53:53,722 Epoch: [1/20] Iter:[30/192], Time: 1.12, lr: [0.009478150585620286], Loss: 1.342955, Acc:0.176712, Semantic loss: 0.000000, BCE loss: 1.342955, SB loss: 0.000000
2025-01-03 20:54:00,203 Epoch: [1/20] Iter:[40/192], Time: 1.00, lr: [0.009454569834934885], Loss: 1.363756, Acc:0.179984, Semantic loss: 0.000000, BCE loss: 1.363756, SB loss: 0.000000
2025-01-03 20:54:06,694 Epoch: [1/20] Iter:[50/192], Time: 0.93, lr: [0.009430982547650114], Loss: 1.373384, Acc:0.181315, Semantic loss: 0.000000, BCE loss: 1.373384, SB loss: -0.000000
2025-01-03 20:54:13,197 Epoch: [1/20] Iter:[60/192], Time: 0.89, lr: [0.009407388703779091], Loss: 1.386120, Acc:0.180235, Semantic loss: 0.000000, BCE loss: 1.386120, SB loss: 0.000000
2025-01-03 20:54:19,728 Epoch: [1/20] Iter:[70/192], Time: 0.85, lr: [0.009383788283217955], Loss: 1.355189, Acc:0.177662, Semantic loss: 0.000000, BCE loss: 1.355189, SB loss: 0.000000
2025-01-03 20:54:26,246 Epoch: [1/20] Iter:[80/192], Time: 0.83, lr: [0.00936018126574482], Loss: 1.360604, Acc:0.175925, Semantic loss: 0.000000, BCE loss: 1.360604, SB loss: 0.000000
2025-01-03 20:54:32,738 Epoch: [1/20] Iter:[90/192], Time: 0.81, lr: [0.009336567631018769], Loss: 1.367082, Acc:0.176598, Semantic loss: 0.000000, BCE loss: 1.367082, SB loss: 0.000000
2025-01-03 20:54:39,336 Epoch: [1/20] Iter:[100/192], Time: 0.79, lr: [0.009312947358578814], Loss: 1.384473, Acc:0.178543, Semantic loss: 0.000000, BCE loss: 1.384472, SB loss: 0.000000
2025-01-03 20:54:45,813 Epoch: [1/20] Iter:[110/192], Time: 0.78, lr: [0.009289320427842841], Loss: 1.389440, Acc:0.180117, Semantic loss: 0.000000, BCE loss: 1.389440, SB loss: 0.000000
2025-01-03 20:54:52,294 Epoch: [1/20] Iter:[120/192], Time: 0.77, lr: [0.009265686818106552], Loss: 1.388731, Acc:0.178959, Semantic loss: 0.000000, BCE loss: 1.388731, SB loss: 0.000000
2025-01-03 20:54:58,804 Epoch: [1/20] Iter:[130/192], Time: 0.76, lr: [0.009242046508542393], Loss: 1.389081, Acc:0.179909, Semantic loss: 0.000000, BCE loss: 1.389081, SB loss: 0.000000
