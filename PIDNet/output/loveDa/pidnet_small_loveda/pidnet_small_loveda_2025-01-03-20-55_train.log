2025-01-03 20:55:40,041 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 20:55:40,041 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 4
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 20:55:40,238 Attention!!!
2025-01-03 20:55:40,238 Loaded 302 parameters!
2025-01-03 20:55:40,238 Over!!!
2025-01-03 20:56:00,745 Epoch: [0/20] Iter:[0/192], Time: 18.95, lr: [0.01], Loss: 2.769941, Acc:0.174589, Semantic loss: 0.000000, BCE loss: 2.769940, SB loss: 0.000001
2025-01-03 20:56:07,710 Epoch: [0/20] Iter:[10/192], Time: 2.33, lr: [0.009976559445324192], Loss: 2.082577, Acc:0.204231, Semantic loss: 0.000000, BCE loss: 2.082577, SB loss: -0.000000
2025-01-03 20:56:14,465 Epoch: [0/20] Iter:[20/192], Time: 1.54, lr: [0.009953112769592761], Loss: 1.691456, Acc:0.191070, Semantic loss: 0.000000, BCE loss: 1.691456, SB loss: 0.000000
2025-01-03 20:56:21,319 Epoch: [0/20] Iter:[30/192], Time: 1.27, lr: [0.009929659955177281], Loss: 1.616589, Acc:0.191173, Semantic loss: 0.000000, BCE loss: 1.616589, SB loss: 0.000000
2025-01-03 20:56:28,318 Epoch: [0/20] Iter:[40/192], Time: 1.13, lr: [0.009906200984352154], Loss: 1.572587, Acc:0.188310, Semantic loss: 0.000000, BCE loss: 1.572587, SB loss: 0.000000
2025-01-03 20:56:35,237 Epoch: [0/20] Iter:[50/192], Time: 1.04, lr: [0.009882735839293803], Loss: 1.527330, Acc:0.188787, Semantic loss: 0.000000, BCE loss: 1.527330, SB loss: 0.000000
2025-01-03 20:56:41,948 Epoch: [0/20] Iter:[60/192], Time: 0.98, lr: [0.00985926450207989], Loss: 1.489617, Acc:0.186456, Semantic loss: 0.000000, BCE loss: 1.489617, SB loss: 0.000000
2025-01-03 20:56:48,814 Epoch: [0/20] Iter:[70/192], Time: 0.94, lr: [0.009835786954688485], Loss: 1.482671, Acc:0.184179, Semantic loss: 0.000000, BCE loss: 1.482671, SB loss: 0.000000
2025-01-03 20:56:55,803 Epoch: [0/20] Iter:[80/192], Time: 0.91, lr: [0.00981230317899726], Loss: 1.480103, Acc:0.184852, Semantic loss: 0.000000, BCE loss: 1.480103, SB loss: 0.000000
2025-01-03 20:57:02,896 Epoch: [0/20] Iter:[90/192], Time: 0.89, lr: [0.009788813156782662], Loss: 1.500348, Acc:0.186363, Semantic loss: 0.000000, BCE loss: 1.500348, SB loss: 0.000000
2025-01-03 20:57:09,736 Epoch: [0/20] Iter:[100/192], Time: 0.87, lr: [0.009765316869719067], Loss: 1.503872, Acc:0.186781, Semantic loss: 0.000000, BCE loss: 1.503872, SB loss: 0.000000
2025-01-03 20:57:16,518 Epoch: [0/20] Iter:[110/192], Time: 0.85, lr: [0.009741814299377942], Loss: 1.490154, Acc:0.186140, Semantic loss: 0.000000, BCE loss: 1.490154, SB loss: 0.000000
2025-01-03 20:57:23,433 Epoch: [0/20] Iter:[120/192], Time: 0.84, lr: [0.009718305427226986], Loss: 1.482163, Acc:0.187388, Semantic loss: 0.000000, BCE loss: 1.482163, SB loss: 0.000000
2025-01-03 20:57:30,331 Epoch: [0/20] Iter:[130/192], Time: 0.83, lr: [0.009694790234629266], Loss: 1.487330, Acc:0.187121, Semantic loss: 0.000000, BCE loss: 1.487330, SB loss: 0.000000
2025-01-03 20:57:37,217 Epoch: [0/20] Iter:[140/192], Time: 0.82, lr: [0.009671268702842338], Loss: 1.476102, Acc:0.186387, Semantic loss: 0.000000, BCE loss: 1.476102, SB loss: 0.000000
2025-01-03 20:57:44,000 Epoch: [0/20] Iter:[150/192], Time: 0.81, lr: [0.009647740813017376], Loss: 1.470102, Acc:0.185326, Semantic loss: 0.000000, BCE loss: 1.470102, SB loss: 0.000000
2025-01-03 20:57:51,017 Epoch: [0/20] Iter:[160/192], Time: 0.80, lr: [0.009624206546198262], Loss: 1.468727, Acc:0.185146, Semantic loss: 0.000000, BCE loss: 1.468727, SB loss: 0.000000
2025-01-03 20:57:57,888 Epoch: [0/20] Iter:[170/192], Time: 0.79, lr: [0.009600665883320689], Loss: 1.471737, Acc:0.185123, Semantic loss: 0.000000, BCE loss: 1.471737, SB loss: 0.000000
2025-01-03 20:58:04,771 Epoch: [0/20] Iter:[180/192], Time: 0.79, lr: [0.009577118805211254], Loss: 1.467594, Acc:0.184611, Semantic loss: 0.000000, BCE loss: 1.467594, SB loss: 0.000000
2025-01-03 20:58:11,470 Epoch: [0/20] Iter:[190/192], Time: 0.78, lr: [0.009553565292586523], Loss: 1.463982, Acc:0.184760, Semantic loss: 0.000000, BCE loss: 1.463982, SB loss: 0.000000
