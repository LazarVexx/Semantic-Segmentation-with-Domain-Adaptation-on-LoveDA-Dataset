2025-01-03 21:49:44,043 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 21:49:44,043 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 4
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.05
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 21:49:44,238 Attention!!!
2025-01-03 21:49:44,238 Loaded 302 parameters!
2025-01-03 21:49:44,238 Over!!!
2025-01-03 21:50:06,092 Epoch: [0/20] Iter:[0/192], Time: 20.36, lr: [0.05], Loss: 3.479972, Acc:0.203126, Semantic loss: 0.000000, BCE loss: 3.479971, SB loss: 0.000001
2025-01-03 21:50:32,429 Epoch: [0/20] Iter:[10/192], Time: 4.20, lr: [0.049882797226620965], Loss: 3.305765, Acc:0.173267, Semantic loss: 0.000000, BCE loss: 3.305765, SB loss: 0.000000
2025-01-03 21:50:57,062 Epoch: [0/20] Iter:[20/192], Time: 3.15, lr: [0.0497655638479638], Loss: 2.544429, Acc:0.160360, Semantic loss: 0.000000, BCE loss: 2.544429, SB loss: 0.000000
2025-01-03 21:51:36,171 Epoch: [0/20] Iter:[30/192], Time: 3.55, lr: [0.04964829977588641], Loss: 2.332193, Acc:0.156119, Semantic loss: 0.000000, BCE loss: 2.332193, SB loss: 0.000000
2025-01-03 21:52:03,618 Epoch: [0/20] Iter:[40/192], Time: 3.33, lr: [0.04953100492176077], Loss: 2.168942, Acc:0.156675, Semantic loss: 0.000000, BCE loss: 2.168942, SB loss: 0.000000
2025-01-03 21:52:24,818 Epoch: [0/20] Iter:[50/192], Time: 3.09, lr: [0.049413679196469024], Loss: 2.047189, Acc:0.158209, Semantic loss: 0.000000, BCE loss: 2.047189, SB loss: 0.000000
2025-01-03 21:52:54,392 Epoch: [0/20] Iter:[60/192], Time: 3.07, lr: [0.04929632251039945], Loss: 1.981254, Acc:0.157000, Semantic loss: 0.000000, BCE loss: 1.981254, SB loss: 0.000000
2025-01-03 21:53:16,180 Epoch: [0/20] Iter:[70/192], Time: 2.94, lr: [0.04917893477344243], Loss: 1.949902, Acc:0.156996, Semantic loss: 0.000000, BCE loss: 1.949902, SB loss: 0.000000
2025-01-03 21:53:45,362 Epoch: [0/20] Iter:[80/192], Time: 2.95, lr: [0.0490615158949863], Loss: 1.903838, Acc:0.157528, Semantic loss: 0.000000, BCE loss: 1.903838, SB loss: 0.000000
2025-01-03 21:54:06,332 Epoch: [0/20] Iter:[90/192], Time: 2.86, lr: [0.04894406578391331], Loss: 1.900335, Acc:0.157563, Semantic loss: 0.000000, BCE loss: 1.900335, SB loss: 0.000000
2025-01-03 21:54:29,570 Epoch: [0/20] Iter:[100/192], Time: 2.81, lr: [0.04882658434859534], Loss: 1.885122, Acc:0.157965, Semantic loss: 0.000000, BCE loss: 1.885122, SB loss: 0.000000
2025-01-03 21:54:50,837 Epoch: [0/20] Iter:[110/192], Time: 2.74, lr: [0.048709071496889716], Loss: 1.867909, Acc:0.158327, Semantic loss: 0.000000, BCE loss: 1.867909, SB loss: 0.000000
2025-01-03 21:55:12,245 Epoch: [0/20] Iter:[120/192], Time: 2.69, lr: [0.04859152713613493], Loss: 1.850164, Acc:0.158898, Semantic loss: 0.000000, BCE loss: 1.850164, SB loss: 0.000000
2025-01-03 21:55:35,119 Epoch: [0/20] Iter:[130/192], Time: 2.66, lr: [0.04847395117314633], Loss: 1.836081, Acc:0.158602, Semantic loss: 0.000000, BCE loss: 1.836081, SB loss: 0.000000
2025-01-03 21:55:59,689 Epoch: [0/20] Iter:[140/192], Time: 2.65, lr: [0.0483563435142117], Loss: 1.815777, Acc:0.158514, Semantic loss: 0.000000, BCE loss: 1.815777, SB loss: 0.000000
2025-01-03 21:56:21,465 Epoch: [0/20] Iter:[150/192], Time: 2.61, lr: [0.04823870406508688], Loss: 1.807153, Acc:0.157779, Semantic loss: 0.000000, BCE loss: 1.807153, SB loss: 0.000000
2025-01-03 21:56:47,854 Epoch: [0/20] Iter:[160/192], Time: 2.62, lr: [0.04812103273099131], Loss: 1.797026, Acc:0.156903, Semantic loss: 0.000000, BCE loss: 1.797025, SB loss: 0.000000
2025-01-03 21:57:13,932 Epoch: [0/20] Iter:[170/192], Time: 2.62, lr: [0.04800332941660345], Loss: 1.794312, Acc:0.157168, Semantic loss: 0.000000, BCE loss: 1.794312, SB loss: 0.000000
2025-01-03 21:57:43,203 Epoch: [0/20] Iter:[180/192], Time: 2.64, lr: [0.04788559402605627], Loss: 1.785393, Acc:0.157176, Semantic loss: 0.000000, BCE loss: 1.785393, SB loss: 0.000000
2025-01-03 21:58:12,719 Epoch: [0/20] Iter:[190/192], Time: 2.65, lr: [0.04776782646293262], Loss: 1.771717, Acc:0.157299, Semantic loss: 0.000000, BCE loss: 1.771716, SB loss: 0.000000
2025-01-03 22:00:17,804 0 [0.00000000e+00 1.26173104e-05 4.84634311e-03 3.68818673e-05
 9.87694088e-04 7.70754904e-03 5.35285910e-03 8.32658288e-03] 0.003408815924486159
2025-01-03 22:00:17,804 1 [0.00000000e+00 1.15507356e-03 6.09479251e-05 1.89373693e-02
 3.16578473e-02 5.45026222e-02 2.85946179e-02 2.10485958e-04] 0.01688987052520115
2025-01-03 22:00:17,807 => saving checkpoint to output\loveDa\pidnet_small_lovedacheckpoint.pth.tar
2025-01-03 22:00:17,994 Loss: 1.665, MeanIU:  0.0169, Best_mIoU:  0.0169
2025-01-03 22:00:17,995 [0.00000000e+00 1.15507356e-03 6.09479251e-05 1.89373693e-02
 3.16578473e-02 5.45026222e-02 2.85946179e-02 2.10485958e-04]
2025-01-03 22:00:35,768 Epoch: [1/20] Iter:[0/192], Time: 17.40, lr: [0.047744269081074986], Loss: 1.843766, Acc:0.148944, Semantic loss: 0.000000, BCE loss: 1.843767, SB loss: -0.000000
2025-01-03 22:01:00,978 Epoch: [1/20] Iter:[10/192], Time: 3.78, lr: [0.04762646278280739], Loss: 1.583861, Acc:0.167244, Semantic loss: 0.000000, BCE loss: 1.583861, SB loss: 0.000000
2025-01-03 22:01:31,781 Epoch: [1/20] Iter:[20/192], Time: 3.45, lr: [0.047508624097885956], Loss: 1.631818, Acc:0.165600, Semantic loss: 0.000000, BCE loss: 1.631818, SB loss: 0.000000
2025-01-03 22:01:47,162 Epoch: [1/20] Iter:[30/192], Time: 2.86, lr: [0.04739075292810143], Loss: 1.588142, Acc:0.161273, Semantic loss: 0.000000, BCE loss: 1.588142, SB loss: -0.000000
2025-01-03 22:02:09,210 Epoch: [1/20] Iter:[40/192], Time: 2.68, lr: [0.047272849174674426], Loss: 1.586748, Acc:0.160335, Semantic loss: 0.000000, BCE loss: 1.586748, SB loss: 0.000000
2025-01-03 22:02:25,902 Epoch: [1/20] Iter:[50/192], Time: 2.48, lr: [0.047154912738250565], Loss: 1.589921, Acc:0.159868, Semantic loss: 0.000000, BCE loss: 1.589921, SB loss: 0.000000
2025-01-03 22:02:48,383 Epoch: [1/20] Iter:[60/192], Time: 2.36, lr: [0.04703694351889546], Loss: 1.603972, Acc:0.161308, Semantic loss: 0.000000, BCE loss: 1.603972, SB loss: 0.000000
2025-01-03 22:03:01,377 Epoch: [1/20] Iter:[70/192], Time: 2.29, lr: [0.04691894141608978], Loss: 1.587092, Acc:0.158413, Semantic loss: 0.000000, BCE loss: 1.587091, SB loss: 0.000000
2025-01-03 22:03:33,411 Epoch: [1/20] Iter:[80/192], Time: 2.40, lr: [0.04680090632872411], Loss: 1.599814, Acc:0.157812, Semantic loss: 0.000000, BCE loss: 1.599814, SB loss: 0.000000
2025-01-03 22:04:10,113 Epoch: [1/20] Iter:[90/192], Time: 2.53, lr: [0.04668283815509385], Loss: 1.612330, Acc:0.158459, Semantic loss: 0.000000, BCE loss: 1.612330, SB loss: 0.000000
2025-01-03 22:04:39,474 Epoch: [1/20] Iter:[100/192], Time: 2.58, lr: [0.04656473679289408], Loss: 1.628864, Acc:0.158866, Semantic loss: 0.000000, BCE loss: 1.628863, SB loss: 0.000000
2025-01-03 22:04:54,424 Epoch: [1/20] Iter:[110/192], Time: 2.49, lr: [0.046446602139214206], Loss: 1.636443, Acc:0.160641, Semantic loss: 0.000000, BCE loss: 1.636443, SB loss: 0.000000
