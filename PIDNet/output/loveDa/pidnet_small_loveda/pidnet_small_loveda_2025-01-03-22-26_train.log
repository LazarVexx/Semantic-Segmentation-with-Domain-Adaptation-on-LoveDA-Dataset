2025-01-03 22:26:58,872 Namespace(cfg='configs/loveDa/pidnet_small_loveda.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-03 22:26:58,872 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 4
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  IGNORE_LABEL: 0
  IMAGE_SIZE: [1024, 1024]
  LR: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SHUFFLE: True
  WD: 0.0005
WORKERS: 6
2025-01-03 22:26:59,024 Attention!!!
2025-01-03 22:26:59,025 Loaded 302 parameters!
2025-01-03 22:26:59,025 Over!!!
2025-01-03 22:27:21,033 Epoch: [0/20] Iter:[0/192], Time: 20.65, lr: [0.01], Loss: 3.479950, Acc:0.203124, Semantic loss: 0.000000, BCE loss: 3.479950, SB loss: 0.000000
2025-01-03 22:27:29,243 Epoch: [0/20] Iter:[10/192], Time: 2.59, lr: [0.009976559445324192], Loss: 2.352794, Acc:0.195511, Semantic loss: 0.000000, BCE loss: 2.352794, SB loss: 0.000000
2025-01-03 22:27:37,246 Epoch: [0/20] Iter:[20/192], Time: 1.74, lr: [0.009953112769592761], Loss: 2.026227, Acc:0.186982, Semantic loss: 0.000000, BCE loss: 2.026227, SB loss: 0.000000
2025-01-03 22:27:45,308 Epoch: [0/20] Iter:[30/192], Time: 1.44, lr: [0.009929659955177281], Loss: 1.984985, Acc:0.188964, Semantic loss: 0.000000, BCE loss: 1.984985, SB loss: -0.000000
2025-01-03 22:27:53,428 Epoch: [0/20] Iter:[40/192], Time: 1.29, lr: [0.009906200984352154], Loss: 1.907766, Acc:0.187126, Semantic loss: 0.000000, BCE loss: 1.907766, SB loss: 0.000000
2025-01-03 22:28:01,550 Epoch: [0/20] Iter:[50/192], Time: 1.19, lr: [0.009882735839293803], Loss: 1.838767, Acc:0.189193, Semantic loss: 0.000000, BCE loss: 1.838767, SB loss: -0.000000
2025-01-03 22:28:09,250 Epoch: [0/20] Iter:[60/192], Time: 1.12, lr: [0.00985926450207989], Loss: 1.809114, Acc:0.188704, Semantic loss: 0.000000, BCE loss: 1.809114, SB loss: 0.000000
2025-01-03 22:28:16,988 Epoch: [0/20] Iter:[70/192], Time: 1.07, lr: [0.009835786954688485], Loss: 1.806324, Acc:0.187960, Semantic loss: 0.000000, BCE loss: 1.806324, SB loss: 0.000000
2025-01-03 22:28:25,223 Epoch: [0/20] Iter:[80/192], Time: 1.04, lr: [0.00981230317899726], Loss: 1.779982, Acc:0.187830, Semantic loss: 0.000000, BCE loss: 1.779982, SB loss: 0.000000
2025-01-03 22:28:33,153 Epoch: [0/20] Iter:[90/192], Time: 1.02, lr: [0.009788813156782662], Loss: 1.791980, Acc:0.187958, Semantic loss: 0.000000, BCE loss: 1.791980, SB loss: 0.000000
2025-01-03 22:28:40,553 Epoch: [0/20] Iter:[100/192], Time: 0.99, lr: [0.009765316869719067], Loss: 1.788901, Acc:0.189235, Semantic loss: 0.000000, BCE loss: 1.788901, SB loss: 0.000000
2025-01-03 22:28:48,103 Epoch: [0/20] Iter:[110/192], Time: 0.97, lr: [0.009741814299377942], Loss: 1.780949, Acc:0.189033, Semantic loss: 0.000000, BCE loss: 1.780949, SB loss: 0.000000
2025-01-03 22:28:55,875 Epoch: [0/20] Iter:[120/192], Time: 0.95, lr: [0.009718305427226986], Loss: 1.770983, Acc:0.189049, Semantic loss: 0.000000, BCE loss: 1.770983, SB loss: 0.000000
2025-01-03 22:29:03,243 Epoch: [0/20] Iter:[130/192], Time: 0.94, lr: [0.009694790234629266], Loss: 1.763190, Acc:0.188325, Semantic loss: 0.000000, BCE loss: 1.763190, SB loss: 0.000000
