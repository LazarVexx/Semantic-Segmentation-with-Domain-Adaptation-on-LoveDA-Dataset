/content/AML2024/PIDNet
/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
Seeding with 304
=> creating output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2
=> creating log/loveDa/pidnet_small/pidnet_small_loveda_3b_AUG_CHANCE+AUG2_2025-01-20-14-32
Namespace(cfg='configs/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/cityscapes/val.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDA-Urban/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 20
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: True
  AUG1: False
  AUG2: True
  AUG3: False
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 1
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.001
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
/content/AML2024/PIDNet/tools/../models/pidnet.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']
Attention!!!
Loaded 302 parameters!
Over!!!
Warm-up Epoch 1: Learning Rate = 0.0002
Epoch: [0/20] Iter:[0/290], Time: 7.79, lr: [0.0002], Loss: 5.292594, Acc:0.117866, Semantic loss: 0.641234, BCE loss: 4.390707, SB loss: 0.260653
Epoch: [0/20] Iter:[10/290], Time: 1.53, lr: [0.0001996896284016207], Loss: 3.435827, Acc:0.220276, Semantic loss: 0.507168, BCE loss: 2.737722, SB loss: 0.190937
Epoch: [0/20] Iter:[20/290], Time: 1.23, lr: [0.00019937920319381742], Loss: 2.920016, Acc:0.269735, Semantic loss: 0.445784, BCE loss: 2.312652, SB loss: 0.161580
Epoch: [0/20] Iter:[30/290], Time: 1.10, lr: [0.0001990687242745565], Loss: 2.647703, Acc:0.300489, Semantic loss: 0.398719, BCE loss: 2.104430, SB loss: 0.144554
Epoch: [0/20] Iter:[40/290], Time: 1.06, lr: [0.00019875819154143275], Loss: 2.508794, Acc:0.325486, Semantic loss: 0.361208, BCE loss: 2.017850, SB loss: 0.129736
Epoch: [0/20] Iter:[50/290], Time: 1.03, lr: [0.0001984476048916676], Loss: 2.336179, Acc:0.352415, Semantic loss: 0.334413, BCE loss: 1.882664, SB loss: 0.119102
Epoch: [0/20] Iter:[60/290], Time: 1.00, lr: [0.00019813696422210707], Loss: 2.222371, Acc:0.370954, Semantic loss: 0.314615, BCE loss: 1.796803, SB loss: 0.110953
Epoch: [0/20] Iter:[70/290], Time: 0.99, lr: [0.00019782626942921983], Loss: 2.138327, Acc:0.381540, Semantic loss: 0.296437, BCE loss: 1.737500, SB loss: 0.104390
Epoch: [0/20] Iter:[80/290], Time: 0.98, lr: [0.000197515520409095], Loss: 2.054832, Acc:0.390472, Semantic loss: 0.285120, BCE loss: 1.670417, SB loss: 0.099296
Epoch: [0/20] Iter:[90/290], Time: 0.96, lr: [0.0001972047170574402], Loss: 1.998811, Acc:0.400050, Semantic loss: 0.275328, BCE loss: 1.627964, SB loss: 0.095519
Epoch: [0/20] Iter:[100/290], Time: 0.96, lr: [0.0001968938592695795], Loss: 1.964509, Acc:0.408047, Semantic loss: 0.266979, BCE loss: 1.605044, SB loss: 0.092486
Epoch: [0/20] Iter:[110/290], Time: 0.95, lr: [0.00019658294694045123], Loss: 1.923249, Acc:0.413263, Semantic loss: 0.257850, BCE loss: 1.575845, SB loss: 0.089555
Epoch: [0/20] Iter:[120/290], Time: 0.95, lr: [0.00019627197996460594], Loss: 1.881670, Acc:0.419069, Semantic loss: 0.250197, BCE loss: 1.544518, SB loss: 0.086955
Epoch: [0/20] Iter:[130/290], Time: 0.94, lr: [0.00019596095823620432], Loss: 1.844358, Acc:0.422525, Semantic loss: 0.244276, BCE loss: 1.515276, SB loss: 0.084806
Epoch: [0/20] Iter:[140/290], Time: 0.94, lr: [0.000195649881649015], Loss: 1.830952, Acc:0.428448, Semantic loss: 0.238383, BCE loss: 1.509649, SB loss: 0.082919
Epoch: [0/20] Iter:[150/290], Time: 0.94, lr: [0.00019533875009641242], Loss: 1.814534, Acc:0.429852, Semantic loss: 0.233991, BCE loss: 1.499224, SB loss: 0.081319
Epoch: [0/20] Iter:[160/290], Time: 0.93, lr: [0.00019502756347137466], Loss: 1.789597, Acc:0.430598, Semantic loss: 0.229509, BCE loss: 1.480315, SB loss: 0.079773
Epoch: [0/20] Iter:[170/290], Time: 0.93, lr: [0.0001947163216664814], Loss: 1.771081, Acc:0.432356, Semantic loss: 0.227092, BCE loss: 1.465335, SB loss: 0.078653
Epoch: [0/20] Iter:[180/290], Time: 0.93, lr: [0.00019440502457391143], Loss: 1.749915, Acc:0.436851, Semantic loss: 0.224032, BCE loss: 1.448363, SB loss: 0.077520
Epoch: [0/20] Iter:[190/290], Time: 0.93, lr: [0.00019409367208544069], Loss: 1.749260, Acc:0.439893, Semantic loss: 0.222884, BCE loss: 1.449842, SB loss: 0.076534
Epoch: [0/20] Iter:[200/290], Time: 0.93, lr: [0.00019378226409244], Loss: 1.734086, Acc:0.442635, Semantic loss: 0.219888, BCE loss: 1.438544, SB loss: 0.075653
Epoch: [0/20] Iter:[210/290], Time: 0.93, lr: [0.00019347080048587277], Loss: 1.722903, Acc:0.443645, Semantic loss: 0.218103, BCE loss: 1.429763, SB loss: 0.075038
Epoch: [0/20] Iter:[220/290], Time: 0.93, lr: [0.0001931592811562927], Loss: 1.710383, Acc:0.446948, Semantic loss: 0.215754, BCE loss: 1.420304, SB loss: 0.074325
Epoch: [0/20] Iter:[230/290], Time: 0.92, lr: [0.0001928477059938416], Loss: 1.704663, Acc:0.449352, Semantic loss: 0.213592, BCE loss: 1.417269, SB loss: 0.073802
Epoch: [0/20] Iter:[240/290], Time: 0.92, lr: [0.00019253607488824707], Loss: 1.703552, Acc:0.451486, Semantic loss: 0.211655, BCE loss: 1.418821, SB loss: 0.073075
Epoch: [0/20] Iter:[250/290], Time: 0.92, lr: [0.00019222438772882014], Loss: 1.698289, Acc:0.454130, Semantic loss: 0.209445, BCE loss: 1.416595, SB loss: 0.072248
Epoch: [0/20] Iter:[260/290], Time: 0.92, lr: [0.00019191264440445303], Loss: 1.688520, Acc:0.457031, Semantic loss: 0.206963, BCE loss: 1.410118, SB loss: 0.071439
Epoch: [0/20] Iter:[270/290], Time: 0.92, lr: [0.00019160084480361665], Loss: 1.675330, Acc:0.457712, Semantic loss: 0.205716, BCE loss: 1.398602, SB loss: 0.071012
Epoch: [0/20] Iter:[280/290], Time: 0.92, lr: [0.00019128898881435848], Loss: 1.667677, Acc:0.460407, Semantic loss: 0.203739, BCE loss: 1.393544, SB loss: 0.070394
0
10
20
30
40
50
60
70
0 [0.         0.39413241 0.17220746 0.07602874 0.16564365 0.1400819
 0.09156503 0.0218715 ] 0.13269133603246847
1 [0.         0.40433231 0.28541572 0.12326212 0.22867818 0.05354494
 0.05233675 0.00143968] 0.1436262125574641
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.263, MeanIU:  0.1436, Best_mIoU:  0.1436
[0.         0.40433231 0.28541572 0.12326212 0.22867818 0.05354494
 0.05233675 0.00143968]
Warm-up Epoch 2: Learning Rate = 0.0004
Epoch: [1/20] Iter:[0/290], Time: 0.74, lr: [0.0003819541526485999], Loss: 1.608717, Acc:0.594409, Semantic loss: 0.140354, BCE loss: 1.418118, SB loss: 0.050244
Epoch: [1/20] Iter:[10/290], Time: 0.87, lr: [0.00038133021444126844], Loss: 1.265389, Acc:0.472032, Semantic loss: 0.158964, BCE loss: 1.049126, SB loss: 0.057300
Epoch: [1/20] Iter:[20/290], Time: 0.87, lr: [0.0003807061627802472], Loss: 1.216478, Acc:0.455796, Semantic loss: 0.159119, BCE loss: 0.998173, SB loss: 0.059186
Epoch: [1/20] Iter:[30/290], Time: 0.88, lr: [0.00038008199743819464], Loss: 1.286862, Acc:0.461836, Semantic loss: 0.158498, BCE loss: 1.071207, SB loss: 0.057158
Epoch: [1/20] Iter:[40/290], Time: 0.89, lr: [0.0003794577181868978], Loss: 1.327197, Acc:0.474766, Semantic loss: 0.158203, BCE loss: 1.112529, SB loss: 0.056465
Epoch: [1/20] Iter:[50/290], Time: 0.88, lr: [0.0003788333247972675], Loss: 1.348608, Acc:0.481716, Semantic loss: 0.157182, BCE loss: 1.135868, SB loss: 0.055558
Epoch: [1/20] Iter:[60/290], Time: 0.88, lr: [0.0003782088170393335], Loss: 1.339749, Acc:0.489909, Semantic loss: 0.155625, BCE loss: 1.128997, SB loss: 0.055128
Epoch: [1/20] Iter:[70/290], Time: 0.88, lr: [0.000377584194682239], Loss: 1.337158, Acc:0.489670, Semantic loss: 0.155162, BCE loss: 1.127149, SB loss: 0.054847
Epoch: [1/20] Iter:[80/290], Time: 0.88, lr: [0.00037695945749423657], Loss: 1.338786, Acc:0.499861, Semantic loss: 0.153398, BCE loss: 1.130756, SB loss: 0.054632
Epoch: [1/20] Iter:[90/290], Time: 0.88, lr: [0.0003763346052426817], Loss: 1.327548, Acc:0.498587, Semantic loss: 0.154348, BCE loss: 1.118593, SB loss: 0.054607
Epoch: [1/20] Iter:[100/290], Time: 0.88, lr: [0.00037570963769402904], Loss: 1.342610, Acc:0.499484, Semantic loss: 0.152645, BCE loss: 1.135954, SB loss: 0.054011
Epoch: [1/20] Iter:[110/290], Time: 0.88, lr: [0.0003750845546138262], Loss: 1.360411, Acc:0.500532, Semantic loss: 0.153529, BCE loss: 1.152864, SB loss: 0.054018
Epoch: [1/20] Iter:[120/290], Time: 0.88, lr: [0.0003744593557667091], Loss: 1.357815, Acc:0.500980, Semantic loss: 0.153362, BCE loss: 1.150757, SB loss: 0.053696
Epoch: [1/20] Iter:[130/290], Time: 0.88, lr: [0.0003738340409163965], Loss: 1.356166, Acc:0.500401, Semantic loss: 0.153024, BCE loss: 1.149698, SB loss: 0.053444
Epoch: [1/20] Iter:[140/290], Time: 0.88, lr: [0.00037320860982568494], Loss: 1.351262, Acc:0.499772, Semantic loss: 0.153123, BCE loss: 1.144809, SB loss: 0.053329
Epoch: [1/20] Iter:[150/290], Time: 0.88, lr: [0.00037258306225644295], Loss: 1.349914, Acc:0.496935, Semantic loss: 0.152589, BCE loss: 1.144237, SB loss: 0.053088
Epoch: [1/20] Iter:[160/290], Time: 0.88, lr: [0.00037195739796960645], Loss: 1.360358, Acc:0.498877, Semantic loss: 0.152236, BCE loss: 1.155261, SB loss: 0.052861
Epoch: [1/20] Iter:[170/290], Time: 0.89, lr: [0.0003713316167251724], Loss: 1.362914, Acc:0.500085, Semantic loss: 0.152255, BCE loss: 1.157861, SB loss: 0.052797
Epoch: [1/20] Iter:[180/290], Time: 0.88, lr: [0.00037070571828219423], Loss: 1.360544, Acc:0.501691, Semantic loss: 0.152069, BCE loss: 1.155683, SB loss: 0.052791
Epoch: [1/20] Iter:[190/290], Time: 0.88, lr: [0.0003700797023987757], Loss: 1.365310, Acc:0.501527, Semantic loss: 0.152054, BCE loss: 1.160312, SB loss: 0.052944
Epoch: [1/20] Iter:[200/290], Time: 0.88, lr: [0.0003694535688320657], Loss: 1.358903, Acc:0.499860, Semantic loss: 0.151662, BCE loss: 1.154624, SB loss: 0.052617
Epoch: [1/20] Iter:[210/290], Time: 0.88, lr: [0.00036882731733825275], Loss: 1.371855, Acc:0.502977, Semantic loss: 0.151369, BCE loss: 1.167932, SB loss: 0.052555
Epoch: [1/20] Iter:[220/290], Time: 0.88, lr: [0.000368200947672559], Loss: 1.373228, Acc:0.503540, Semantic loss: 0.151144, BCE loss: 1.169437, SB loss: 0.052647
Epoch: [1/20] Iter:[230/290], Time: 0.88, lr: [0.00036757445958923496], Loss: 1.377409, Acc:0.504068, Semantic loss: 0.151053, BCE loss: 1.173486, SB loss: 0.052869
Epoch: [1/20] Iter:[240/290], Time: 0.89, lr: [0.00036694785284155346], Loss: 1.377284, Acc:0.504143, Semantic loss: 0.151216, BCE loss: 1.173160, SB loss: 0.052907
Epoch: [1/20] Iter:[250/290], Time: 0.88, lr: [0.0003663211271818044], Loss: 1.368823, Acc:0.504646, Semantic loss: 0.150958, BCE loss: 1.164993, SB loss: 0.052871
Epoch: [1/20] Iter:[260/290], Time: 0.88, lr: [0.00036569428236128817], Loss: 1.374641, Acc:0.503096, Semantic loss: 0.151415, BCE loss: 1.170069, SB loss: 0.053157
Epoch: [1/20] Iter:[270/290], Time: 0.89, lr: [0.0003650673181303107], Loss: 1.375967, Acc:0.503633, Semantic loss: 0.151150, BCE loss: 1.171650, SB loss: 0.053167
Epoch: [1/20] Iter:[280/290], Time: 0.88, lr: [0.0003644402342381768], Loss: 1.376965, Acc:0.503420, Semantic loss: 0.150780, BCE loss: 1.173024, SB loss: 0.053161
0
10
20
30
40
50
60
70
0 [0.         0.44761953 0.2286973  0.0842798  0.16512158 0.04320644
 0.07916155 0.03682706] 0.13561415678474076
1 [0.         0.47127483 0.20654766 0.19076564 0.24399138 0.0461916
 0.07223482 0.1968323 ] 0.1784797769037333
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.186, MeanIU:  0.1785, Best_mIoU:  0.1785
[0.         0.47127483 0.20654766 0.19076564 0.24399138 0.0461916
 0.07223482 0.1968323 ]
Warm-up Epoch 3: Learning Rate = 0.0006000000000000001
Epoch: [2/20] Iter:[0/290], Time: 0.76, lr: [0.0005457195456497774], Loss: 1.577135, Acc:0.577460, Semantic loss: 0.185732, BCE loss: 1.337495, SB loss: 0.053908
Epoch: [2/20] Iter:[10/290], Time: 0.88, lr: [0.0005447785596939306], Loss: 1.422001, Acc:0.535356, Semantic loss: 0.144319, BCE loss: 1.227520, SB loss: 0.050162
Epoch: [2/20] Iter:[20/290], Time: 0.86, lr: [0.0005438373931091254], Loss: 1.334017, Acc:0.516656, Semantic loss: 0.142676, BCE loss: 1.140594, SB loss: 0.050747
Epoch: [2/20] Iter:[30/290], Time: 0.87, lr: [0.0005428960455132244], Loss: 1.340876, Acc:0.519787, Semantic loss: 0.140793, BCE loss: 1.149785, SB loss: 0.050298
Epoch: [2/20] Iter:[40/290], Time: 0.88, lr: [0.0005419545165225449], Loss: 1.340727, Acc:0.515820, Semantic loss: 0.142660, BCE loss: 1.147454, SB loss: 0.050613
Epoch: [2/20] Iter:[50/290], Time: 0.87, lr: [0.0005410128057518475], Loss: 1.317515, Acc:0.512749, Semantic loss: 0.142682, BCE loss: 1.124252, SB loss: 0.050581
Epoch: [2/20] Iter:[60/290], Time: 0.87, lr: [0.0005400709128143285], Loss: 1.320787, Acc:0.514946, Semantic loss: 0.143021, BCE loss: 1.127115, SB loss: 0.050651
Epoch: [2/20] Iter:[70/290], Time: 0.88, lr: [0.0005391288373216097], Loss: 1.315677, Acc:0.517141, Semantic loss: 0.143452, BCE loss: 1.121544, SB loss: 0.050680
Epoch: [2/20] Iter:[80/290], Time: 0.87, lr: [0.0005381865788837288], Loss: 1.305636, Acc:0.514688, Semantic loss: 0.142915, BCE loss: 1.112234, SB loss: 0.050487
Epoch: [2/20] Iter:[90/290], Time: 0.87, lr: [0.0005372441371091307], Loss: 1.312679, Acc:0.511541, Semantic loss: 0.141704, BCE loss: 1.120488, SB loss: 0.050488
Epoch: [2/20] Iter:[100/290], Time: 0.87, lr: [0.0005363015116046565], Loss: 1.302480, Acc:0.509392, Semantic loss: 0.141655, BCE loss: 1.110478, SB loss: 0.050347
Epoch: [2/20] Iter:[110/290], Time: 0.87, lr: [0.0005353587019755353], Loss: 1.326380, Acc:0.513345, Semantic loss: 0.141000, BCE loss: 1.135232, SB loss: 0.050148
Epoch: [2/20] Iter:[120/290], Time: 0.87, lr: [0.0005344157078253733], Loss: 1.308272, Acc:0.510120, Semantic loss: 0.140996, BCE loss: 1.117200, SB loss: 0.050075
Epoch: [2/20] Iter:[130/290], Time: 0.87, lr: [0.0005334725287561445], Loss: 1.307097, Acc:0.507745, Semantic loss: 0.141371, BCE loss: 1.115763, SB loss: 0.049963
Epoch: [2/20] Iter:[140/290], Time: 0.87, lr: [0.0005325291643681802], Loss: 1.303842, Acc:0.503484, Semantic loss: 0.141653, BCE loss: 1.112096, SB loss: 0.050093
Epoch: [2/20] Iter:[150/290], Time: 0.87, lr: [0.00053158561426016], Loss: 1.314921, Acc:0.505732, Semantic loss: 0.141760, BCE loss: 1.122975, SB loss: 0.050186
Epoch: [2/20] Iter:[160/290], Time: 0.87, lr: [0.0005306418780291005], Loss: 1.318420, Acc:0.507286, Semantic loss: 0.142055, BCE loss: 1.125917, SB loss: 0.050449
Epoch: [2/20] Iter:[170/290], Time: 0.88, lr: [0.000529697955270346], Loss: 1.326990, Acc:0.507984, Semantic loss: 0.142602, BCE loss: 1.133769, SB loss: 0.050619
Epoch: [2/20] Iter:[180/290], Time: 0.87, lr: [0.0005287538455775576], Loss: 1.323956, Acc:0.509270, Semantic loss: 0.142304, BCE loss: 1.131124, SB loss: 0.050528
Epoch: [2/20] Iter:[190/290], Time: 0.87, lr: [0.0005278095485427036], Loss: 1.327739, Acc:0.509173, Semantic loss: 0.142584, BCE loss: 1.134366, SB loss: 0.050789
Epoch: [2/20] Iter:[200/290], Time: 0.88, lr: [0.0005268650637560482], Loss: 1.323359, Acc:0.508793, Semantic loss: 0.142042, BCE loss: 1.130686, SB loss: 0.050631
Epoch: [2/20] Iter:[210/290], Time: 0.87, lr: [0.0005259203908061416], Loss: 1.321898, Acc:0.509782, Semantic loss: 0.141516, BCE loss: 1.129909, SB loss: 0.050474
Epoch: [2/20] Iter:[220/290], Time: 0.88, lr: [0.0005249755292798091], Loss: 1.322647, Acc:0.510168, Semantic loss: 0.142373, BCE loss: 1.129463, SB loss: 0.050811
Epoch: [2/20] Iter:[230/290], Time: 0.88, lr: [0.0005240304787621406], Loss: 1.315825, Acc:0.509418, Semantic loss: 0.142075, BCE loss: 1.123110, SB loss: 0.050641
Epoch: [2/20] Iter:[240/290], Time: 0.88, lr: [0.0005230852388364795], Loss: 1.322528, Acc:0.509949, Semantic loss: 0.142459, BCE loss: 1.129268, SB loss: 0.050800
Epoch: [2/20] Iter:[250/290], Time: 0.88, lr: [0.0005221398090844122], Loss: 1.320593, Acc:0.511196, Semantic loss: 0.142116, BCE loss: 1.127642, SB loss: 0.050835
Epoch: [2/20] Iter:[260/290], Time: 0.88, lr: [0.000521194189085757], Loss: 1.320807, Acc:0.510129, Semantic loss: 0.142261, BCE loss: 1.127669, SB loss: 0.050877
Epoch: [2/20] Iter:[270/290], Time: 0.88, lr: [0.000520248378418553], Loss: 1.321937, Acc:0.509753, Semantic loss: 0.142105, BCE loss: 1.128986, SB loss: 0.050846
Epoch: [2/20] Iter:[280/290], Time: 0.88, lr: [0.0005193023766590487], Loss: 1.320524, Acc:0.509954, Semantic loss: 0.142066, BCE loss: 1.127628, SB loss: 0.050830
0
10
20
30
40
50
60
70
0 [0.         0.42910087 0.27628023 0.11257121 0.20491923 0.06866291
 0.07297126 0.0388484 ] 0.1504192637390217
1 [0.         0.49037678 0.36978324 0.24394806 0.30412106 0.04013479
 0.03941273 0.25623035] 0.21800087444415828
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.179, MeanIU:  0.2180, Best_mIoU:  0.2180
[0.         0.49037678 0.36978324 0.24394806 0.30412106 0.04013479
 0.03941273 0.25623035]
Warm-up Epoch 4: Learning Rate = 0.0008
Epoch: [3/20] Iter:[0/290], Time: 0.77, lr: [0.0006911415778422553], Loss: 1.892409, Acc:0.680957, Semantic loss: 0.169004, BCE loss: 1.679098, SB loss: 0.044307
Epoch: [3/20] Iter:[10/290], Time: 0.84, lr: [0.0006898797308788209], Loss: 1.255098, Acc:0.531402, Semantic loss: 0.134768, BCE loss: 1.070546, SB loss: 0.049783
Epoch: [3/20] Iter:[20/290], Time: 0.87, lr: [0.0006886176274161748], Loss: 1.345253, Acc:0.528498, Semantic loss: 0.135455, BCE loss: 1.159830, SB loss: 0.049967
Epoch: [3/20] Iter:[30/290], Time: 0.87, lr: [0.000687355266879616], Loss: 1.313573, Acc:0.514346, Semantic loss: 0.137613, BCE loss: 1.124551, SB loss: 0.051408
Epoch: [3/20] Iter:[40/290], Time: 0.88, lr: [0.00068609264869198], Loss: 1.330046, Acc:0.514718, Semantic loss: 0.140225, BCE loss: 1.137503, SB loss: 0.052318
Epoch: [3/20] Iter:[50/290], Time: 0.87, lr: [0.0006848297722736237], Loss: 1.311172, Acc:0.520061, Semantic loss: 0.140036, BCE loss: 1.118933, SB loss: 0.052203
Epoch: [3/20] Iter:[60/290], Time: 0.88, lr: [0.0006835666370424091], Loss: 1.295258, Acc:0.514529, Semantic loss: 0.141637, BCE loss: 1.100899, SB loss: 0.052722
Epoch: [3/20] Iter:[70/290], Time: 0.88, lr: [0.0006823032424136877], Loss: 1.273567, Acc:0.511127, Semantic loss: 0.142777, BCE loss: 1.077896, SB loss: 0.052894
Epoch: [3/20] Iter:[80/290], Time: 0.87, lr: [0.0006810395878002848], Loss: 1.281555, Acc:0.511752, Semantic loss: 0.143243, BCE loss: 1.085596, SB loss: 0.052716
Epoch: [3/20] Iter:[90/290], Time: 0.88, lr: [0.0006797756726124826], Loss: 1.290128, Acc:0.513678, Semantic loss: 0.143995, BCE loss: 1.093151, SB loss: 0.052982
Epoch: [3/20] Iter:[100/290], Time: 0.88, lr: [0.0006785114962580045], Loss: 1.304962, Acc:0.514787, Semantic loss: 0.144770, BCE loss: 1.107313, SB loss: 0.052878
Epoch: [3/20] Iter:[110/290], Time: 0.88, lr: [0.0006772470581419988], Loss: 1.319478, Acc:0.516163, Semantic loss: 0.144891, BCE loss: 1.121575, SB loss: 0.053013
Epoch: [3/20] Iter:[120/290], Time: 0.88, lr: [0.0006759823576670213], Loss: 1.323333, Acc:0.514482, Semantic loss: 0.145367, BCE loss: 1.125169, SB loss: 0.052798
Epoch: [3/20] Iter:[130/290], Time: 0.88, lr: [0.0006747173942330195], Loss: 1.306039, Acc:0.505844, Semantic loss: 0.146509, BCE loss: 1.106720, SB loss: 0.052810
Epoch: [3/20] Iter:[140/290], Time: 0.88, lr: [0.0006734521672373155], Loss: 1.312844, Acc:0.506292, Semantic loss: 0.146647, BCE loss: 1.113697, SB loss: 0.052500
Epoch: [3/20] Iter:[150/290], Time: 0.88, lr: [0.0006721866760745884], Loss: 1.319459, Acc:0.506089, Semantic loss: 0.146479, BCE loss: 1.120552, SB loss: 0.052428
Epoch: [3/20] Iter:[160/290], Time: 0.88, lr: [0.0006709209201368584], Loss: 1.333815, Acc:0.511933, Semantic loss: 0.145592, BCE loss: 1.135991, SB loss: 0.052232
Epoch: [3/20] Iter:[170/290], Time: 0.88, lr: [0.000669654898813468], Loss: 1.329227, Acc:0.511203, Semantic loss: 0.145146, BCE loss: 1.131959, SB loss: 0.052122
Epoch: [3/20] Iter:[180/290], Time: 0.87, lr: [0.0006683886114910658], Loss: 1.326963, Acc:0.511111, Semantic loss: 0.144821, BCE loss: 1.130290, SB loss: 0.051851
Epoch: [3/20] Iter:[190/290], Time: 0.88, lr: [0.0006671220575535883], Loss: 1.327521, Acc:0.510538, Semantic loss: 0.144693, BCE loss: 1.131027, SB loss: 0.051802
Epoch: [3/20] Iter:[200/290], Time: 0.88, lr: [0.0006658552363822425], Loss: 1.334857, Acc:0.513284, Semantic loss: 0.144134, BCE loss: 1.139184, SB loss: 0.051539
Epoch: [3/20] Iter:[210/290], Time: 0.88, lr: [0.0006645881473554874], Loss: 1.337564, Acc:0.513827, Semantic loss: 0.143961, BCE loss: 1.141970, SB loss: 0.051632
Epoch: [3/20] Iter:[220/290], Time: 0.88, lr: [0.0006633207898490168], Loss: 1.341250, Acc:0.515662, Semantic loss: 0.143857, BCE loss: 1.145876, SB loss: 0.051517
Epoch: [3/20] Iter:[230/290], Time: 0.88, lr: [0.0006620531632357405], Loss: 1.341347, Acc:0.514915, Semantic loss: 0.144040, BCE loss: 1.145697, SB loss: 0.051610
Epoch: [3/20] Iter:[240/290], Time: 0.88, lr: [0.0006607852668857661], Loss: 1.344184, Acc:0.514039, Semantic loss: 0.144418, BCE loss: 1.148118, SB loss: 0.051647
Epoch: [3/20] Iter:[250/290], Time: 0.88, lr: [0.0006595171001663804], Loss: 1.343161, Acc:0.515045, Semantic loss: 0.144316, BCE loss: 1.147138, SB loss: 0.051707
Epoch: [3/20] Iter:[260/290], Time: 0.88, lr: [0.000658248662442031], Loss: 1.342782, Acc:0.517289, Semantic loss: 0.143524, BCE loss: 1.147693, SB loss: 0.051565
Epoch: [3/20] Iter:[270/290], Time: 0.88, lr: [0.0006569799530743069], Loss: 1.345949, Acc:0.518319, Semantic loss: 0.143355, BCE loss: 1.151100, SB loss: 0.051495
Epoch: [3/20] Iter:[280/290], Time: 0.88, lr: [0.0006557109714219203], Loss: 1.340640, Acc:0.519102, Semantic loss: 0.142936, BCE loss: 1.146328, SB loss: 0.051375
0
10
20
30
40
50
60
70
0 [0.         0.47360091 0.22582935 0.12271205 0.31590415 0.11086012
 0.08782266 0.00608827] 0.16785218912263106
1 [0.         0.47656854 0.23275911 0.19526389 0.19690194 0.08486613
 0.02756326 0.09914194] 0.16413310337718393
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.274, MeanIU:  0.1641, Best_mIoU:  0.2180
[0.         0.47656854 0.23275911 0.19526389 0.19690194 0.08486613
 0.02756326 0.09914194]
Warm-up Epoch 5: Learning Rate = 0.001
Epoch: [4/20] Iter:[0/290], Time: 0.77, lr: [0.0008180521460508585], Loss: 1.810658, Acc:0.658486, Semantic loss: 0.139301, BCE loss: 1.628199, SB loss: 0.043159
Epoch: [4/20] Iter:[10/290], Time: 0.83, lr: [0.000816465235854382], Loss: 1.573883, Acc:0.561184, Semantic loss: 0.136349, BCE loss: 1.388343, SB loss: 0.049191
Epoch: [4/20] Iter:[20/290], Time: 0.87, lr: [0.0008148779828754266], Loss: 1.429226, Acc:0.549584, Semantic loss: 0.136002, BCE loss: 1.243644, SB loss: 0.049580
Epoch: [4/20] Iter:[30/290], Time: 0.88, lr: [0.0008132903862977541], Loss: 1.377843, Acc:0.526593, Semantic loss: 0.140970, BCE loss: 1.185134, SB loss: 0.051739
Epoch: [4/20] Iter:[40/290], Time: 0.87, lr: [0.0008117024453014075], Loss: 1.365652, Acc:0.516803, Semantic loss: 0.140048, BCE loss: 1.174038, SB loss: 0.051566
Epoch: [4/20] Iter:[50/290], Time: 0.87, lr: [0.0008101141590626857], Loss: 1.362053, Acc:0.510202, Semantic loss: 0.143929, BCE loss: 1.165484, SB loss: 0.052641
Epoch: [4/20] Iter:[60/290], Time: 0.88, lr: [0.0008085255267541194], Loss: 1.380628, Acc:0.515487, Semantic loss: 0.144579, BCE loss: 1.183729, SB loss: 0.052320
Epoch: [4/20] Iter:[70/290], Time: 0.88, lr: [0.0008069365475444442], Loss: 1.353955, Acc:0.510643, Semantic loss: 0.142893, BCE loss: 1.158872, SB loss: 0.052190
Epoch: [4/20] Iter:[80/290], Time: 0.88, lr: [0.0008053472205985761], Loss: 1.342371, Acc:0.505657, Semantic loss: 0.145356, BCE loss: 1.144442, SB loss: 0.052574
Epoch: [4/20] Iter:[90/290], Time: 0.88, lr: [0.0008037575450775838], Loss: 1.324122, Acc:0.503011, Semantic loss: 0.144959, BCE loss: 1.126584, SB loss: 0.052580
Epoch: [4/20] Iter:[100/290], Time: 0.88, lr: [0.0008021675201386647], Loss: 1.335244, Acc:0.501944, Semantic loss: 0.145271, BCE loss: 1.137003, SB loss: 0.052970
Epoch: [4/20] Iter:[110/290], Time: 0.87, lr: [0.0008005771449351167], Loss: 1.333783, Acc:0.500468, Semantic loss: 0.144606, BCE loss: 1.136303, SB loss: 0.052875
Epoch: [4/20] Iter:[120/290], Time: 0.88, lr: [0.0007989864186163125], Loss: 1.344927, Acc:0.499149, Semantic loss: 0.144404, BCE loss: 1.147530, SB loss: 0.052993
Epoch: [4/20] Iter:[130/290], Time: 0.88, lr: [0.0007973953403276719], Loss: 1.332213, Acc:0.498823, Semantic loss: 0.144221, BCE loss: 1.135189, SB loss: 0.052803
Epoch: [4/20] Iter:[140/290], Time: 0.87, lr: [0.0007958039092106355], Loss: 1.326614, Acc:0.497799, Semantic loss: 0.143588, BCE loss: 1.130444, SB loss: 0.052582
Epoch: [4/20] Iter:[150/290], Time: 0.87, lr: [0.0007942121244026365], Loss: 1.331328, Acc:0.499443, Semantic loss: 0.143681, BCE loss: 1.135059, SB loss: 0.052588
Epoch: [4/20] Iter:[160/290], Time: 0.87, lr: [0.0007926199850370737], Loss: 1.323717, Acc:0.501601, Semantic loss: 0.142839, BCE loss: 1.128634, SB loss: 0.052244
Epoch: [4/20] Iter:[170/290], Time: 0.87, lr: [0.0007910274902432828], Loss: 1.314078, Acc:0.502830, Semantic loss: 0.141880, BCE loss: 1.120205, SB loss: 0.051993
Epoch: [4/20] Iter:[180/290], Time: 0.87, lr: [0.0007894346391465089], Loss: 1.315586, Acc:0.504381, Semantic loss: 0.142251, BCE loss: 1.121287, SB loss: 0.052048
Epoch: [4/20] Iter:[190/290], Time: 0.87, lr: [0.0007878414308678781], Loss: 1.320698, Acc:0.505416, Semantic loss: 0.142251, BCE loss: 1.126643, SB loss: 0.051804
Epoch: [4/20] Iter:[200/290], Time: 0.87, lr: [0.0007862478645243679], Loss: 1.321200, Acc:0.505294, Semantic loss: 0.142198, BCE loss: 1.127238, SB loss: 0.051764
Epoch: [4/20] Iter:[210/290], Time: 0.87, lr: [0.0007846539392287796], Loss: 1.317542, Acc:0.503053, Semantic loss: 0.142746, BCE loss: 1.123080, SB loss: 0.051717
Epoch: [4/20] Iter:[220/290], Time: 0.87, lr: [0.0007830596540897076], Loss: 1.315529, Acc:0.502846, Semantic loss: 0.142351, BCE loss: 1.121659, SB loss: 0.051519
Epoch: [4/20] Iter:[230/290], Time: 0.87, lr: [0.0007814650082115113], Loss: 1.318035, Acc:0.502537, Semantic loss: 0.142546, BCE loss: 1.123959, SB loss: 0.051529
Epoch: [4/20] Iter:[240/290], Time: 0.87, lr: [0.0007798700006942841], Loss: 1.318793, Acc:0.501776, Semantic loss: 0.142813, BCE loss: 1.124459, SB loss: 0.051521
Epoch: [4/20] Iter:[250/290], Time: 0.87, lr: [0.0007782746306338242], Loss: 1.315547, Acc:0.501295, Semantic loss: 0.142571, BCE loss: 1.121369, SB loss: 0.051608
Epoch: [4/20] Iter:[260/290], Time: 0.87, lr: [0.000776678897121604], Loss: 1.314556, Acc:0.502190, Semantic loss: 0.142542, BCE loss: 1.120508, SB loss: 0.051507
Epoch: [4/20] Iter:[270/290], Time: 0.87, lr: [0.000775082799244739], Loss: 1.318148, Acc:0.504139, Semantic loss: 0.142371, BCE loss: 1.124411, SB loss: 0.051366
Epoch: [4/20] Iter:[280/290], Time: 0.87, lr: [0.0007734863360859578], Loss: 1.320915, Acc:0.504711, Semantic loss: 0.142317, BCE loss: 1.127340, SB loss: 0.051258
0
10
20
30
40
50
60
70
0 [0.         0.45648497 0.17022612 0.1079582  0.09582866 0.07344186
 0.06908686 0.06693202] 0.1299948369418779
1 [0.         0.46853636 0.26215748 0.26601292 0.19658883 0.03553653
 0.04827619 0.30138281] 0.1973113904538101
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.236, MeanIU:  0.1973, Best_mIoU:  0.2180
[0.         0.46853636 0.26215748 0.26601292 0.19658883 0.03553653
 0.04827619 0.30138281]
Epoch: [5/20] Iter:[0/290], Time: 0.74, lr: [0.000589433791587783], Loss: 1.212453, Acc:0.363863, Semantic loss: 0.173175, BCE loss: 0.968535, SB loss: 0.070743
Epoch: [5/20] Iter:[10/290], Time: 0.87, lr: [0.0005882141331054891], Loss: 1.297693, Acc:0.491658, Semantic loss: 0.140206, BCE loss: 1.102411, SB loss: 0.055076
Epoch: [5/20] Iter:[20/290], Time: 0.88, lr: [0.0005869941935632548], Loss: 1.243794, Acc:0.482377, Semantic loss: 0.137097, BCE loss: 1.054326, SB loss: 0.052372
Epoch: [5/20] Iter:[30/290], Time: 0.87, lr: [0.0005857739722469876], Loss: 1.274853, Acc:0.499260, Semantic loss: 0.142058, BCE loss: 1.081143, SB loss: 0.051652
Epoch: [5/20] Iter:[40/290], Time: 0.88, lr: [0.000584553468439123], Loss: 1.338870, Acc:0.507021, Semantic loss: 0.143304, BCE loss: 1.144542, SB loss: 0.051024
Epoch: [5/20] Iter:[50/290], Time: 0.88, lr: [0.0005833326814185999], Loss: 1.355686, Acc:0.508088, Semantic loss: 0.140690, BCE loss: 1.164485, SB loss: 0.050511
Epoch: [5/20] Iter:[60/290], Time: 0.88, lr: [0.000582111610460835], Loss: 1.361840, Acc:0.513607, Semantic loss: 0.142394, BCE loss: 1.168554, SB loss: 0.050892
Epoch: [5/20] Iter:[70/290], Time: 0.88, lr: [0.000580890254837698], Loss: 1.359876, Acc:0.521624, Semantic loss: 0.141303, BCE loss: 1.168317, SB loss: 0.050257
Epoch: [5/20] Iter:[80/290], Time: 0.88, lr: [0.0005796686138174856], Loss: 1.327510, Acc:0.513389, Semantic loss: 0.141215, BCE loss: 1.136112, SB loss: 0.050184
Epoch: [5/20] Iter:[90/290], Time: 0.88, lr: [0.0005784466866648953], Loss: 1.316518, Acc:0.515786, Semantic loss: 0.139431, BCE loss: 1.127467, SB loss: 0.049621
Epoch: [5/20] Iter:[100/290], Time: 0.87, lr: [0.0005772244726409997], Loss: 1.305825, Acc:0.518338, Semantic loss: 0.139537, BCE loss: 1.116500, SB loss: 0.049788
Epoch: [5/20] Iter:[110/290], Time: 0.88, lr: [0.0005760019710032194], Loss: 1.323272, Acc:0.520692, Semantic loss: 0.138797, BCE loss: 1.134888, SB loss: 0.049588
Epoch: [5/20] Iter:[120/290], Time: 0.88, lr: [0.000574779181005297], Loss: 1.318951, Acc:0.523468, Semantic loss: 0.138207, BCE loss: 1.130870, SB loss: 0.049874
Epoch: [5/20] Iter:[130/290], Time: 0.88, lr: [0.0005735561018972698], Loss: 1.321338, Acc:0.523185, Semantic loss: 0.138588, BCE loss: 1.132886, SB loss: 0.049864
Epoch: [5/20] Iter:[140/290], Time: 0.88, lr: [0.0005723327329254426], Loss: 1.311083, Acc:0.522680, Semantic loss: 0.138465, BCE loss: 1.122607, SB loss: 0.050011
Epoch: [5/20] Iter:[150/290], Time: 0.88, lr: [0.00057110907333236], Loss: 1.318290, Acc:0.521782, Semantic loss: 0.139034, BCE loss: 1.129175, SB loss: 0.050081
Epoch: [5/20] Iter:[160/290], Time: 0.88, lr: [0.0005698851223567792], Loss: 1.317727, Acc:0.520704, Semantic loss: 0.139258, BCE loss: 1.128458, SB loss: 0.050010
Epoch: [5/20] Iter:[170/290], Time: 0.88, lr: [0.0005686608792336415], Loss: 1.310840, Acc:0.522054, Semantic loss: 0.138512, BCE loss: 1.122482, SB loss: 0.049846
Epoch: [5/20] Iter:[180/290], Time: 0.88, lr: [0.0005674363431940441], Loss: 1.306559, Acc:0.524321, Semantic loss: 0.137939, BCE loss: 1.118865, SB loss: 0.049756
Epoch: [5/20] Iter:[190/290], Time: 0.88, lr: [0.0005662115134652113], Loss: 1.310865, Acc:0.525488, Semantic loss: 0.138130, BCE loss: 1.123028, SB loss: 0.049707
Epoch: [5/20] Iter:[200/290], Time: 0.88, lr: [0.0005649863892704664], Loss: 1.317719, Acc:0.527057, Semantic loss: 0.137818, BCE loss: 1.130307, SB loss: 0.049594
Epoch: [5/20] Iter:[210/290], Time: 0.88, lr: [0.0005637609698292017], Loss: 1.321695, Acc:0.528312, Semantic loss: 0.137889, BCE loss: 1.134157, SB loss: 0.049649
Epoch: [5/20] Iter:[220/290], Time: 0.88, lr: [0.0005625352543568496], Loss: 1.319386, Acc:0.526308, Semantic loss: 0.137925, BCE loss: 1.131644, SB loss: 0.049817
Epoch: [5/20] Iter:[230/290], Time: 0.88, lr: [0.0005613092420648524], Loss: 1.321790, Acc:0.525097, Semantic loss: 0.137866, BCE loss: 1.134151, SB loss: 0.049773
Epoch: [5/20] Iter:[240/290], Time: 0.88, lr: [0.0005600829321606327], Loss: 1.321638, Acc:0.525925, Semantic loss: 0.138046, BCE loss: 1.133667, SB loss: 0.049925
Epoch: [5/20] Iter:[250/290], Time: 0.88, lr: [0.0005588563238475633], Loss: 1.319260, Acc:0.525429, Semantic loss: 0.137784, BCE loss: 1.131558, SB loss: 0.049919
Epoch: [5/20] Iter:[260/290], Time: 0.88, lr: [0.0005576294163249355], Loss: 1.316400, Acc:0.524446, Semantic loss: 0.137387, BCE loss: 1.129208, SB loss: 0.049805
Epoch: [5/20] Iter:[270/290], Time: 0.88, lr: [0.0005564022087879294], Loss: 1.319524, Acc:0.523122, Semantic loss: 0.137321, BCE loss: 1.132343, SB loss: 0.049861
Epoch: [5/20] Iter:[280/290], Time: 0.88, lr: [0.000555174700427582], Loss: 1.316576, Acc:0.522379, Semantic loss: 0.137588, BCE loss: 1.129009, SB loss: 0.049979
0
10
20
30
40
50
60
70
0 [0.         0.45207024 0.20535803 0.15036305 0.26891521 0.03848582
 0.13449632 0.00273282] 0.1565526837591185
1 [0.         0.45500853 0.21006918 0.24099269 0.26528842 0.00734023
 0.00619755 0.00581424] 0.14883885356582122
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.243, MeanIU:  0.1488, Best_mIoU:  0.2180
[0.         0.45500853 0.21006918 0.24099269 0.26528842 0.00734023
 0.00619755 0.00581424]
Epoch: [6/20] Iter:[0/290], Time: 0.77, lr: [0.0003888294729839952], Loss: 1.940711, Acc:0.550946, Semantic loss: 0.131005, BCE loss: 1.762758, SB loss: 0.046948
Epoch: [6/20] Iter:[10/290], Time: 0.90, lr: [0.00038796742948271347], Loss: 1.351132, Acc:0.518694, Semantic loss: 0.136135, BCE loss: 1.165108, SB loss: 0.049889
Epoch: [6/20] Iter:[20/290], Time: 0.91, lr: [0.0003871051731046774], Loss: 1.295520, Acc:0.514902, Semantic loss: 0.140617, BCE loss: 1.103177, SB loss: 0.051725
Epoch: [6/20] Iter:[30/290], Time: 0.88, lr: [0.00038624270327019904], Loss: 1.291613, Acc:0.512662, Semantic loss: 0.136923, BCE loss: 1.104656, SB loss: 0.050035
Epoch: [6/20] Iter:[40/290], Time: 0.89, lr: [0.00038538001939656953], Loss: 1.296224, Acc:0.510652, Semantic loss: 0.137113, BCE loss: 1.108295, SB loss: 0.050816
Epoch: [6/20] Iter:[50/290], Time: 0.89, lr: [0.0003845171208980356], Loss: 1.328037, Acc:0.520299, Semantic loss: 0.136707, BCE loss: 1.141134, SB loss: 0.050196
Epoch: [6/20] Iter:[60/290], Time: 0.89, lr: [0.00038365400718577594], Loss: 1.344864, Acc:0.525393, Semantic loss: 0.136301, BCE loss: 1.158995, SB loss: 0.049568
Epoch: [6/20] Iter:[70/290], Time: 0.88, lr: [0.0003827906776678776], Loss: 1.341898, Acc:0.528750, Semantic loss: 0.135387, BCE loss: 1.157074, SB loss: 0.049437
Epoch: [6/20] Iter:[80/290], Time: 0.88, lr: [0.0003819271317493119], Loss: 1.310084, Acc:0.522443, Semantic loss: 0.135398, BCE loss: 1.125362, SB loss: 0.049324
Epoch: [6/20] Iter:[90/290], Time: 0.88, lr: [0.0003810633688319101], Loss: 1.312327, Acc:0.527854, Semantic loss: 0.134264, BCE loss: 1.129223, SB loss: 0.048840
Epoch: [6/20] Iter:[100/290], Time: 0.88, lr: [0.00038019938831433903], Loss: 1.330177, Acc:0.532555, Semantic loss: 0.135149, BCE loss: 1.145810, SB loss: 0.049217
Epoch: [6/20] Iter:[110/290], Time: 0.88, lr: [0.0003793351895920761], Loss: 1.324298, Acc:0.537535, Semantic loss: 0.135437, BCE loss: 1.139798, SB loss: 0.049062
Epoch: [6/20] Iter:[120/290], Time: 0.88, lr: [0.00037847077205738455], Loss: 1.326524, Acc:0.537051, Semantic loss: 0.136530, BCE loss: 1.140650, SB loss: 0.049343
Epoch: [6/20] Iter:[130/290], Time: 0.88, lr: [0.0003776061350992877], Loss: 1.342352, Acc:0.536669, Semantic loss: 0.137936, BCE loss: 1.154697, SB loss: 0.049720
Epoch: [6/20] Iter:[140/290], Time: 0.88, lr: [0.0003767412781035442], Loss: 1.345659, Acc:0.538476, Semantic loss: 0.137165, BCE loss: 1.158993, SB loss: 0.049501
Epoch: [6/20] Iter:[150/290], Time: 0.88, lr: [0.0003758762004526211], Loss: 1.345065, Acc:0.537788, Semantic loss: 0.136838, BCE loss: 1.158850, SB loss: 0.049376
Epoch: [6/20] Iter:[160/290], Time: 0.88, lr: [0.0003750109015256687], Loss: 1.341033, Acc:0.537960, Semantic loss: 0.136829, BCE loss: 1.154736, SB loss: 0.049468
Epoch: [6/20] Iter:[170/290], Time: 0.88, lr: [0.00037414538069849366], Loss: 1.336788, Acc:0.536392, Semantic loss: 0.136450, BCE loss: 1.151053, SB loss: 0.049285
Epoch: [6/20] Iter:[180/290], Time: 0.88, lr: [0.0003732796373435327], Loss: 1.339381, Acc:0.538686, Semantic loss: 0.136109, BCE loss: 1.154085, SB loss: 0.049188
Epoch: [6/20] Iter:[190/290], Time: 0.88, lr: [0.0003724136708298253], Loss: 1.334831, Acc:0.540096, Semantic loss: 0.136086, BCE loss: 1.149665, SB loss: 0.049080
Epoch: [6/20] Iter:[200/290], Time: 0.88, lr: [0.00037154748052298686], Loss: 1.333894, Acc:0.541038, Semantic loss: 0.135736, BCE loss: 1.149085, SB loss: 0.049073
Epoch: [6/20] Iter:[210/290], Time: 0.88, lr: [0.0003706810657851807], Loss: 1.322309, Acc:0.538299, Semantic loss: 0.135603, BCE loss: 1.137532, SB loss: 0.049173
Epoch: [6/20] Iter:[220/290], Time: 0.88, lr: [0.000369814425975091], Loss: 1.329185, Acc:0.539136, Semantic loss: 0.135413, BCE loss: 1.144696, SB loss: 0.049076
Epoch: [6/20] Iter:[230/290], Time: 0.88, lr: [0.0003689475604478939], Loss: 1.324474, Acc:0.538754, Semantic loss: 0.135603, BCE loss: 1.139784, SB loss: 0.049087
Epoch: [6/20] Iter:[240/290], Time: 0.88, lr: [0.00036808046855522946], Loss: 1.320650, Acc:0.537820, Semantic loss: 0.135670, BCE loss: 1.135889, SB loss: 0.049091
Epoch: [6/20] Iter:[250/290], Time: 0.88, lr: [0.00036721314964517334], Loss: 1.314051, Acc:0.536777, Semantic loss: 0.135605, BCE loss: 1.129484, SB loss: 0.048962
Epoch: [6/20] Iter:[260/290], Time: 0.88, lr: [0.0003663456030622073], Loss: 1.316547, Acc:0.538561, Semantic loss: 0.135309, BCE loss: 1.132455, SB loss: 0.048783
Epoch: [6/20] Iter:[270/290], Time: 0.88, lr: [0.0003654778281471897], Loss: 1.310283, Acc:0.537665, Semantic loss: 0.135080, BCE loss: 1.126404, SB loss: 0.048798
Epoch: [6/20] Iter:[280/290], Time: 0.88, lr: [0.0003646098242373266], Loss: 1.308877, Acc:0.538007, Semantic loss: 0.134814, BCE loss: 1.125302, SB loss: 0.048761
0
10
20
30
40
50
60
70
0 [0.         0.3374777  0.2253791  0.0993865  0.25857135 0.10598168
 0.10306913 0.02194266] 0.14397601439443686
1 [0.         0.23639947 0.23796468 0.14653639 0.31072129 0.09005185
 0.08130592 0.08342934] 0.14830111765173337
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.267, MeanIU:  0.1483, Best_mIoU:  0.2180
[0.         0.23639947 0.23796468 0.14653639 0.31072129 0.09005185
 0.08130592 0.08342934]
Epoch: [7/20] Iter:[0/290], Time: 0.71, lr: [0.00023344870940195965], Loss: 1.395888, Acc:0.541538, Semantic loss: 0.112874, BCE loss: 1.236088, SB loss: 0.046926
Epoch: [7/20] Iter:[10/290], Time: 0.89, lr: [0.00023289133080558932], Loss: 1.262376, Acc:0.542327, Semantic loss: 0.129389, BCE loss: 1.085173, SB loss: 0.047813
Epoch: [7/20] Iter:[20/290], Time: 0.88, lr: [0.00023233380395033275], Loss: 1.211748, Acc:0.528766, Semantic loss: 0.131103, BCE loss: 1.031447, SB loss: 0.049198
Epoch: [7/20] Iter:[30/290], Time: 0.87, lr: [0.0002317761284012381], Loss: 1.254001, Acc:0.535807, Semantic loss: 0.132354, BCE loss: 1.072819, SB loss: 0.048829
Epoch: [7/20] Iter:[40/290], Time: 0.87, lr: [0.000231218303720911], Loss: 1.249327, Acc:0.541308, Semantic loss: 0.131801, BCE loss: 1.069153, SB loss: 0.048373
Epoch: [7/20] Iter:[50/290], Time: 0.88, lr: [0.0002306603294694943], Loss: 1.303233, Acc:0.546754, Semantic loss: 0.130970, BCE loss: 1.124640, SB loss: 0.047623
Epoch: [7/20] Iter:[60/290], Time: 0.87, lr: [0.00023010220520464737], Loss: 1.317507, Acc:0.552426, Semantic loss: 0.130149, BCE loss: 1.140233, SB loss: 0.047125
Epoch: [7/20] Iter:[70/290], Time: 0.87, lr: [0.00022954393048152548], Loss: 1.330352, Acc:0.558667, Semantic loss: 0.129086, BCE loss: 1.154017, SB loss: 0.047249
Epoch: [7/20] Iter:[80/290], Time: 0.87, lr: [0.00022898550485275855], Loss: 1.321403, Acc:0.555995, Semantic loss: 0.128714, BCE loss: 1.145543, SB loss: 0.047146
Epoch: [7/20] Iter:[90/290], Time: 0.87, lr: [0.00022842692786843042], Loss: 1.317815, Acc:0.553408, Semantic loss: 0.127730, BCE loss: 1.142999, SB loss: 0.047086
Epoch: [7/20] Iter:[100/290], Time: 0.87, lr: [0.00022786819907605698], Loss: 1.318582, Acc:0.554658, Semantic loss: 0.128363, BCE loss: 1.143231, SB loss: 0.046988
Epoch: [7/20] Iter:[110/290], Time: 0.87, lr: [0.00022730931802056458], Loss: 1.300308, Acc:0.553587, Semantic loss: 0.128422, BCE loss: 1.124603, SB loss: 0.047283
Epoch: [7/20] Iter:[120/290], Time: 0.87, lr: [0.0002267502842442682], Loss: 1.304979, Acc:0.554551, Semantic loss: 0.129443, BCE loss: 1.128094, SB loss: 0.047442
Epoch: [7/20] Iter:[130/290], Time: 0.87, lr: [0.00022619109728684906], Loss: 1.302047, Acc:0.553897, Semantic loss: 0.128925, BCE loss: 1.125793, SB loss: 0.047329
Epoch: [7/20] Iter:[140/290], Time: 0.87, lr: [0.00022563175668533257], Loss: 1.303684, Acc:0.556534, Semantic loss: 0.128434, BCE loss: 1.127959, SB loss: 0.047292
Epoch: [7/20] Iter:[150/290], Time: 0.87, lr: [0.00022507226197406518], Loss: 1.306275, Acc:0.556951, Semantic loss: 0.129014, BCE loss: 1.129614, SB loss: 0.047648
Epoch: [7/20] Iter:[160/290], Time: 0.87, lr: [0.0002245126126846917], Loss: 1.295535, Acc:0.556994, Semantic loss: 0.129480, BCE loss: 1.118173, SB loss: 0.047881
Epoch: [7/20] Iter:[170/290], Time: 0.87, lr: [0.000223952808346132], Loss: 1.286463, Acc:0.553822, Semantic loss: 0.129597, BCE loss: 1.109028, SB loss: 0.047838
Epoch: [7/20] Iter:[180/290], Time: 0.88, lr: [0.00022339284848455775], Loss: 1.285820, Acc:0.555133, Semantic loss: 0.129262, BCE loss: 1.108967, SB loss: 0.047591
Epoch: [7/20] Iter:[190/290], Time: 0.87, lr: [0.00022283273262336848], Loss: 1.276735, Acc:0.550317, Semantic loss: 0.129701, BCE loss: 1.099535, SB loss: 0.047499
Epoch: [7/20] Iter:[200/290], Time: 0.87, lr: [0.0002222724602831674], Loss: 1.284735, Acc:0.552001, Semantic loss: 0.129476, BCE loss: 1.107738, SB loss: 0.047521
Epoch: [7/20] Iter:[210/290], Time: 0.87, lr: [0.0002217120309817376], Loss: 1.280131, Acc:0.551263, Semantic loss: 0.129040, BCE loss: 1.103656, SB loss: 0.047436
Epoch: [7/20] Iter:[220/290], Time: 0.87, lr: [0.00022115144423401708], Loss: 1.280624, Acc:0.550895, Semantic loss: 0.129143, BCE loss: 1.104044, SB loss: 0.047436
Epoch: [7/20] Iter:[230/290], Time: 0.87, lr: [0.0002205906995520742], Loss: 1.274882, Acc:0.549663, Semantic loss: 0.129080, BCE loss: 1.098454, SB loss: 0.047348
Epoch: [7/20] Iter:[240/290], Time: 0.87, lr: [0.00022002979644508217], Loss: 1.275963, Acc:0.550938, Semantic loss: 0.128749, BCE loss: 1.099892, SB loss: 0.047322
Epoch: [7/20] Iter:[250/290], Time: 0.87, lr: [0.00021946873441929391], Loss: 1.277648, Acc:0.550967, Semantic loss: 0.128788, BCE loss: 1.101548, SB loss: 0.047311
Epoch: [7/20] Iter:[260/290], Time: 0.87, lr: [0.00021890751297801638], Loss: 1.271543, Acc:0.549945, Semantic loss: 0.128455, BCE loss: 1.095821, SB loss: 0.047267
Epoch: [7/20] Iter:[270/290], Time: 0.87, lr: [0.00021834613162158436], Loss: 1.265837, Acc:0.549125, Semantic loss: 0.128407, BCE loss: 1.090238, SB loss: 0.047192
Epoch: [7/20] Iter:[280/290], Time: 0.87, lr: [0.0002177845898473342], Loss: 1.270331, Acc:0.550650, Semantic loss: 0.128435, BCE loss: 1.094783, SB loss: 0.047113
0
10
20
30
40
50
60
70
0 [0.         0.44822123 0.22221304 0.15665837 0.27527512 0.08364276
 0.11246805 0.03133677] 0.1662269178056326
1 [0.         0.45819752 0.28003214 0.23735277 0.3190165  0.01747155
 0.1063597  0.14551694] 0.19549338849742748
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.232, MeanIU:  0.1955, Best_mIoU:  0.2180
[0.         0.45819752 0.28003214 0.23735277 0.3190165  0.01747155
 0.1063597  0.14551694]
Epoch: [8/20] Iter:[0/290], Time: 0.73, lr: [0.00012663949333164735], Loss: 1.670201, Acc:0.573990, Semantic loss: 0.145460, BCE loss: 1.468946, SB loss: 0.055795
Epoch: [8/20] Iter:[10/290], Time: 0.89, lr: [0.0001263119302940746], Loss: 1.229230, Acc:0.577989, Semantic loss: 0.121553, BCE loss: 1.062442, SB loss: 0.045236
Epoch: [8/20] Iter:[20/290], Time: 0.89, lr: [0.00012598427284419138], Loss: 1.200127, Acc:0.550732, Semantic loss: 0.123154, BCE loss: 1.030117, SB loss: 0.046856
Epoch: [8/20] Iter:[30/290], Time: 0.87, lr: [0.0001256565206817986], Loss: 1.232721, Acc:0.552229, Semantic loss: 0.122398, BCE loss: 1.064584, SB loss: 0.045739
Epoch: [8/20] Iter:[40/290], Time: 0.88, lr: [0.0001253286735048698], Loss: 1.239548, Acc:0.558022, Semantic loss: 0.124788, BCE loss: 1.068637, SB loss: 0.046123
Epoch: [8/20] Iter:[50/290], Time: 0.88, lr: [0.0001250007310095344], Loss: 1.222917, Acc:0.558642, Semantic loss: 0.124960, BCE loss: 1.051995, SB loss: 0.045963
Epoch: [8/20] Iter:[60/290], Time: 0.88, lr: [0.0001246726928900611], Loss: 1.208664, Acc:0.560820, Semantic loss: 0.124530, BCE loss: 1.037836, SB loss: 0.046299
Epoch: [8/20] Iter:[70/290], Time: 0.88, lr: [0.00012434455883884103], Loss: 1.243015, Acc:0.565334, Semantic loss: 0.125301, BCE loss: 1.071525, SB loss: 0.046188
Epoch: [8/20] Iter:[80/290], Time: 0.88, lr: [0.00012401632854637076], Loss: 1.251974, Acc:0.568824, Semantic loss: 0.124498, BCE loss: 1.081389, SB loss: 0.046088
Epoch: [8/20] Iter:[90/290], Time: 0.88, lr: [0.00012368800170123472], Loss: 1.264508, Acc:0.572699, Semantic loss: 0.123848, BCE loss: 1.094821, SB loss: 0.045839
Epoch: [8/20] Iter:[100/290], Time: 0.88, lr: [0.00012335957799008813], Loss: 1.248751, Acc:0.569494, Semantic loss: 0.124226, BCE loss: 1.078463, SB loss: 0.046062
Epoch: [8/20] Iter:[110/290], Time: 0.88, lr: [0.00012303105709763902], Loss: 1.261223, Acc:0.570568, Semantic loss: 0.124297, BCE loss: 1.090837, SB loss: 0.046090
Epoch: [8/20] Iter:[120/290], Time: 0.88, lr: [0.0001227024387066304], Loss: 1.267479, Acc:0.574338, Semantic loss: 0.123862, BCE loss: 1.097563, SB loss: 0.046054
Epoch: [8/20] Iter:[130/290], Time: 0.88, lr: [0.00012237372249782207], Loss: 1.248224, Acc:0.570856, Semantic loss: 0.123539, BCE loss: 1.078356, SB loss: 0.046328
Epoch: [8/20] Iter:[140/290], Time: 0.88, lr: [0.00012204490814997236], Loss: 1.244277, Acc:0.569055, Semantic loss: 0.123243, BCE loss: 1.074889, SB loss: 0.046145
Epoch: [8/20] Iter:[150/290], Time: 0.88, lr: [0.00012171599533981954], Loss: 1.255744, Acc:0.570590, Semantic loss: 0.123399, BCE loss: 1.086126, SB loss: 0.046218
Epoch: [8/20] Iter:[160/290], Time: 0.88, lr: [0.00012138698374206294], Loss: 1.251273, Acc:0.568586, Semantic loss: 0.124062, BCE loss: 1.080925, SB loss: 0.046286
Epoch: [8/20] Iter:[170/290], Time: 0.88, lr: [0.00012105787302934392], Loss: 1.252793, Acc:0.569178, Semantic loss: 0.124111, BCE loss: 1.082494, SB loss: 0.046189
Epoch: [8/20] Iter:[180/290], Time: 0.89, lr: [0.00012072866287222665], Loss: 1.256532, Acc:0.569069, Semantic loss: 0.124280, BCE loss: 1.086063, SB loss: 0.046189
Epoch: [8/20] Iter:[190/290], Time: 0.88, lr: [0.00012039935293917857], Loss: 1.249327, Acc:0.565477, Semantic loss: 0.124230, BCE loss: 1.078853, SB loss: 0.046244
Epoch: [8/20] Iter:[200/290], Time: 0.88, lr: [0.00012006994289655062], Loss: 1.249129, Acc:0.566586, Semantic loss: 0.124024, BCE loss: 1.078919, SB loss: 0.046185
Epoch: [8/20] Iter:[210/290], Time: 0.88, lr: [0.00011974043240855725], Loss: 1.252743, Acc:0.565100, Semantic loss: 0.124387, BCE loss: 1.082166, SB loss: 0.046191
Epoch: [8/20] Iter:[220/290], Time: 0.88, lr: [0.00011941082113725605], Loss: 1.259392, Acc:0.566840, Semantic loss: 0.124451, BCE loss: 1.088719, SB loss: 0.046222
Epoch: [8/20] Iter:[230/290], Time: 0.88, lr: [0.00011908110874252747], Loss: 1.264490, Acc:0.567883, Semantic loss: 0.124582, BCE loss: 1.093599, SB loss: 0.046308
Epoch: [8/20] Iter:[240/290], Time: 0.88, lr: [0.00011875129488205389], Loss: 1.262986, Acc:0.568331, Semantic loss: 0.124767, BCE loss: 1.091959, SB loss: 0.046261
Epoch: [8/20] Iter:[250/290], Time: 0.88, lr: [0.00011842137921129854], Loss: 1.252625, Acc:0.567833, Semantic loss: 0.124696, BCE loss: 1.081753, SB loss: 0.046177
Epoch: [8/20] Iter:[260/290], Time: 0.88, lr: [0.00011809136138348431], Loss: 1.251676, Acc:0.568401, Semantic loss: 0.124671, BCE loss: 1.080814, SB loss: 0.046191
Epoch: [8/20] Iter:[270/290], Time: 0.88, lr: [0.00011776124104957215], Loss: 1.254371, Acc:0.569355, Semantic loss: 0.124623, BCE loss: 1.083577, SB loss: 0.046171
Epoch: [8/20] Iter:[280/290], Time: 0.88, lr: [0.00011743101785823923], Loss: 1.253150, Acc:0.569954, Semantic loss: 0.124295, BCE loss: 1.082782, SB loss: 0.046073
0
10
20
30
40
50
60
70
0 [0.         0.44855372 0.25675882 0.13171812 0.24661648 0.10160884
 0.12416607 0.05406556] 0.1704359507742934
1 [0.         0.48984967 0.28603715 0.23589236 0.37931909 0.062671
 0.08827198 0.32963254] 0.2339592232450195
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.214, MeanIU:  0.2340, Best_mIoU:  0.2340
[0.         0.48984967 0.28603715 0.23589236 0.37931909 0.062671
 0.08827198 0.32963254]
Epoch: [9/20] Iter:[0/290], Time: 0.75, lr: [6.152153759656528e-05], Loss: 1.444320, Acc:0.568042, Semantic loss: 0.114457, BCE loss: 1.285543, SB loss: 0.044319
Epoch: [9/20] Iter:[10/290], Time: 0.84, lr: [6.134793862347577e-05], Loss: 1.478107, Acc:0.610523, Semantic loss: 0.128094, BCE loss: 1.305021, SB loss: 0.044992
Epoch: [9/20] Iter:[20/290], Time: 0.86, lr: [6.117428505083533e-05], Loss: 1.247613, Acc:0.594604, Semantic loss: 0.121356, BCE loss: 1.081597, SB loss: 0.044660
Epoch: [9/20] Iter:[30/290], Time: 0.87, lr: [6.100057668915133e-05], Loss: 1.187347, Acc:0.593554, Semantic loss: 0.118255, BCE loss: 1.025214, SB loss: 0.043878
Epoch: [9/20] Iter:[40/290], Time: 0.87, lr: [6.082681334767156e-05], Loss: 1.174986, Acc:0.593849, Semantic loss: 0.118538, BCE loss: 1.011996, SB loss: 0.044452
Epoch: [9/20] Iter:[50/290], Time: 0.87, lr: [6.065299483437197e-05], Loss: 1.157882, Acc:0.586140, Semantic loss: 0.121277, BCE loss: 0.991373, SB loss: 0.045232
Epoch: [9/20] Iter:[60/290], Time: 0.87, lr: [6.047912095594403e-05], Loss: 1.148597, Acc:0.576851, Semantic loss: 0.119920, BCE loss: 0.983619, SB loss: 0.045059
Epoch: [9/20] Iter:[70/290], Time: 0.88, lr: [6.030519151778209e-05], Loss: 1.152864, Acc:0.578596, Semantic loss: 0.119944, BCE loss: 0.987886, SB loss: 0.045033
Epoch: [9/20] Iter:[80/290], Time: 0.87, lr: [6.01312063239704e-05], Loss: 1.180811, Acc:0.579554, Semantic loss: 0.121026, BCE loss: 1.014407, SB loss: 0.045378
Epoch: [9/20] Iter:[90/290], Time: 0.87, lr: [5.9957165177270084e-05], Loss: 1.196992, Acc:0.582275, Semantic loss: 0.119999, BCE loss: 1.031857, SB loss: 0.045136
Epoch: [9/20] Iter:[100/290], Time: 0.88, lr: [5.978306787910593e-05], Loss: 1.197217, Acc:0.583856, Semantic loss: 0.120303, BCE loss: 1.031928, SB loss: 0.044986
Epoch: [9/20] Iter:[110/290], Time: 0.87, lr: [5.960891422955294e-05], Loss: 1.202171, Acc:0.584610, Semantic loss: 0.120218, BCE loss: 1.037103, SB loss: 0.044850
Epoch: [9/20] Iter:[120/290], Time: 0.87, lr: [5.9434704027322796e-05], Loss: 1.212163, Acc:0.588946, Semantic loss: 0.119846, BCE loss: 1.047629, SB loss: 0.044688
Epoch: [9/20] Iter:[130/290], Time: 0.88, lr: [5.9260437069750006e-05], Loss: 1.204988, Acc:0.588016, Semantic loss: 0.119698, BCE loss: 1.040431, SB loss: 0.044859
Epoch: [9/20] Iter:[140/290], Time: 0.87, lr: [5.9086113152778044e-05], Loss: 1.207300, Acc:0.587721, Semantic loss: 0.119976, BCE loss: 1.042358, SB loss: 0.044966
Epoch: [9/20] Iter:[150/290], Time: 0.87, lr: [5.891173207094515e-05], Loss: 1.206540, Acc:0.584957, Semantic loss: 0.119660, BCE loss: 1.042035, SB loss: 0.044846
Epoch: [9/20] Iter:[160/290], Time: 0.88, lr: [5.873729361737003e-05], Loss: 1.227868, Acc:0.588264, Semantic loss: 0.119843, BCE loss: 1.063214, SB loss: 0.044811
Epoch: [9/20] Iter:[170/290], Time: 0.87, lr: [5.856279758373729e-05], Loss: 1.221133, Acc:0.582866, Semantic loss: 0.119666, BCE loss: 1.056757, SB loss: 0.044709
Epoch: [9/20] Iter:[180/290], Time: 0.87, lr: [5.838824376028274e-05], Loss: 1.228856, Acc:0.582189, Semantic loss: 0.119893, BCE loss: 1.064192, SB loss: 0.044772
Epoch: [9/20] Iter:[190/290], Time: 0.87, lr: [5.82136319357785e-05], Loss: 1.233921, Acc:0.582609, Semantic loss: 0.119883, BCE loss: 1.069218, SB loss: 0.044820
Epoch: [9/20] Iter:[200/290], Time: 0.88, lr: [5.8038961897517816e-05], Loss: 1.226155, Acc:0.582018, Semantic loss: 0.120176, BCE loss: 1.060906, SB loss: 0.045074
Epoch: [9/20] Iter:[210/290], Time: 0.87, lr: [5.786423343129975e-05], Loss: 1.233194, Acc:0.582401, Semantic loss: 0.120233, BCE loss: 1.067970, SB loss: 0.044991
Epoch: [9/20] Iter:[220/290], Time: 0.88, lr: [5.7689446321413634e-05], Loss: 1.230799, Acc:0.580846, Semantic loss: 0.120245, BCE loss: 1.065553, SB loss: 0.045001
Epoch: [9/20] Iter:[230/290], Time: 0.88, lr: [5.75146003506233e-05], Loss: 1.226704, Acc:0.579738, Semantic loss: 0.120666, BCE loss: 1.061038, SB loss: 0.045000
Epoch: [9/20] Iter:[240/290], Time: 0.87, lr: [5.73396953001511e-05], Loss: 1.227097, Acc:0.580130, Semantic loss: 0.120929, BCE loss: 1.060976, SB loss: 0.045192
Epoch: [9/20] Iter:[250/290], Time: 0.88, lr: [5.716473094966179e-05], Loss: 1.223022, Acc:0.579487, Semantic loss: 0.121098, BCE loss: 1.056672, SB loss: 0.045252
Epoch: [9/20] Iter:[260/290], Time: 0.88, lr: [5.6989707077245915e-05], Loss: 1.221698, Acc:0.580611, Semantic loss: 0.120670, BCE loss: 1.055913, SB loss: 0.045115
Epoch: [9/20] Iter:[270/290], Time: 0.87, lr: [5.681462345940331e-05], Loss: 1.223578, Acc:0.580757, Semantic loss: 0.120947, BCE loss: 1.057508, SB loss: 0.045123
Epoch: [9/20] Iter:[280/290], Time: 0.88, lr: [5.6639479871026194e-05], Loss: 1.226693, Acc:0.580623, Semantic loss: 0.120891, BCE loss: 1.060663, SB loss: 0.045140
0
10
20
30
40
50
60
70
0 [0.         0.41927166 0.2711298  0.1740583  0.26198291 0.06209377
 0.14734183 0.06551328] 0.17517394279011292
1 [0.         0.46654548 0.26319068 0.30771336 0.39638445 0.02688441
 0.15554029 0.30755902] 0.24047721234089833
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.219, MeanIU:  0.2405, Best_mIoU:  0.2405
[0.         0.46654548 0.26319068 0.30771336 0.39638445 0.02688441
 0.15554029 0.30755902]
Epoch: [10/20] Iter:[0/290], Time: 0.75, lr: [2.6482314014831744e-05], Loss: 1.148729, Acc:0.497997, Semantic loss: 0.123294, BCE loss: 0.975360, SB loss: 0.050075
Epoch: [10/20] Iter:[10/290], Time: 0.89, lr: [2.6400113335028084e-05], Loss: 1.119691, Acc:0.544055, Semantic loss: 0.117830, BCE loss: 0.956511, SB loss: 0.045350
Epoch: [10/20] Iter:[20/290], Time: 0.88, lr: [2.631788420710148e-05], Loss: 1.170847, Acc:0.577802, Semantic loss: 0.117542, BCE loss: 1.008066, SB loss: 0.045239
Epoch: [10/20] Iter:[30/290], Time: 0.88, lr: [2.623562652237661e-05], Loss: 1.206598, Acc:0.585046, Semantic loss: 0.118566, BCE loss: 1.042165, SB loss: 0.045867
Epoch: [10/20] Iter:[40/290], Time: 0.88, lr: [2.6153340171382804e-05], Loss: 1.231094, Acc:0.581415, Semantic loss: 0.122270, BCE loss: 1.062483, SB loss: 0.046342
Epoch: [10/20] Iter:[50/290], Time: 0.89, lr: [2.6071025043845475e-05], Loss: 1.241700, Acc:0.576649, Semantic loss: 0.123721, BCE loss: 1.071506, SB loss: 0.046473
Epoch: [10/20] Iter:[60/290], Time: 0.88, lr: [2.5988681028677295e-05], Loss: 1.206180, Acc:0.572341, Semantic loss: 0.122573, BCE loss: 1.037617, SB loss: 0.045990
Epoch: [10/20] Iter:[70/290], Time: 0.88, lr: [2.5906308013969415e-05], Loss: 1.205182, Acc:0.576703, Semantic loss: 0.122879, BCE loss: 1.035776, SB loss: 0.046528
Epoch: [10/20] Iter:[80/290], Time: 0.88, lr: [2.5823905886982364e-05], Loss: 1.218953, Acc:0.578861, Semantic loss: 0.121855, BCE loss: 1.050911, SB loss: 0.046187
Epoch: [10/20] Iter:[90/290], Time: 0.88, lr: [2.5741474534137018e-05], Loss: 1.203834, Acc:0.576526, Semantic loss: 0.121224, BCE loss: 1.036658, SB loss: 0.045952
Epoch: [10/20] Iter:[100/290], Time: 0.88, lr: [2.565901384100525e-05], Loss: 1.214116, Acc:0.578152, Semantic loss: 0.120518, BCE loss: 1.047912, SB loss: 0.045687
Epoch: [10/20] Iter:[110/290], Time: 0.88, lr: [2.5576523692300575e-05], Loss: 1.231057, Acc:0.581518, Semantic loss: 0.120412, BCE loss: 1.064936, SB loss: 0.045709
Epoch: [10/20] Iter:[120/290], Time: 0.88, lr: [2.549400397186859e-05], Loss: 1.228362, Acc:0.582904, Semantic loss: 0.119774, BCE loss: 1.063056, SB loss: 0.045532
Epoch: [10/20] Iter:[130/290], Time: 0.88, lr: [2.5411454562677336e-05], Loss: 1.217843, Acc:0.582611, Semantic loss: 0.119435, BCE loss: 1.052996, SB loss: 0.045413
Epoch: [10/20] Iter:[140/290], Time: 0.88, lr: [2.532887534680739e-05], Loss: 1.224698, Acc:0.585514, Semantic loss: 0.119423, BCE loss: 1.060057, SB loss: 0.045218
Epoch: [10/20] Iter:[150/290], Time: 0.88, lr: [2.5246266205441993e-05], Loss: 1.228273, Acc:0.585053, Semantic loss: 0.119084, BCE loss: 1.064075, SB loss: 0.045114
Epoch: [10/20] Iter:[160/290], Time: 0.88, lr: [2.5163627018856862e-05], Loss: 1.218455, Acc:0.583778, Semantic loss: 0.119233, BCE loss: 1.054048, SB loss: 0.045174
Epoch: [10/20] Iter:[170/290], Time: 0.88, lr: [2.5080957666409956e-05], Loss: 1.217650, Acc:0.583466, Semantic loss: 0.118886, BCE loss: 1.053674, SB loss: 0.045089
Epoch: [10/20] Iter:[180/290], Time: 0.88, lr: [2.4998258026531043e-05], Loss: 1.209086, Acc:0.582946, Semantic loss: 0.118884, BCE loss: 1.045085, SB loss: 0.045117
Epoch: [10/20] Iter:[190/290], Time: 0.88, lr: [2.4915527976711102e-05], Loss: 1.208595, Acc:0.582479, Semantic loss: 0.118684, BCE loss: 1.044854, SB loss: 0.045057
Epoch: [10/20] Iter:[200/290], Time: 0.88, lr: [2.4832767393491636e-05], Loss: 1.210978, Acc:0.581491, Semantic loss: 0.118957, BCE loss: 1.046835, SB loss: 0.045186
Epoch: [10/20] Iter:[210/290], Time: 0.88, lr: [2.4749976152453675e-05], Loss: 1.206038, Acc:0.581848, Semantic loss: 0.119293, BCE loss: 1.041587, SB loss: 0.045157
Epoch: [10/20] Iter:[220/290], Time: 0.88, lr: [2.4667154128206823e-05], Loss: 1.205073, Acc:0.582545, Semantic loss: 0.119104, BCE loss: 1.040973, SB loss: 0.044996
Epoch: [10/20] Iter:[230/290], Time: 0.88, lr: [2.4584301194377904e-05], Loss: 1.202370, Acc:0.584342, Semantic loss: 0.118919, BCE loss: 1.038538, SB loss: 0.044913
Epoch: [10/20] Iter:[240/290], Time: 0.88, lr: [2.4501417223599644e-05], Loss: 1.207826, Acc:0.585111, Semantic loss: 0.118884, BCE loss: 1.044032, SB loss: 0.044910
Epoch: [10/20] Iter:[250/290], Time: 0.88, lr: [2.441850208749901e-05], Loss: 1.207527, Acc:0.585471, Semantic loss: 0.118863, BCE loss: 1.043840, SB loss: 0.044824
Epoch: [10/20] Iter:[260/290], Time: 0.88, lr: [2.4335555656685503e-05], Loss: 1.207359, Acc:0.585962, Semantic loss: 0.118814, BCE loss: 1.043738, SB loss: 0.044807
Epoch: [10/20] Iter:[270/290], Time: 0.88, lr: [2.4252577800739147e-05], Loss: 1.205543, Acc:0.584900, Semantic loss: 0.118752, BCE loss: 1.041925, SB loss: 0.044865
Epoch: [10/20] Iter:[280/290], Time: 0.88, lr: [2.416956838819841e-05], Loss: 1.205513, Acc:0.584250, Semantic loss: 0.118595, BCE loss: 1.042131, SB loss: 0.044787
0
10
20
30
40
50
60
70
0 [0.         0.44395963 0.26414189 0.14732725 0.2469146  0.11019711
 0.13643019 0.06553247] 0.1768128928297623
1 [0.         0.46136513 0.22830932 0.32071195 0.28270203 0.04036247
 0.1310878  0.36192758] 0.2283082854011055
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.244, MeanIU:  0.2283, Best_mIoU:  0.2405
[0.         0.46136513 0.22830932 0.32071195 0.28270203 0.04036247
 0.1310878  0.36192758]
Epoch: [11/20] Iter:[0/290], Time: 0.73, lr: [9.985527756770789e-06], Loss: 1.902496, Acc:0.702589, Semantic loss: 0.110155, BCE loss: 1.747968, SB loss: 0.044372
Epoch: [11/20] Iter:[10/290], Time: 0.83, lr: [9.951088296819287e-06], Loss: 1.116501, Acc:0.574096, Semantic loss: 0.108829, BCE loss: 0.965155, SB loss: 0.042518
Epoch: [11/20] Iter:[20/290], Time: 0.84, lr: [9.91663558833972e-06], Loss: 1.149731, Acc:0.573952, Semantic loss: 0.111824, BCE loss: 0.994041, SB loss: 0.043866
Epoch: [11/20] Iter:[30/290], Time: 0.85, lr: [9.88216957505307e-06], Loss: 1.168716, Acc:0.579284, Semantic loss: 0.112924, BCE loss: 1.011740, SB loss: 0.044051
Epoch: [11/20] Iter:[40/290], Time: 0.86, lr: [9.847690200222132e-06], Loss: 1.144893, Acc:0.579287, Semantic loss: 0.112494, BCE loss: 0.988323, SB loss: 0.044077
Epoch: [11/20] Iter:[50/290], Time: 0.86, lr: [9.813197406646001e-06], Loss: 1.151520, Acc:0.581589, Semantic loss: 0.113664, BCE loss: 0.993409, SB loss: 0.044447
Epoch: [11/20] Iter:[60/290], Time: 0.86, lr: [9.778691136654457e-06], Loss: 1.160696, Acc:0.584683, Semantic loss: 0.114434, BCE loss: 1.001851, SB loss: 0.044411
Epoch: [11/20] Iter:[70/290], Time: 0.87, lr: [9.744171332102235e-06], Loss: 1.139644, Acc:0.582047, Semantic loss: 0.114780, BCE loss: 0.980345, SB loss: 0.044519
Epoch: [11/20] Iter:[80/290], Time: 0.86, lr: [9.709637934363264e-06], Loss: 1.156323, Acc:0.590435, Semantic loss: 0.115007, BCE loss: 0.996891, SB loss: 0.044425
Epoch: [11/20] Iter:[90/290], Time: 0.87, lr: [9.675090884324736e-06], Loss: 1.180868, Acc:0.591875, Semantic loss: 0.115951, BCE loss: 1.020311, SB loss: 0.044606
Epoch: [11/20] Iter:[100/290], Time: 0.87, lr: [9.640530122381145e-06], Loss: 1.168547, Acc:0.589956, Semantic loss: 0.116132, BCE loss: 1.007941, SB loss: 0.044475
Epoch: [11/20] Iter:[110/290], Time: 0.86, lr: [9.605955588428187e-06], Loss: 1.178263, Acc:0.594821, Semantic loss: 0.115936, BCE loss: 1.018039, SB loss: 0.044288
Epoch: [11/20] Iter:[120/290], Time: 0.87, lr: [9.57136722185658e-06], Loss: 1.170960, Acc:0.596102, Semantic loss: 0.115314, BCE loss: 1.011473, SB loss: 0.044173
Epoch: [11/20] Iter:[130/290], Time: 0.87, lr: [9.536764961545762e-06], Loss: 1.173859, Acc:0.594019, Semantic loss: 0.116185, BCE loss: 1.013355, SB loss: 0.044320
Epoch: [11/20] Iter:[140/290], Time: 0.86, lr: [9.502148745857526e-06], Loss: 1.174809, Acc:0.591878, Semantic loss: 0.116405, BCE loss: 1.013916, SB loss: 0.044488
Epoch: [11/20] Iter:[150/290], Time: 0.87, lr: [9.46751851262949e-06], Loss: 1.180918, Acc:0.594254, Semantic loss: 0.116275, BCE loss: 1.020307, SB loss: 0.044335
Epoch: [11/20] Iter:[160/290], Time: 0.87, lr: [9.432874199168515e-06], Loss: 1.182862, Acc:0.595098, Semantic loss: 0.116659, BCE loss: 1.021854, SB loss: 0.044349
Epoch: [11/20] Iter:[170/290], Time: 0.87, lr: [9.398215742243974e-06], Loss: 1.181101, Acc:0.593726, Semantic loss: 0.116316, BCE loss: 1.020574, SB loss: 0.044211
Epoch: [11/20] Iter:[180/290], Time: 0.87, lr: [9.363543078080927e-06], Loss: 1.176883, Acc:0.592415, Semantic loss: 0.116324, BCE loss: 1.016309, SB loss: 0.044250
Epoch: [11/20] Iter:[190/290], Time: 0.87, lr: [9.328856142353173e-06], Loss: 1.183939, Acc:0.592523, Semantic loss: 0.116811, BCE loss: 1.022829, SB loss: 0.044299
Epoch: [11/20] Iter:[200/290], Time: 0.87, lr: [9.294154870176192e-06], Loss: 1.186660, Acc:0.593584, Semantic loss: 0.116602, BCE loss: 1.025645, SB loss: 0.044413
Epoch: [11/20] Iter:[210/290], Time: 0.87, lr: [9.259439196099957e-06], Loss: 1.193115, Acc:0.595672, Semantic loss: 0.116418, BCE loss: 1.032356, SB loss: 0.044341
Epoch: [11/20] Iter:[220/290], Time: 0.87, lr: [9.224709054101622e-06], Loss: 1.195870, Acc:0.595473, Semantic loss: 0.116681, BCE loss: 1.034714, SB loss: 0.044474
Epoch: [11/20] Iter:[230/290], Time: 0.87, lr: [9.18996437757811e-06], Loss: 1.202492, Acc:0.596841, Semantic loss: 0.116563, BCE loss: 1.041494, SB loss: 0.044435
Epoch: [11/20] Iter:[240/290], Time: 0.87, lr: [9.15520509933853e-06], Loss: 1.201645, Acc:0.595822, Semantic loss: 0.116784, BCE loss: 1.040421, SB loss: 0.044440
Epoch: [11/20] Iter:[250/290], Time: 0.87, lr: [9.120431151596509e-06], Loss: 1.202563, Acc:0.595355, Semantic loss: 0.116891, BCE loss: 1.041288, SB loss: 0.044383
Epoch: [11/20] Iter:[260/290], Time: 0.87, lr: [9.085642465962343e-06], Loss: 1.201689, Acc:0.594255, Semantic loss: 0.117292, BCE loss: 1.040080, SB loss: 0.044317
Epoch: [11/20] Iter:[270/290], Time: 0.87, lr: [9.050838973435063e-06], Loss: 1.200725, Acc:0.594586, Semantic loss: 0.117250, BCE loss: 1.039252, SB loss: 0.044223
Epoch: [11/20] Iter:[280/290], Time: 0.87, lr: [9.016020604394316e-06], Loss: 1.202627, Acc:0.595512, Semantic loss: 0.117434, BCE loss: 1.040971, SB loss: 0.044222
0
10
20
30
40
50
60
70
0 [0.         0.43614878 0.29374347 0.15784291 0.26798225 0.10932091
 0.13202796 0.07905189] 0.1845147704964983
1 [0.         0.45498996 0.23365722 0.30071024 0.33157131 0.04702297
 0.13602366 0.41837588] 0.24029390340655932
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.225, MeanIU:  0.2403, Best_mIoU:  0.2405
[0.         0.45498996 0.23365722 0.30071024 0.33157131 0.04702297
 0.13602366 0.41837588]
Epoch: [12/20] Iter:[0/290], Time: 0.75, lr: [3.276210238778349e-06], Loss: 1.175703, Acc:0.473523, Semantic loss: 0.113460, BCE loss: 1.020550, SB loss: 0.041694
Epoch: [12/20] Iter:[10/290], Time: 0.86, lr: [3.2634980590657166e-06], Loss: 1.261645, Acc:0.605619, Semantic loss: 0.117043, BCE loss: 1.100254, SB loss: 0.044348
Epoch: [12/20] Iter:[20/290], Time: 0.86, lr: [3.2507803750356083e-06], Loss: 1.297951, Acc:0.605175, Semantic loss: 0.119387, BCE loss: 1.134528, SB loss: 0.044036
Epoch: [12/20] Iter:[30/290], Time: 0.88, lr: [3.2380571603571358e-06], Loss: 1.260525, Acc:0.605032, Semantic loss: 0.118097, BCE loss: 1.098122, SB loss: 0.044306
Epoch: [12/20] Iter:[40/290], Time: 0.88, lr: [3.225328388457898e-06], Loss: 1.201143, Acc:0.594392, Semantic loss: 0.116841, BCE loss: 1.040083, SB loss: 0.044220
Epoch: [12/20] Iter:[50/290], Time: 0.87, lr: [3.212594032520688e-06], Loss: 1.178753, Acc:0.599718, Semantic loss: 0.115717, BCE loss: 1.018740, SB loss: 0.044296
Epoch: [12/20] Iter:[60/290], Time: 0.87, lr: [3.1998540654801573e-06], Loss: 1.181941, Acc:0.597439, Semantic loss: 0.116145, BCE loss: 1.021595, SB loss: 0.044201
Epoch: [12/20] Iter:[70/290], Time: 0.87, lr: [3.187108460019404e-06], Loss: 1.163539, Acc:0.590782, Semantic loss: 0.116054, BCE loss: 1.003304, SB loss: 0.044180
Epoch: [12/20] Iter:[80/290], Time: 0.87, lr: [3.1743571885665094e-06], Loss: 1.189226, Acc:0.597034, Semantic loss: 0.116809, BCE loss: 1.028273, SB loss: 0.044144
Epoch: [12/20] Iter:[90/290], Time: 0.87, lr: [3.16160022329101e-06], Loss: 1.180684, Acc:0.594355, Semantic loss: 0.116810, BCE loss: 1.019780, SB loss: 0.044094
Epoch: [12/20] Iter:[100/290], Time: 0.87, lr: [3.148837536100298e-06], Loss: 1.165408, Acc:0.588292, Semantic loss: 0.116548, BCE loss: 1.004867, SB loss: 0.043993
Epoch: [12/20] Iter:[110/290], Time: 0.86, lr: [3.1360690986359657e-06], Loss: 1.145258, Acc:0.587171, Semantic loss: 0.115923, BCE loss: 0.985448, SB loss: 0.043886
Epoch: [12/20] Iter:[120/290], Time: 0.87, lr: [3.1232948822700672e-06], Loss: 1.165190, Acc:0.591993, Semantic loss: 0.115578, BCE loss: 1.005953, SB loss: 0.043659
Epoch: [12/20] Iter:[130/290], Time: 0.87, lr: [3.1105148581013306e-06], Loss: 1.169714, Acc:0.595507, Semantic loss: 0.115652, BCE loss: 1.010390, SB loss: 0.043673
Epoch: [12/20] Iter:[140/290], Time: 0.87, lr: [3.097728996951278e-06], Loss: 1.163416, Acc:0.594561, Semantic loss: 0.115392, BCE loss: 1.004477, SB loss: 0.043547
Epoch: [12/20] Iter:[150/290], Time: 0.87, lr: [3.084937269360291e-06], Loss: 1.165265, Acc:0.596212, Semantic loss: 0.115589, BCE loss: 1.006062, SB loss: 0.043614
Epoch: [12/20] Iter:[160/290], Time: 0.87, lr: [3.072139645583585e-06], Loss: 1.172391, Acc:0.595618, Semantic loss: 0.115739, BCE loss: 1.012990, SB loss: 0.043661
Epoch: [12/20] Iter:[170/290], Time: 0.87, lr: [3.0593360955871245e-06], Loss: 1.176104, Acc:0.597827, Semantic loss: 0.115848, BCE loss: 1.016547, SB loss: 0.043708
Epoch: [12/20] Iter:[180/290], Time: 0.87, lr: [3.0465265890434432e-06], Loss: 1.172029, Acc:0.597532, Semantic loss: 0.115343, BCE loss: 1.013120, SB loss: 0.043566
Epoch: [12/20] Iter:[190/290], Time: 0.87, lr: [3.0337110953273994e-06], Loss: 1.170698, Acc:0.598068, Semantic loss: 0.115025, BCE loss: 1.012173, SB loss: 0.043499
Epoch: [12/20] Iter:[200/290], Time: 0.87, lr: [3.0208895835118364e-06], Loss: 1.175982, Acc:0.596924, Semantic loss: 0.115353, BCE loss: 1.017056, SB loss: 0.043572
Epoch: [12/20] Iter:[210/290], Time: 0.87, lr: [3.00806202236317e-06], Loss: 1.177575, Acc:0.597400, Semantic loss: 0.115382, BCE loss: 1.018666, SB loss: 0.043528
Epoch: [12/20] Iter:[220/290], Time: 0.87, lr: [2.9952283803368813e-06], Loss: 1.178026, Acc:0.596494, Semantic loss: 0.115411, BCE loss: 1.019091, SB loss: 0.043525
Epoch: [12/20] Iter:[230/290], Time: 0.87, lr: [2.9823886255729303e-06], Loss: 1.183827, Acc:0.596713, Semantic loss: 0.115643, BCE loss: 1.024606, SB loss: 0.043578
Epoch: [12/20] Iter:[240/290], Time: 0.87, lr: [2.96954272589107e-06], Loss: 1.184933, Acc:0.595741, Semantic loss: 0.116155, BCE loss: 1.025231, SB loss: 0.043548
Epoch: [12/20] Iter:[250/290], Time: 0.87, lr: [2.9566906487860745e-06], Loss: 1.187864, Acc:0.596724, Semantic loss: 0.116247, BCE loss: 1.027968, SB loss: 0.043650
Epoch: [12/20] Iter:[260/290], Time: 0.87, lr: [2.9438323614228736e-06], Loss: 1.183457, Acc:0.595636, Semantic loss: 0.116223, BCE loss: 1.023630, SB loss: 0.043605
Epoch: [12/20] Iter:[270/290], Time: 0.87, lr: [2.930967830631583e-06], Loss: 1.184817, Acc:0.596641, Semantic loss: 0.116121, BCE loss: 1.025097, SB loss: 0.043598
Epoch: [12/20] Iter:[280/290], Time: 0.87, lr: [2.918097022902444e-06], Loss: 1.188695, Acc:0.595960, Semantic loss: 0.116345, BCE loss: 1.028678, SB loss: 0.043672
0
10
20
30
40
50
60
70
0 [0.         0.44538336 0.28083024 0.15962656 0.26728605 0.1333153
 0.13037019 0.0763202 ] 0.18664148870112374
1 [0.         0.44069022 0.19184123 0.3008369  0.37722883 0.04843553
 0.1436479  0.40434499] 0.23837820027478485
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.257, MeanIU:  0.2384, Best_mIoU:  0.2405
[0.         0.44069022 0.19184123 0.3008369  0.37722883 0.04843553
 0.1436479  0.40434499]
/content/AML2024/PIDNet
/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
Seeding with 304
=> creating output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2
=> creating log/loveDa/pidnet_small/pidnet_small_loveda_3b_AUG_CHANCE+AUG2_2025-01-21-08-50
Namespace(cfg='configs/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/cityscapes/val.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDA-Urban/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 20
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: True
  AUG1: False
  AUG2: True
  AUG3: False
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 1
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.001
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: True
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
/content/AML2024/PIDNet/tools/../models/pidnet.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']
Attention!!!
Loaded 302 parameters!
Over!!!
/content/AML2024/PIDNet/tools/train.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_state_file, map_location={'cuda:0': 'cpu'})
=> loaded checkpoint (epoch 13)
/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Epoch: [13/20] Iter:[0/286], Time: 6.12, lr: [1.121783273664456e-06], Loss: 1.077530, Acc:0.599522, Semantic loss: 0.114736, BCE loss: 0.922882, SB loss: 0.039912
Epoch: [13/20] Iter:[10/286], Time: 1.39, lr: [1.1167390301159452e-06], Loss: 1.180641, Acc:0.577282, Semantic loss: 0.122321, BCE loss: 1.013440, SB loss: 0.044880
Epoch: [13/20] Iter:[20/286], Time: 1.16, lr: [1.1116922536697725e-06], Loss: 1.158906, Acc:0.591956, Semantic loss: 0.120148, BCE loss: 0.994017, SB loss: 0.044741
Epoch: [13/20] Iter:[30/286], Time: 1.05, lr: [1.106642930264817e-06], Loss: 1.169491, Acc:0.595696, Semantic loss: 0.119543, BCE loss: 1.005146, SB loss: 0.044803
Epoch: [13/20] Iter:[40/286], Time: 1.02, lr: [1.101591045690181e-06], Loss: 1.176477, Acc:0.600962, Semantic loss: 0.120119, BCE loss: 1.011293, SB loss: 0.045066
Epoch: [13/20] Iter:[50/286], Time: 1.00, lr: [1.096536585582821e-06], Loss: 1.201383, Acc:0.596438, Semantic loss: 0.121348, BCE loss: 1.035004, SB loss: 0.045032
Epoch: [13/20] Iter:[60/286], Time: 0.97, lr: [1.0914795354251324e-06], Loss: 1.190529, Acc:0.594596, Semantic loss: 0.120436, BCE loss: 1.025049, SB loss: 0.045045
Epoch: [13/20] Iter:[70/286], Time: 0.96, lr: [1.0864198805424793e-06], Loss: 1.200759, Acc:0.594059, Semantic loss: 0.120394, BCE loss: 1.035575, SB loss: 0.044790
Epoch: [13/20] Iter:[80/286], Time: 0.96, lr: [1.0813576061006763e-06], Loss: 1.191915, Acc:0.593630, Semantic loss: 0.120473, BCE loss: 1.026489, SB loss: 0.044953
Epoch: [13/20] Iter:[90/286], Time: 0.95, lr: [1.0762926971034137e-06], Loss: 1.195656, Acc:0.590434, Semantic loss: 0.119527, BCE loss: 1.031357, SB loss: 0.044773
Epoch: [13/20] Iter:[100/286], Time: 0.94, lr: [1.0712251383896279e-06], Loss: 1.207350, Acc:0.592179, Semantic loss: 0.119412, BCE loss: 1.043174, SB loss: 0.044764
Epoch: [13/20] Iter:[110/286], Time: 0.94, lr: [1.0661549146308161e-06], Loss: 1.206734, Acc:0.593386, Semantic loss: 0.118601, BCE loss: 1.043575, SB loss: 0.044558
Epoch: [13/20] Iter:[120/286], Time: 0.93, lr: [1.0610820103282901e-06], Loss: 1.211758, Acc:0.593651, Semantic loss: 0.118152, BCE loss: 1.049255, SB loss: 0.044350
Epoch: [13/20] Iter:[130/286], Time: 0.93, lr: [1.0560064098103753e-06], Loss: 1.208690, Acc:0.592478, Semantic loss: 0.117805, BCE loss: 1.046611, SB loss: 0.044274
Epoch: [13/20] Iter:[140/286], Time: 0.93, lr: [1.0509280972295425e-06], Loss: 1.206555, Acc:0.595951, Semantic loss: 0.117601, BCE loss: 1.044717, SB loss: 0.044237
Epoch: [13/20] Iter:[150/286], Time: 0.92, lr: [1.0458470565594814e-06], Loss: 1.219176, Acc:0.596853, Semantic loss: 0.117203, BCE loss: 1.057864, SB loss: 0.044109
Epoch: [13/20] Iter:[160/286], Time: 0.92, lr: [1.040763271592106e-06], Loss: 1.219227, Acc:0.593240, Semantic loss: 0.118279, BCE loss: 1.056463, SB loss: 0.044485
Epoch: [13/20] Iter:[170/286], Time: 0.92, lr: [1.0356767259344934e-06], Loss: 1.217880, Acc:0.594745, Semantic loss: 0.118145, BCE loss: 1.055256, SB loss: 0.044479
Epoch: [13/20] Iter:[180/286], Time: 0.92, lr: [1.0305874030057582e-06], Loss: 1.219952, Acc:0.596519, Semantic loss: 0.117871, BCE loss: 1.057712, SB loss: 0.044369
Epoch: [13/20] Iter:[190/286], Time: 0.92, lr: [1.0254952860338513e-06], Loss: 1.229469, Acc:0.598591, Semantic loss: 0.118093, BCE loss: 1.067007, SB loss: 0.044370
Epoch: [13/20] Iter:[200/286], Time: 0.92, lr: [1.0204003580522889e-06], Loss: 1.227293, Acc:0.598251, Semantic loss: 0.118107, BCE loss: 1.064888, SB loss: 0.044298
Epoch: [13/20] Iter:[210/286], Time: 0.92, lr: [1.0153026018968066e-06], Loss: 1.227622, Acc:0.598051, Semantic loss: 0.118054, BCE loss: 1.065309, SB loss: 0.044258
Epoch: [13/20] Iter:[220/286], Time: 0.92, lr: [1.0102020002019372e-06], Loss: 1.238643, Acc:0.598393, Semantic loss: 0.118702, BCE loss: 1.075525, SB loss: 0.044416
Epoch: [13/20] Iter:[230/286], Time: 0.92, lr: [1.0050985353975094e-06], Loss: 1.240060, Acc:0.598096, Semantic loss: 0.118797, BCE loss: 1.076886, SB loss: 0.044376
Epoch: [13/20] Iter:[240/286], Time: 0.92, lr: [9.999921897050657e-07], Loss: 1.237546, Acc:0.598913, Semantic loss: 0.118690, BCE loss: 1.074559, SB loss: 0.044297
Epoch: [13/20] Iter:[250/286], Time: 0.92, lr: [9.948829451341966e-07], Loss: 1.232493, Acc:0.599321, Semantic loss: 0.118484, BCE loss: 1.069653, SB loss: 0.044356
Epoch: [13/20] Iter:[260/286], Time: 0.92, lr: [9.897707834787882e-07], Loss: 1.233427, Acc:0.600018, Semantic loss: 0.118587, BCE loss: 1.070487, SB loss: 0.044353
Epoch: [13/20] Iter:[270/286], Time: 0.91, lr: [9.846556863131846e-07], Loss: 1.232736, Acc:0.601597, Semantic loss: 0.118301, BCE loss: 1.070155, SB loss: 0.044280
Epoch: [13/20] Iter:[280/286], Time: 0.92, lr: [9.795376349882552e-07], Loss: 1.228544, Acc:0.602438, Semantic loss: 0.118156, BCE loss: 1.066232, SB loss: 0.044157
0
10
20
30
40
50
60
70
0 [0.         0.44305771 0.28653763 0.15716777 0.26122781 0.13566088
 0.12659817 0.08094702] 0.18639962336468946
1 [0.         0.46512912 0.24109152 0.31236813 0.3592621  0.05968479
 0.11857369 0.41347134] 0.24619758569201006
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.213, MeanIU:  0.2462, Best_mIoU:  0.2462
[0.         0.46512912 0.24109152 0.31236813 0.3592621  0.05968479
 0.11857369 0.41347134]
Epoch: [14/20] Iter:[0/286], Time: 0.84, lr: [3.308474463948434e-07], Loss: 1.390380, Acc:0.692992, Semantic loss: 0.133180, BCE loss: 1.211006, SB loss: 0.046194
Epoch: [14/20] Iter:[10/286], Time: 0.84, lr: [3.2911172583304454e-07], Loss: 1.102791, Acc:0.535506, Semantic loss: 0.121364, BCE loss: 0.935774, SB loss: 0.045654
Epoch: [14/20] Iter:[20/286], Time: 0.88, lr: [3.273749875456802e-07], Loss: 1.092338, Acc:0.542324, Semantic loss: 0.116923, BCE loss: 0.931074, SB loss: 0.044341
Epoch: [14/20] Iter:[30/286], Time: 0.89, lr: [3.256372249299133e-07], Loss: 1.099871, Acc:0.562698, Semantic loss: 0.116290, BCE loss: 0.939592, SB loss: 0.043989
Epoch: [14/20] Iter:[40/286], Time: 0.89, lr: [3.2389843130063926e-07], Loss: 1.151483, Acc:0.577721, Semantic loss: 0.115797, BCE loss: 0.992028, SB loss: 0.043659
Epoch: [14/20] Iter:[50/286], Time: 0.89, lr: [3.221585998889637e-07], Loss: 1.142166, Acc:0.580721, Semantic loss: 0.117586, BCE loss: 0.980466, SB loss: 0.044114
Epoch: [14/20] Iter:[60/286], Time: 0.89, lr: [3.2041772384064283e-07], Loss: 1.140959, Acc:0.579054, Semantic loss: 0.118394, BCE loss: 0.978136, SB loss: 0.044429
Epoch: [14/20] Iter:[70/286], Time: 0.89, lr: [3.186757962144856e-07], Loss: 1.144844, Acc:0.578408, Semantic loss: 0.118244, BCE loss: 0.982157, SB loss: 0.044442
Epoch: [14/20] Iter:[80/286], Time: 0.89, lr: [3.1693280998071507e-07], Loss: 1.159558, Acc:0.588331, Semantic loss: 0.117422, BCE loss: 0.998161, SB loss: 0.043976
Epoch: [14/20] Iter:[90/286], Time: 0.89, lr: [3.151887580192898e-07], Loss: 1.170611, Acc:0.593011, Semantic loss: 0.117670, BCE loss: 1.008917, SB loss: 0.044024
Epoch: [14/20] Iter:[100/286], Time: 0.90, lr: [3.134436331181818e-07], Loss: 1.178899, Acc:0.594682, Semantic loss: 0.117624, BCE loss: 1.017179, SB loss: 0.044096
Epoch: [14/20] Iter:[110/286], Time: 0.89, lr: [3.116974279716119e-07], Loss: 1.181422, Acc:0.595143, Semantic loss: 0.117209, BCE loss: 1.020060, SB loss: 0.044153
Epoch: [14/20] Iter:[120/286], Time: 0.89, lr: [3.0995013517823914e-07], Loss: 1.191238, Acc:0.596880, Semantic loss: 0.117062, BCE loss: 1.030020, SB loss: 0.044156
Epoch: [14/20] Iter:[130/286], Time: 0.89, lr: [3.0820174723930423e-07], Loss: 1.186245, Acc:0.594332, Semantic loss: 0.117023, BCE loss: 1.024991, SB loss: 0.044231
Epoch: [14/20] Iter:[140/286], Time: 0.89, lr: [3.0645225655672455e-07], Loss: 1.196179, Acc:0.597210, Semantic loss: 0.116661, BCE loss: 1.035428, SB loss: 0.044089
Epoch: [14/20] Iter:[150/286], Time: 0.89, lr: [3.047016554311398e-07], Loss: 1.193047, Acc:0.596153, Semantic loss: 0.116987, BCE loss: 1.031967, SB loss: 0.044092
Epoch: [14/20] Iter:[160/286], Time: 0.90, lr: [3.0294993605990665e-07], Loss: 1.200335, Acc:0.596722, Semantic loss: 0.117276, BCE loss: 1.038721, SB loss: 0.044337
Epoch: [14/20] Iter:[170/286], Time: 0.90, lr: [3.0119709053504017e-07], Loss: 1.200686, Acc:0.598151, Semantic loss: 0.116747, BCE loss: 1.039723, SB loss: 0.044216
Epoch: [14/20] Iter:[180/286], Time: 0.90, lr: [2.9944311084110085e-07], Loss: 1.190469, Acc:0.597362, Semantic loss: 0.116201, BCE loss: 1.030184, SB loss: 0.044084
Epoch: [14/20] Iter:[190/286], Time: 0.90, lr: [2.976879888530251e-07], Loss: 1.190648, Acc:0.599339, Semantic loss: 0.116106, BCE loss: 1.030510, SB loss: 0.044033
Epoch: [14/20] Iter:[200/286], Time: 0.90, lr: [2.9593171633389743e-07], Loss: 1.188682, Acc:0.596922, Semantic loss: 0.116094, BCE loss: 1.028523, SB loss: 0.044064
Epoch: [14/20] Iter:[210/286], Time: 0.90, lr: [2.941742849326628e-07], Loss: 1.195586, Acc:0.597326, Semantic loss: 0.116257, BCE loss: 1.035255, SB loss: 0.044075
Epoch: [14/20] Iter:[220/286], Time: 0.90, lr: [2.924156861817758e-07], Loss: 1.198152, Acc:0.597641, Semantic loss: 0.116643, BCE loss: 1.037369, SB loss: 0.044140
Epoch: [14/20] Iter:[230/286], Time: 0.90, lr: [2.9065591149478633e-07], Loss: 1.197545, Acc:0.598287, Semantic loss: 0.116787, BCE loss: 1.036634, SB loss: 0.044124
Epoch: [14/20] Iter:[240/286], Time: 0.90, lr: [2.8889495216385747e-07], Loss: 1.201134, Acc:0.598409, Semantic loss: 0.116762, BCE loss: 1.040341, SB loss: 0.044031
Epoch: [14/20] Iter:[250/286], Time: 0.90, lr: [2.8713279935721597e-07], Loss: 1.204006, Acc:0.598331, Semantic loss: 0.116799, BCE loss: 1.043226, SB loss: 0.043981
Epoch: [14/20] Iter:[260/286], Time: 0.90, lr: [2.8536944411653e-07], Loss: 1.204466, Acc:0.598077, Semantic loss: 0.116861, BCE loss: 1.043563, SB loss: 0.044042
Epoch: [14/20] Iter:[270/286], Time: 0.90, lr: [2.836048773542136e-07], Loss: 1.210011, Acc:0.598729, Semantic loss: 0.117016, BCE loss: 1.048931, SB loss: 0.044063
Epoch: [14/20] Iter:[280/286], Time: 0.90, lr: [2.818390898506549e-07], Loss: 1.209689, Acc:0.597645, Semantic loss: 0.117258, BCE loss: 1.048314, SB loss: 0.044118
0
10
20
30
40
50
60
70
0 [0.         0.43969113 0.2649386  0.14989126 0.27935331 0.13172975
 0.13193186 0.07412481] 0.1839575888593365
1 [0.         0.40751656 0.15856674 0.31102961 0.40131401 0.05549242
 0.13454093 0.40455998] 0.23412753100157457
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.208, MeanIU:  0.2341, Best_mIoU:  0.2462
[0.         0.40751656 0.15856674 0.31102961 0.40131401 0.05549242
 0.13454093 0.40455998]
Epoch: [15/20] Iter:[0/286], Time: 1.02, lr: [9.196304432153449e-08], Loss: 1.281859, Acc:0.479622, Semantic loss: 0.177918, BCE loss: 1.060332, SB loss: 0.043609
Epoch: [15/20] Iter:[10/286], Time: 0.89, lr: [9.138405303631647e-08], Loss: 1.241541, Acc:0.626631, Semantic loss: 0.113949, BCE loss: 1.085180, SB loss: 0.042412
Epoch: [15/20] Iter:[20/286], Time: 0.91, lr: [9.080465386361122e-08], Loss: 1.292345, Acc:0.626000, Semantic loss: 0.116857, BCE loss: 1.130316, SB loss: 0.045172
Epoch: [15/20] Iter:[30/286], Time: 0.91, lr: [9.022484362013594e-08], Loss: 1.240666, Acc:0.606248, Semantic loss: 0.117199, BCE loss: 1.078514, SB loss: 0.044953
Epoch: [15/20] Iter:[40/286], Time: 0.89, lr: [8.964461907484023e-08], Loss: 1.246908, Acc:0.593216, Semantic loss: 0.118481, BCE loss: 1.083920, SB loss: 0.044507
Epoch: [15/20] Iter:[50/286], Time: 0.89, lr: [8.906397694784055e-08], Loss: 1.257697, Acc:0.596944, Semantic loss: 0.117429, BCE loss: 1.096036, SB loss: 0.044232
Epoch: [15/20] Iter:[60/286], Time: 0.90, lr: [8.84829139093227e-08], Loss: 1.250679, Acc:0.590517, Semantic loss: 0.118116, BCE loss: 1.088168, SB loss: 0.044394
Epoch: [15/20] Iter:[70/286], Time: 0.89, lr: [8.790142657841167e-08], Loss: 1.260584, Acc:0.596015, Semantic loss: 0.118467, BCE loss: 1.097642, SB loss: 0.044475
Epoch: [15/20] Iter:[80/286], Time: 0.89, lr: [8.731951152200693e-08], Loss: 1.249348, Acc:0.593746, Semantic loss: 0.117696, BCE loss: 1.087184, SB loss: 0.044469
Epoch: [15/20] Iter:[90/286], Time: 0.89, lr: [8.67371652535832e-08], Loss: 1.235697, Acc:0.594273, Semantic loss: 0.117651, BCE loss: 1.073235, SB loss: 0.044811
Epoch: [15/20] Iter:[100/286], Time: 0.89, lr: [8.615438423195364e-08], Loss: 1.241301, Acc:0.595656, Semantic loss: 0.118048, BCE loss: 1.078513, SB loss: 0.044740
Epoch: [15/20] Iter:[110/286], Time: 0.89, lr: [8.557116485999536e-08], Loss: 1.231835, Acc:0.592694, Semantic loss: 0.118123, BCE loss: 1.068825, SB loss: 0.044887
Epoch: [15/20] Iter:[120/286], Time: 0.89, lr: [8.498750348333533e-08], Loss: 1.227284, Acc:0.592210, Semantic loss: 0.117534, BCE loss: 1.065112, SB loss: 0.044637
Epoch: [15/20] Iter:[130/286], Time: 0.89, lr: [8.440339638899473e-08], Loss: 1.221071, Acc:0.589582, Semantic loss: 0.117667, BCE loss: 1.058798, SB loss: 0.044606
Epoch: [15/20] Iter:[140/286], Time: 0.89, lr: [8.381883980399122e-08], Loss: 1.216236, Acc:0.591456, Semantic loss: 0.117261, BCE loss: 1.054586, SB loss: 0.044389
Epoch: [15/20] Iter:[150/286], Time: 0.89, lr: [8.323382989389617e-08], Loss: 1.205819, Acc:0.589916, Semantic loss: 0.116770, BCE loss: 1.044674, SB loss: 0.044375
Epoch: [15/20] Iter:[160/286], Time: 0.89, lr: [8.264836276134583e-08], Loss: 1.208528, Acc:0.592477, Semantic loss: 0.117246, BCE loss: 1.046802, SB loss: 0.044479
Epoch: [15/20] Iter:[170/286], Time: 0.89, lr: [8.206243444450466e-08], Loss: 1.212234, Acc:0.593076, Semantic loss: 0.117115, BCE loss: 1.050603, SB loss: 0.044516
Epoch: [15/20] Iter:[180/286], Time: 0.89, lr: [8.14760409154781e-08], Loss: 1.210851, Acc:0.594316, Semantic loss: 0.116936, BCE loss: 1.049558, SB loss: 0.044356
Epoch: [15/20] Iter:[190/286], Time: 0.89, lr: [8.088917807867389e-08], Loss: 1.204113, Acc:0.596877, Semantic loss: 0.116479, BCE loss: 1.043444, SB loss: 0.044190
Epoch: [15/20] Iter:[200/286], Time: 0.89, lr: [8.030184176910877e-08], Loss: 1.203399, Acc:0.597739, Semantic loss: 0.116305, BCE loss: 1.042976, SB loss: 0.044118
Epoch: [15/20] Iter:[210/286], Time: 0.89, lr: [7.97140277506588e-08], Loss: 1.206627, Acc:0.597394, Semantic loss: 0.116428, BCE loss: 1.046111, SB loss: 0.044088
Epoch: [15/20] Iter:[220/286], Time: 0.89, lr: [7.912573171425073e-08], Loss: 1.202956, Acc:0.596768, Semantic loss: 0.116153, BCE loss: 1.042714, SB loss: 0.044089
Epoch: [15/20] Iter:[230/286], Time: 0.89, lr: [7.853694927599253e-08], Loss: 1.201818, Acc:0.598391, Semantic loss: 0.115998, BCE loss: 1.041829, SB loss: 0.043991
Epoch: [15/20] Iter:[240/286], Time: 0.89, lr: [7.794767597523935e-08], Loss: 1.202172, Acc:0.597514, Semantic loss: 0.116094, BCE loss: 1.042068, SB loss: 0.044010
Epoch: [15/20] Iter:[250/286], Time: 0.89, lr: [7.735790727259336e-08], Loss: 1.207050, Acc:0.598493, Semantic loss: 0.116009, BCE loss: 1.047070, SB loss: 0.043971
Epoch: [15/20] Iter:[260/286], Time: 0.89, lr: [7.676763854783377e-08], Loss: 1.206630, Acc:0.598498, Semantic loss: 0.115809, BCE loss: 1.046900, SB loss: 0.043922
Epoch: [15/20] Iter:[270/286], Time: 0.89, lr: [7.617686509777423e-08], Loss: 1.203600, Acc:0.597325, Semantic loss: 0.115954, BCE loss: 1.043723, SB loss: 0.043923
Epoch: [15/20] Iter:[280/286], Time: 0.89, lr: [7.55855821340448e-08], Loss: 1.202723, Acc:0.596731, Semantic loss: 0.116071, BCE loss: 1.042757, SB loss: 0.043895
0
10
20
30
40
50
60
70
0 [0.         0.43201203 0.28104171 0.15082401 0.27386395 0.11757516
 0.12457457 0.07436894] 0.18178254464008983
1 [0.         0.4383847  0.27361678 0.30767    0.4146319  0.04432645
 0.1310091  0.38621214] 0.2494813838976459
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.208, MeanIU:  0.2495, Best_mIoU:  0.2495
[0.         0.4383847  0.27361678 0.30767    0.4146319  0.04432645
 0.1310091  0.38621214]
Epoch: [16/20] Iter:[0/286], Time: 0.72, lr: [3.4485658479574866e-08], Loss: 1.563909, Acc:0.505066, Semantic loss: 0.133706, BCE loss: 1.372086, SB loss: 0.058117
Epoch: [16/20] Iter:[10/286], Time: 0.91, lr: [3.421423626390021e-08], Loss: 1.286256, Acc:0.627736, Semantic loss: 0.116251, BCE loss: 1.124225, SB loss: 0.045779
Epoch: [16/20] Iter:[20/286], Time: 0.88, lr: [3.394257459000454e-08], Loss: 1.285726, Acc:0.612363, Semantic loss: 0.113210, BCE loss: 1.127712, SB loss: 0.044804
Epoch: [16/20] Iter:[30/286], Time: 0.88, lr: [3.3670671113331164e-08], Loss: 1.205166, Acc:0.598781, Semantic loss: 0.113725, BCE loss: 1.046942, SB loss: 0.044499
Epoch: [16/20] Iter:[40/286], Time: 0.89, lr: [3.339852344510455e-08], Loss: 1.192083, Acc:0.596662, Semantic loss: 0.117317, BCE loss: 1.029901, SB loss: 0.044865
Epoch: [16/20] Iter:[50/286], Time: 0.88, lr: [3.312612915108812e-08], Loss: 1.175653, Acc:0.597085, Semantic loss: 0.116437, BCE loss: 1.014551, SB loss: 0.044665
Epoch: [16/20] Iter:[60/286], Time: 0.88, lr: [3.285348575029537e-08], Loss: 1.157631, Acc:0.596842, Semantic loss: 0.116394, BCE loss: 0.996733, SB loss: 0.044504
Epoch: [16/20] Iter:[70/286], Time: 0.89, lr: [3.25805907136522e-08], Loss: 1.150778, Acc:0.596936, Semantic loss: 0.115604, BCE loss: 0.990853, SB loss: 0.044322
Epoch: [16/20] Iter:[80/286], Time: 0.88, lr: [3.230744146260825e-08], Loss: 1.182517, Acc:0.596025, Semantic loss: 0.115992, BCE loss: 1.022346, SB loss: 0.044179
Epoch: [16/20] Iter:[90/286], Time: 0.89, lr: [3.203403536769461e-08], Loss: 1.177924, Acc:0.595994, Semantic loss: 0.115575, BCE loss: 1.018399, SB loss: 0.043950
Epoch: [16/20] Iter:[100/286], Time: 0.89, lr: [3.1760369747025405e-08], Loss: 1.186098, Acc:0.596853, Semantic loss: 0.115697, BCE loss: 1.026577, SB loss: 0.043824
Epoch: [16/20] Iter:[110/286], Time: 0.89, lr: [3.148644186474048e-08], Loss: 1.188345, Acc:0.599899, Semantic loss: 0.115157, BCE loss: 1.029438, SB loss: 0.043749
Epoch: [16/20] Iter:[120/286], Time: 0.89, lr: [3.121224892938645e-08], Loss: 1.199334, Acc:0.601118, Semantic loss: 0.115694, BCE loss: 1.039760, SB loss: 0.043880
Epoch: [16/20] Iter:[130/286], Time: 0.89, lr: [3.093778809223265e-08], Loss: 1.187586, Acc:0.601624, Semantic loss: 0.116128, BCE loss: 1.027387, SB loss: 0.044072
Epoch: [16/20] Iter:[140/286], Time: 0.89, lr: [3.0663056445519166e-08], Loss: 1.195058, Acc:0.602622, Semantic loss: 0.116278, BCE loss: 1.034673, SB loss: 0.044107
Epoch: [16/20] Iter:[150/286], Time: 0.89, lr: [3.038805102063317e-08], Loss: 1.199074, Acc:0.598651, Semantic loss: 0.116694, BCE loss: 1.038240, SB loss: 0.044141
Epoch: [16/20] Iter:[160/286], Time: 0.89, lr: [3.011276878620995e-08], Loss: 1.206828, Acc:0.599847, Semantic loss: 0.117093, BCE loss: 1.045485, SB loss: 0.044249
Epoch: [16/20] Iter:[170/286], Time: 0.89, lr: [2.9837206646154985e-08], Loss: 1.210477, Acc:0.600255, Semantic loss: 0.117439, BCE loss: 1.048640, SB loss: 0.044398
Epoch: [16/20] Iter:[180/286], Time: 0.89, lr: [2.9561361437582394e-08], Loss: 1.213833, Acc:0.600458, Semantic loss: 0.117560, BCE loss: 1.051876, SB loss: 0.044397
Epoch: [16/20] Iter:[190/286], Time: 0.89, lr: [2.928522992866595e-08], Loss: 1.212650, Acc:0.598017, Semantic loss: 0.117687, BCE loss: 1.050602, SB loss: 0.044360
Epoch: [16/20] Iter:[200/286], Time: 0.89, lr: [2.900880881639743e-08], Loss: 1.217445, Acc:0.600683, Semantic loss: 0.117553, BCE loss: 1.055498, SB loss: 0.044394
Epoch: [16/20] Iter:[210/286], Time: 0.89, lr: [2.873209472424777e-08], Loss: 1.219926, Acc:0.600221, Semantic loss: 0.117568, BCE loss: 1.057993, SB loss: 0.044365
Epoch: [16/20] Iter:[220/286], Time: 0.89, lr: [2.8455084199725332e-08], Loss: 1.228128, Acc:0.601206, Semantic loss: 0.117677, BCE loss: 1.066081, SB loss: 0.044369
Epoch: [16/20] Iter:[230/286], Time: 0.89, lr: [2.817777371182579e-08], Loss: 1.224011, Acc:0.600887, Semantic loss: 0.117863, BCE loss: 1.061685, SB loss: 0.044462
Epoch: [16/20] Iter:[240/286], Time: 0.89, lr: [2.7900159648367606e-08], Loss: 1.219728, Acc:0.601033, Semantic loss: 0.117939, BCE loss: 1.057323, SB loss: 0.044467
Epoch: [16/20] Iter:[250/286], Time: 0.89, lr: [2.762223831320648e-08], Loss: 1.224180, Acc:0.601360, Semantic loss: 0.117812, BCE loss: 1.061984, SB loss: 0.044384
Epoch: [16/20] Iter:[260/286], Time: 0.89, lr: [2.7344005923322096e-08], Loss: 1.218731, Acc:0.600490, Semantic loss: 0.117905, BCE loss: 1.056421, SB loss: 0.044405
Epoch: [16/20] Iter:[270/286], Time: 0.89, lr: [2.7065458605769553e-08], Loss: 1.223478, Acc:0.601168, Semantic loss: 0.117956, BCE loss: 1.061166, SB loss: 0.044356
Epoch: [16/20] Iter:[280/286], Time: 0.89, lr: [2.678659239448774e-08], Loss: 1.226830, Acc:0.601637, Semantic loss: 0.117999, BCE loss: 1.064472, SB loss: 0.044359
0
10
20
30
40
50
60
70
0 [0.         0.43031445 0.28791408 0.15502777 0.27785648 0.12921789
 0.1217406  0.08692971] 0.1861251216489553
1 [0.         0.45517549 0.26888359 0.30646377 0.39877483 0.04858167
 0.1256339  0.42012612] 0.25295492097278305
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.231, MeanIU:  0.2530, Best_mIoU:  0.2530
[0.         0.45517549 0.26888359 0.30646377 0.39877483 0.04858167
 0.1256339  0.42012612]
Epoch: [17/20] Iter:[0/286], Time: 0.77, lr: [2.2716857974983714e-08], Loss: 1.153367, Acc:0.688476, Semantic loss: 0.110513, BCE loss: 1.003699, SB loss: 0.039156
Epoch: [17/20] Iter:[10/286], Time: 0.90, lr: [2.2478429794987205e-08], Loss: 1.314162, Acc:0.620768, Semantic loss: 0.116289, BCE loss: 1.153205, SB loss: 0.044668
Epoch: [17/20] Iter:[20/286], Time: 0.88, lr: [2.223972027698662e-08], Loss: 1.234485, Acc:0.616451, Semantic loss: 0.116671, BCE loss: 1.072548, SB loss: 0.045266
Epoch: [17/20] Iter:[30/286], Time: 0.89, lr: [2.2000725725622177e-08], Loss: 1.206853, Acc:0.612574, Semantic loss: 0.117670, BCE loss: 1.044597, SB loss: 0.044586
Epoch: [17/20] Iter:[40/286], Time: 0.89, lr: [2.1761442351747962e-08], Loss: 1.202920, Acc:0.601982, Semantic loss: 0.118241, BCE loss: 1.039874, SB loss: 0.044805
Epoch: [17/20] Iter:[50/286], Time: 0.89, lr: [2.1521866268875087e-08], Loss: 1.186627, Acc:0.599241, Semantic loss: 0.118920, BCE loss: 1.022856, SB loss: 0.044850
Epoch: [17/20] Iter:[60/286], Time: 0.89, lr: [2.1281993489434427e-08], Loss: 1.183146, Acc:0.594046, Semantic loss: 0.118423, BCE loss: 1.019873, SB loss: 0.044850
Epoch: [17/20] Iter:[70/286], Time: 0.89, lr: [2.1041819920847017e-08], Loss: 1.190131, Acc:0.595353, Semantic loss: 0.118430, BCE loss: 1.026399, SB loss: 0.045301
Epoch: [17/20] Iter:[80/286], Time: 0.89, lr: [2.0801341361389947e-08], Loss: 1.203330, Acc:0.603964, Semantic loss: 0.118079, BCE loss: 1.040316, SB loss: 0.044935
Epoch: [17/20] Iter:[90/286], Time: 0.89, lr: [2.0560553495844112e-08], Loss: 1.194439, Acc:0.599620, Semantic loss: 0.117300, BCE loss: 1.032348, SB loss: 0.044791
Epoch: [17/20] Iter:[100/286], Time: 0.89, lr: [2.031945189090961e-08], Loss: 1.181521, Acc:0.596395, Semantic loss: 0.117540, BCE loss: 1.019147, SB loss: 0.044834
Epoch: [17/20] Iter:[110/286], Time: 0.89, lr: [2.0078031990372782e-08], Loss: 1.177896, Acc:0.596444, Semantic loss: 0.117179, BCE loss: 1.015975, SB loss: 0.044741
Epoch: [17/20] Iter:[120/286], Time: 0.89, lr: [1.9836289110008225e-08], Loss: 1.188035, Acc:0.596829, Semantic loss: 0.116890, BCE loss: 1.026625, SB loss: 0.044520
Epoch: [17/20] Iter:[130/286], Time: 0.89, lr: [1.9594218432197354e-08], Loss: 1.185846, Acc:0.594766, Semantic loss: 0.117023, BCE loss: 1.024238, SB loss: 0.044585
Epoch: [17/20] Iter:[140/286], Time: 0.89, lr: [1.935181500024354e-08], Loss: 1.191754, Acc:0.596122, Semantic loss: 0.117027, BCE loss: 1.030228, SB loss: 0.044500
Epoch: [17/20] Iter:[150/286], Time: 0.88, lr: [1.910907371236251e-08], Loss: 1.185551, Acc:0.592280, Semantic loss: 0.116798, BCE loss: 1.024460, SB loss: 0.044293
Epoch: [17/20] Iter:[160/286], Time: 0.89, lr: [1.8865989315324298e-08], Loss: 1.185088, Acc:0.592359, Semantic loss: 0.116903, BCE loss: 1.023877, SB loss: 0.044308
Epoch: [17/20] Iter:[170/286], Time: 0.89, lr: [1.862255639772144e-08], Loss: 1.180725, Acc:0.593272, Semantic loss: 0.116838, BCE loss: 1.019528, SB loss: 0.044358
Epoch: [17/20] Iter:[180/286], Time: 0.89, lr: [1.837876938283568e-08], Loss: 1.182363, Acc:0.592390, Semantic loss: 0.116744, BCE loss: 1.021167, SB loss: 0.044452
Epoch: [17/20] Iter:[190/286], Time: 0.88, lr: [1.813462252107282e-08], Loss: 1.181242, Acc:0.591632, Semantic loss: 0.116801, BCE loss: 1.020032, SB loss: 0.044409
Epoch: [17/20] Iter:[200/286], Time: 0.89, lr: [1.7890109881932973e-08], Loss: 1.191131, Acc:0.592852, Semantic loss: 0.117334, BCE loss: 1.029247, SB loss: 0.044551
Epoch: [17/20] Iter:[210/286], Time: 0.89, lr: [1.7645225345479853e-08], Loss: 1.196717, Acc:0.594417, Semantic loss: 0.117230, BCE loss: 1.034940, SB loss: 0.044547
Epoch: [17/20] Iter:[220/286], Time: 0.89, lr: [1.73999625932699e-08], Loss: 1.198622, Acc:0.594271, Semantic loss: 0.117250, BCE loss: 1.036886, SB loss: 0.044486
Epoch: [17/20] Iter:[230/286], Time: 0.89, lr: [1.7154315098697805e-08], Loss: 1.195837, Acc:0.594084, Semantic loss: 0.117088, BCE loss: 1.034299, SB loss: 0.044449
Epoch: [17/20] Iter:[240/286], Time: 0.89, lr: [1.6908276116711254e-08], Loss: 1.191075, Acc:0.592925, Semantic loss: 0.116885, BCE loss: 1.029767, SB loss: 0.044423
Epoch: [17/20] Iter:[250/286], Time: 0.89, lr: [1.666183867284248e-08], Loss: 1.189495, Acc:0.592088, Semantic loss: 0.116875, BCE loss: 1.028281, SB loss: 0.044340
Epoch: [17/20] Iter:[260/286], Time: 0.89, lr: [1.641499555149947e-08], Loss: 1.189739, Acc:0.593680, Semantic loss: 0.116786, BCE loss: 1.028676, SB loss: 0.044276
Epoch: [17/20] Iter:[270/286], Time: 0.89, lr: [1.6167739283453556e-08], Loss: 1.190108, Acc:0.592201, Semantic loss: 0.116626, BCE loss: 1.029247, SB loss: 0.044235
Epoch: [17/20] Iter:[280/286], Time: 0.89, lr: [1.59200621324535e-08], Loss: 1.187582, Acc:0.591594, Semantic loss: 0.116586, BCE loss: 1.026782, SB loss: 0.044214
0
10
20
30
40
50
60
70
0 [0.         0.4368442  0.31490003 0.16429614 0.26561239 0.12049345
 0.12980228 0.0845141 ] 0.18955782324574108
1 [0.         0.48163777 0.32405967 0.31898958 0.3870657  0.04221855
 0.14405666 0.41936268] 0.2646738248214895
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.218, MeanIU:  0.2647, Best_mIoU:  0.2647
[0.         0.48163777 0.32405967 0.31898958 0.3870657  0.04221855
 0.14405666 0.41936268]
Epoch: [18/20] Iter:[0/286], Time: 0.70, lr: [1.7764303771567715e-08], Loss: 0.974345, Acc:0.596517, Semantic loss: 0.117921, BCE loss: 0.808872, SB loss: 0.047552
Epoch: [18/20] Iter:[10/286], Time: 0.89, lr: [1.7484549589304203e-08], Loss: 1.145956, Acc:0.633346, Semantic loss: 0.112204, BCE loss: 0.991746, SB loss: 0.042005
Epoch: [18/20] Iter:[20/286], Time: 0.88, lr: [1.7204297153034318e-08], Loss: 1.167640, Acc:0.607550, Semantic loss: 0.111893, BCE loss: 1.012989, SB loss: 0.042758
Epoch: [18/20] Iter:[30/286], Time: 0.87, lr: [1.692353652370546e-08], Loss: 1.173782, Acc:0.604288, Semantic loss: 0.114879, BCE loss: 1.015572, SB loss: 0.043330
Epoch: [18/20] Iter:[40/286], Time: 0.88, lr: [1.6642257376753737e-08], Loss: 1.116660, Acc:0.593033, Semantic loss: 0.115097, BCE loss: 0.957778, SB loss: 0.043784
Epoch: [18/20] Iter:[50/286], Time: 0.89, lr: [1.636044897961373e-08], Loss: 1.148487, Acc:0.596850, Semantic loss: 0.114787, BCE loss: 0.990138, SB loss: 0.043562
Epoch: [18/20] Iter:[60/286], Time: 0.88, lr: [1.607810016745969e-08], Loss: 1.179881, Acc:0.600957, Semantic loss: 0.116395, BCE loss: 1.019405, SB loss: 0.044081
Epoch: [18/20] Iter:[70/286], Time: 0.88, lr: [1.5795199317001562e-08], Loss: 1.188297, Acc:0.597546, Semantic loss: 0.116110, BCE loss: 1.028352, SB loss: 0.043836
Epoch: [18/20] Iter:[80/286], Time: 0.89, lr: [1.5511734318138224e-08], Loss: 1.195350, Acc:0.598120, Semantic loss: 0.115729, BCE loss: 1.035890, SB loss: 0.043732
Epoch: [18/20] Iter:[90/286], Time: 0.88, lr: [1.5227692543245153e-08], Loss: 1.211169, Acc:0.601347, Semantic loss: 0.116414, BCE loss: 1.051140, SB loss: 0.043615
Epoch: [18/20] Iter:[100/286], Time: 0.88, lr: [1.4943060813846116e-08], Loss: 1.211315, Acc:0.601896, Semantic loss: 0.115840, BCE loss: 1.051820, SB loss: 0.043656
Epoch: [18/20] Iter:[110/286], Time: 0.89, lr: [1.4657825364385775e-08], Loss: 1.198640, Acc:0.596638, Semantic loss: 0.115636, BCE loss: 1.039243, SB loss: 0.043761
Epoch: [18/20] Iter:[120/286], Time: 0.89, lr: [1.4371971802782524e-08], Loss: 1.201070, Acc:0.597435, Semantic loss: 0.116055, BCE loss: 1.041176, SB loss: 0.043838
Epoch: [18/20] Iter:[130/286], Time: 0.89, lr: [1.4085485067398045e-08], Loss: 1.198694, Acc:0.600144, Semantic loss: 0.116261, BCE loss: 1.038612, SB loss: 0.043821
Epoch: [18/20] Iter:[140/286], Time: 0.89, lr: [1.3798349380009046e-08], Loss: 1.202957, Acc:0.601563, Semantic loss: 0.116259, BCE loss: 1.042885, SB loss: 0.043813
Epoch: [18/20] Iter:[150/286], Time: 0.89, lr: [1.3510548194308739e-08], Loss: 1.194194, Acc:0.601402, Semantic loss: 0.116188, BCE loss: 1.034129, SB loss: 0.043877
Epoch: [18/20] Iter:[160/286], Time: 0.89, lr: [1.3222064139396794e-08], Loss: 1.185011, Acc:0.599610, Semantic loss: 0.115987, BCE loss: 1.025213, SB loss: 0.043811
Epoch: [18/20] Iter:[170/286], Time: 0.89, lr: [1.2932878957636256e-08], Loss: 1.192933, Acc:0.600955, Semantic loss: 0.115871, BCE loss: 1.033350, SB loss: 0.043713
Epoch: [18/20] Iter:[180/286], Time: 0.89, lr: [1.2642973436161712e-08], Loss: 1.197631, Acc:0.600055, Semantic loss: 0.115512, BCE loss: 1.038471, SB loss: 0.043648
Epoch: [18/20] Iter:[190/286], Time: 0.89, lr: [1.2352327331210862e-08], Loss: 1.200303, Acc:0.600025, Semantic loss: 0.115301, BCE loss: 1.041426, SB loss: 0.043576
Epoch: [18/20] Iter:[200/286], Time: 0.89, lr: [1.2060919284319793e-08], Loss: 1.198383, Acc:0.598389, Semantic loss: 0.115892, BCE loss: 1.038778, SB loss: 0.043713
Epoch: [18/20] Iter:[210/286], Time: 0.89, lr: [1.1768726729263999e-08], Loss: 1.200064, Acc:0.599393, Semantic loss: 0.115995, BCE loss: 1.040357, SB loss: 0.043712
Epoch: [18/20] Iter:[220/286], Time: 0.89, lr: [1.1475725788439033e-08], Loss: 1.200402, Acc:0.599408, Semantic loss: 0.116578, BCE loss: 1.040069, SB loss: 0.043755
Epoch: [18/20] Iter:[230/286], Time: 0.89, lr: [1.1181891157146987e-08], Loss: 1.198912, Acc:0.599134, Semantic loss: 0.116712, BCE loss: 1.038428, SB loss: 0.043772
Epoch: [18/20] Iter:[240/286], Time: 0.89, lr: [1.0887195973981603e-08], Loss: 1.198655, Acc:0.598391, Semantic loss: 0.116947, BCE loss: 1.037874, SB loss: 0.043834
Epoch: [18/20] Iter:[250/286], Time: 0.89, lr: [1.0591611675171433e-08], Loss: 1.199088, Acc:0.598576, Semantic loss: 0.116557, BCE loss: 1.038762, SB loss: 0.043769
Epoch: [18/20] Iter:[260/286], Time: 0.89, lr: [1.0295107830334025e-08], Loss: 1.210078, Acc:0.600918, Semantic loss: 0.116686, BCE loss: 1.049615, SB loss: 0.043777
Epoch: [18/20] Iter:[270/286], Time: 0.89, lr: [9.997651956595161e-09], Loss: 1.211297, Acc:0.601658, Semantic loss: 0.116748, BCE loss: 1.050802, SB loss: 0.043746
Epoch: [18/20] Iter:[280/286], Time: 0.89, lr: [9.699209307409454e-09], Loss: 1.205922, Acc:0.601110, Semantic loss: 0.116819, BCE loss: 1.045366, SB loss: 0.043737
0
10
20
30
40
50
60
70
0 [0.         0.44171461 0.28356218 0.15984018 0.27555168 0.13161366
 0.1211344  0.0975599 ] 0.18887207660304658
1 [0.         0.45112278 0.19955235 0.29707275 0.37370034 0.03836804
 0.11555716 0.43684248] 0.23902698607152545
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.230, MeanIU:  0.2390, Best_mIoU:  0.2647
[0.         0.45112278 0.19955235 0.29707275 0.37370034 0.03836804
 0.11555716 0.43684248]
Epoch: [19/20] Iter:[0/286], Time: 0.70, lr: [1.108254708485366e-08], Loss: 1.313596, Acc:0.672042, Semantic loss: 0.101843, BCE loss: 1.173806, SB loss: 0.037947
Epoch: [19/20] Iter:[10/286], Time: 0.88, lr: [1.0733177934363248e-08], Loss: 1.191470, Acc:0.565005, Semantic loss: 0.124200, BCE loss: 1.021920, SB loss: 0.045349
Epoch: [19/20] Iter:[20/286], Time: 0.86, lr: [1.0382540365658778e-08], Loss: 1.189812, Acc:0.582609, Semantic loss: 0.120266, BCE loss: 1.024275, SB loss: 0.045271
Epoch: [19/20] Iter:[30/286], Time: 0.88, lr: [1.003058180225771e-08], Loss: 1.224249, Acc:0.598887, Semantic loss: 0.118396, BCE loss: 1.061896, SB loss: 0.043957
Epoch: [19/20] Iter:[40/286], Time: 0.89, lr: [9.677245343195104e-09], Loss: 1.205266, Acc:0.596722, Semantic loss: 0.118183, BCE loss: 1.042989, SB loss: 0.044093
Epoch: [19/20] Iter:[50/286], Time: 0.88, lr: [9.322469216403537e-09], Loss: 1.213025, Acc:0.600924, Semantic loss: 0.117473, BCE loss: 1.051987, SB loss: 0.043566
Epoch: [19/20] Iter:[60/286], Time: 0.89, lr: [8.966186136799544e-09], Loss: 1.226746, Acc:0.602431, Semantic loss: 0.117655, BCE loss: 1.065647, SB loss: 0.043444
Epoch: [19/20] Iter:[70/286], Time: 0.89, lr: [8.60832254748794e-09], Loss: 1.215265, Acc:0.594212, Semantic loss: 0.117968, BCE loss: 1.053630, SB loss: 0.043667
Epoch: [19/20] Iter:[80/286], Time: 0.89, lr: [8.248797716370505e-09], Loss: 1.225237, Acc:0.592975, Semantic loss: 0.118191, BCE loss: 1.063318, SB loss: 0.043728
Epoch: [19/20] Iter:[90/286], Time: 0.89, lr: [7.887522652200792e-09], Loss: 1.232667, Acc:0.595878, Semantic loss: 0.117962, BCE loss: 1.071040, SB loss: 0.043665
Epoch: [19/20] Iter:[100/286], Time: 0.89, lr: [7.524398792877508e-09], Loss: 1.231042, Acc:0.593816, Semantic loss: 0.118571, BCE loss: 1.068455, SB loss: 0.044016
Epoch: [19/20] Iter:[110/286], Time: 0.89, lr: [7.159316403201828e-09], Loss: 1.219626, Acc:0.591103, Semantic loss: 0.117707, BCE loss: 1.058011, SB loss: 0.043908
Epoch: [19/20] Iter:[120/286], Time: 0.88, lr: [6.7921525974351226e-09], Loss: 1.224092, Acc:0.592993, Semantic loss: 0.117551, BCE loss: 1.062648, SB loss: 0.043892
Epoch: [19/20] Iter:[130/286], Time: 0.89, lr: [6.422768870679645e-09], Loss: 1.223453, Acc:0.597482, Semantic loss: 0.117072, BCE loss: 1.062469, SB loss: 0.043912
Epoch: [19/20] Iter:[140/286], Time: 0.89, lr: [6.051007977428321e-09], Loss: 1.228697, Acc:0.600985, Semantic loss: 0.117657, BCE loss: 1.067141, SB loss: 0.043899
Epoch: [19/20] Iter:[150/286], Time: 0.89, lr: [5.676689927554523e-09], Loss: 1.230209, Acc:0.601347, Semantic loss: 0.117572, BCE loss: 1.068738, SB loss: 0.043899
Epoch: [19/20] Iter:[160/286], Time: 0.89, lr: [5.2996067660650716e-09], Loss: 1.228730, Acc:0.601993, Semantic loss: 0.117485, BCE loss: 1.067401, SB loss: 0.043843
Epoch: [19/20] Iter:[170/286], Time: 0.89, lr: [4.919515639799844e-09], Loss: 1.239359, Acc:0.605030, Semantic loss: 0.117180, BCE loss: 1.078394, SB loss: 0.043784
Epoch: [19/20] Iter:[180/286], Time: 0.89, lr: [4.536129390077404e-09], Loss: 1.231599, Acc:0.604186, Semantic loss: 0.116934, BCE loss: 1.070864, SB loss: 0.043801
Epoch: [19/20] Iter:[190/286], Time: 0.89, lr: [4.1491034667179335e-09], Loss: 1.230899, Acc:0.604933, Semantic loss: 0.116796, BCE loss: 1.070317, SB loss: 0.043786
Epoch: [19/20] Iter:[200/286], Time: 0.89, lr: [3.758017182006024e-09], Loss: 1.238168, Acc:0.604233, Semantic loss: 0.116851, BCE loss: 1.077452, SB loss: 0.043865
Epoch: [19/20] Iter:[210/286], Time: 0.89, lr: [3.3623458927484136e-09], Loss: 1.251327, Acc:0.605662, Semantic loss: 0.116841, BCE loss: 1.090588, SB loss: 0.043898
Epoch: [19/20] Iter:[220/286], Time: 0.89, lr: [2.9614179011200605e-09], Loss: 1.244765, Acc:0.604960, Semantic loss: 0.116977, BCE loss: 1.083901, SB loss: 0.043887
Epoch: [19/20] Iter:[230/286], Time: 0.89, lr: [2.5543439716648542e-09], Loss: 1.245791, Acc:0.604711, Semantic loss: 0.117143, BCE loss: 1.084649, SB loss: 0.043999
Epoch: [19/20] Iter:[240/286], Time: 0.89, lr: [2.1398937179846433e-09], Loss: 1.243265, Acc:0.603448, Semantic loss: 0.117032, BCE loss: 1.082203, SB loss: 0.044030
Epoch: [19/20] Iter:[250/286], Time: 0.89, lr: [1.7162573335133848e-09], Loss: 1.243764, Acc:0.602950, Semantic loss: 0.117194, BCE loss: 1.082545, SB loss: 0.044024
Epoch: [19/20] Iter:[260/286], Time: 0.89, lr: [1.2805194176863323e-09], Loss: 1.242835, Acc:0.603845, Semantic loss: 0.117231, BCE loss: 1.081518, SB loss: 0.044085
Epoch: [19/20] Iter:[270/286], Time: 0.89, lr: [8.272145023583422e-10], Loss: 1.241377, Acc:0.603156, Semantic loss: 0.117421, BCE loss: 1.079879, SB loss: 0.044078
Epoch: [19/20] Iter:[280/286], Time: 0.89, lr: [3.4217342793435723e-10], Loss: 1.236874, Acc:0.603883, Semantic loss: 0.117105, BCE loss: 1.075814, SB loss: 0.043955
0
10
20
30
40
50
60
70
0 [0.         0.43672435 0.28221375 0.15931038 0.26997623 0.13981726
 0.129793   0.08934128] 0.18839702978927508
1 [0.         0.41512442 0.17319178 0.3128658  0.36200059 0.0529298
 0.13094143 0.4300515 ] 0.23463816453302044
=> saving checkpoint to output/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG2checkpoint.pth.tar
Loss: 1.228, MeanIU:  0.2346, Best_mIoU:  0.2647
[0.         0.41512442 0.17319178 0.3128658  0.36200059 0.0529298
 0.13094143 0.4300515 ]
Hours: 1
Done