2025-01-20 17:15:40,761 Namespace(cfg='configs/loveDa/pidnet_small_loveda_3b_AUG_CHANCE+AUG3.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '8'])
2025-01-20 17:15:40,761 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/cityscapes/val.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDA-Urban/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 20
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: True
  AUG1: False
  AUG2: False
  AUG3: True
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 8
  BEGIN_EPOCH: 1
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.001
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: True
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-20 17:15:41,070 Attention!!!
2025-01-20 17:15:41,070 Loaded 302 parameters!
2025-01-20 17:15:41,070 Over!!!
2025-01-20 17:15:41,447 => loaded checkpoint (epoch 7)
2025-01-20 17:15:46,049 Epoch: [7/20] Iter:[0/217], Time: 4.59, lr: [0.00024421006497713806], Loss: 1.886602, Acc:0.580346, Semantic loss: 0.153313, BCE loss: 1.687726, SB loss: 0.045563
2025-01-20 17:15:54,474 Epoch: [7/20] Iter:[10/217], Time: 1.17, lr: [0.00024343080916329598], Loss: 1.449435, Acc:0.566799, Semantic loss: 0.124414, BCE loss: 1.279615, SB loss: 0.045406
2025-01-20 17:16:02,886 Epoch: [7/20] Iter:[20/217], Time: 1.01, lr: [0.00024265127608294205], Loss: 1.321956, Acc:0.555889, Semantic loss: 0.122746, BCE loss: 1.153814, SB loss: 0.045395
2025-01-20 17:16:11,631 Epoch: [7/20] Iter:[30/217], Time: 0.97, lr: [0.0002418714646470047], Loss: 1.306572, Acc:0.548336, Semantic loss: 0.127664, BCE loss: 1.132920, SB loss: 0.045987
2025-01-20 17:16:20,218 Epoch: [7/20] Iter:[40/217], Time: 0.94, lr: [0.00024109137375821673], Loss: 1.282581, Acc:0.552643, Semantic loss: 0.128207, BCE loss: 1.107784, SB loss: 0.046590
2025-01-20 17:16:28,642 Epoch: [7/20] Iter:[50/217], Time: 0.92, lr: [0.0002403110023110235], Loss: 1.280505, Acc:0.554703, Semantic loss: 0.128425, BCE loss: 1.105212, SB loss: 0.046868
2025-01-20 17:16:37,014 Epoch: [7/20] Iter:[60/217], Time: 0.91, lr: [0.0002395303491914904], Loss: 1.265447, Acc:0.548258, Semantic loss: 0.129152, BCE loss: 1.089396, SB loss: 0.046900
2025-01-20 17:16:45,399 Epoch: [7/20] Iter:[70/217], Time: 0.90, lr: [0.00023874941327720867], Loss: 1.255626, Acc:0.554114, Semantic loss: 0.128093, BCE loss: 1.080701, SB loss: 0.046833
2025-01-20 17:16:53,755 Epoch: [7/20] Iter:[80/217], Time: 0.89, lr: [0.0002379681934372001], Loss: 1.255407, Acc:0.554238, Semantic loss: 0.127890, BCE loss: 1.080886, SB loss: 0.046630
2025-01-20 17:17:02,194 Epoch: [7/20] Iter:[90/217], Time: 0.89, lr: [0.00023718668853181974], Loss: 1.267232, Acc:0.551984, Semantic loss: 0.127779, BCE loss: 1.092957, SB loss: 0.046496
2025-01-20 17:17:10,520 Epoch: [7/20] Iter:[100/217], Time: 0.88, lr: [0.00023640489741265776], Loss: 1.274244, Acc:0.552684, Semantic loss: 0.127573, BCE loss: 1.100086, SB loss: 0.046585
2025-01-20 17:17:18,757 Epoch: [7/20] Iter:[110/217], Time: 0.88, lr: [0.00023562281892243947], Loss: 1.266405, Acc:0.549537, Semantic loss: 0.128089, BCE loss: 1.091331, SB loss: 0.046985
2025-01-20 17:17:27,192 Epoch: [7/20] Iter:[120/217], Time: 0.87, lr: [0.00023484045189492406], Loss: 1.265766, Acc:0.550470, Semantic loss: 0.128218, BCE loss: 1.090653, SB loss: 0.046895
2025-01-20 17:17:35,726 Epoch: [7/20] Iter:[130/217], Time: 0.87, lr: [0.0002340577951548015], Loss: 1.269772, Acc:0.553031, Semantic loss: 0.127613, BCE loss: 1.095510, SB loss: 0.046649
2025-01-20 17:17:44,409 Epoch: [7/20] Iter:[140/217], Time: 0.87, lr: [0.00023327484751758812], Loss: 1.273451, Acc:0.552717, Semantic loss: 0.127685, BCE loss: 1.098941, SB loss: 0.046825
2025-01-20 17:17:53,193 Epoch: [7/20] Iter:[150/217], Time: 0.87, lr: [0.00023249160778952052], Loss: 1.281292, Acc:0.554304, Semantic loss: 0.127413, BCE loss: 1.107143, SB loss: 0.046736
2025-01-20 17:18:02,093 Epoch: [7/20] Iter:[160/217], Time: 0.87, lr: [0.00023170807476744783], Loss: 1.279401, Acc:0.557179, Semantic loss: 0.126557, BCE loss: 1.106316, SB loss: 0.046527
2025-01-20 17:18:11,517 Epoch: [7/20] Iter:[170/217], Time: 0.88, lr: [0.00023092424723872242], Loss: 1.270801, Acc:0.553644, Semantic loss: 0.126668, BCE loss: 1.097445, SB loss: 0.046689
2025-01-20 17:18:20,132 Epoch: [7/20] Iter:[180/217], Time: 0.88, lr: [0.00023014012398108857], Loss: 1.265859, Acc:0.553501, Semantic loss: 0.126366, BCE loss: 1.092975, SB loss: 0.046519
2025-01-20 17:18:28,697 Epoch: [7/20] Iter:[190/217], Time: 0.87, lr: [0.0002293557037625701], Loss: 1.266199, Acc:0.554524, Semantic loss: 0.126438, BCE loss: 1.093242, SB loss: 0.046518
2025-01-20 17:18:37,189 Epoch: [7/20] Iter:[200/217], Time: 0.87, lr: [0.00022857098534135532], Loss: 1.264724, Acc:0.555207, Semantic loss: 0.126251, BCE loss: 1.091921, SB loss: 0.046553
2025-01-20 17:18:45,654 Epoch: [7/20] Iter:[210/217], Time: 0.87, lr: [0.000227785967465681], Loss: 1.268657, Acc:0.556224, Semantic loss: 0.126390, BCE loss: 1.095724, SB loss: 0.046544
