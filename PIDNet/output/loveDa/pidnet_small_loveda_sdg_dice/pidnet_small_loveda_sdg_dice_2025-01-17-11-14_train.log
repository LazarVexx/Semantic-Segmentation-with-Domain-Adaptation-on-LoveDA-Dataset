2025-01-17 11:14:25,263 Namespace(cfg='configs/loveDa/pidnet_small_loveda_sdg_dice.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-17 11:14:25,264 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/cityscapes/val.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: True
  USE_FOCAL: False
  USE_OHEM: False
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 20
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: False
  AUG1: False
  AUG2: False
  AUG3: False
  AUG4: False
  AUG_CHANCE: False
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 1
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.001
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-17 11:14:25,435 Attention!!!
2025-01-17 11:14:25,435 Loaded 302 parameters!
2025-01-17 11:14:25,435 Over!!!
2025-01-17 11:14:29,579 Epoch: [0/20] Iter:[0/192], Time: 3.96, lr: [0.001], Loss: 12.266644, Acc:0.153039, Semantic loss: 5.909118, BCE loss: 4.162129, SB loss: 2.195396
2025-01-17 11:14:37,651 Epoch: [0/20] Iter:[10/192], Time: 1.09, lr: [0.0009976559445324194], Loss: 10.552519, Acc:0.215702, Semantic loss: 5.319059, BCE loss: 3.371643, SB loss: 1.861818
2025-01-17 11:14:45,431 Epoch: [0/20] Iter:[20/192], Time: 0.94, lr: [0.000995311276959276], Loss: 9.575542, Acc:0.255502, Semantic loss: 4.865668, BCE loss: 3.097682, SB loss: 1.612192
2025-01-17 11:14:53,337 Epoch: [0/20] Iter:[30/192], Time: 0.89, lr: [0.000992965995517728], Loss: 9.032815, Acc:0.273866, Semantic loss: 4.523031, BCE loss: 3.053029, SB loss: 1.456754
2025-01-17 11:15:01,121 Epoch: [0/20] Iter:[40/192], Time: 0.86, lr: [0.0009906200984352155], Loss: 8.425864, Acc:0.299812, Semantic loss: 4.220569, BCE loss: 2.858892, SB loss: 1.346403
2025-01-17 11:15:09,240 Epoch: [0/20] Iter:[50/192], Time: 0.85, lr: [0.0009882735839293803], Loss: 8.113375, Acc:0.309374, Semantic loss: 4.055921, BCE loss: 2.767609, SB loss: 1.289845
2025-01-17 11:15:17,025 Epoch: [0/20] Iter:[60/192], Time: 0.84, lr: [0.000985926450207989], Loss: 7.769495, Acc:0.322217, Semantic loss: 3.879185, BCE loss: 2.651931, SB loss: 1.238379
2025-01-17 11:15:24,853 Epoch: [0/20] Iter:[70/192], Time: 0.83, lr: [0.0009835786954688485], Loss: 7.487665, Acc:0.332865, Semantic loss: 3.740429, BCE loss: 2.557848, SB loss: 1.189387
2025-01-17 11:15:32,406 Epoch: [0/20] Iter:[80/192], Time: 0.82, lr: [0.000981230317899726], Loss: 7.262920, Acc:0.343216, Semantic loss: 3.623201, BCE loss: 2.490136, SB loss: 1.149584
2025-01-17 11:15:40,282 Epoch: [0/20] Iter:[90/192], Time: 0.82, lr: [0.0009788813156782663], Loss: 7.059358, Acc:0.351331, Semantic loss: 3.507664, BCE loss: 2.441122, SB loss: 1.110572
2025-01-17 11:15:48,062 Epoch: [0/20] Iter:[100/192], Time: 0.82, lr: [0.0009765316869719068], Loss: 6.881982, Acc:0.356513, Semantic loss: 3.406267, BCE loss: 2.398463, SB loss: 1.077252
2025-01-17 11:15:55,857 Epoch: [0/20] Iter:[110/192], Time: 0.81, lr: [0.0009741814299377942], Loss: 6.699027, Acc:0.362226, Semantic loss: 3.309254, BCE loss: 2.345579, SB loss: 1.044194
2025-01-17 11:16:03,600 Epoch: [0/20] Iter:[120/192], Time: 0.81, lr: [0.0009718305427226986], Loss: 6.543935, Acc:0.368170, Semantic loss: 3.230253, BCE loss: 2.291700, SB loss: 1.021982
2025-01-17 11:16:11,651 Epoch: [0/20] Iter:[130/192], Time: 0.81, lr: [0.0009694790234629266], Loss: 6.403691, Acc:0.374584, Semantic loss: 3.151105, BCE loss: 2.258089, SB loss: 0.994496
2025-01-17 11:16:19,659 Epoch: [0/20] Iter:[140/192], Time: 0.81, lr: [0.0009671268702842339], Loss: 6.262193, Acc:0.382063, Semantic loss: 3.073340, BCE loss: 2.220073, SB loss: 0.968780
2025-01-17 11:16:27,604 Epoch: [0/20] Iter:[150/192], Time: 0.81, lr: [0.0009647740813017376], Loss: 6.149990, Acc:0.386377, Semantic loss: 3.017020, BCE loss: 2.185155, SB loss: 0.947815
2025-01-17 11:16:35,158 Epoch: [0/20] Iter:[160/192], Time: 0.80, lr: [0.0009624206546198262], Loss: 6.045727, Acc:0.391401, Semantic loss: 2.965631, BCE loss: 2.149392, SB loss: 0.930704
2025-01-17 11:16:42,941 Epoch: [0/20] Iter:[170/192], Time: 0.80, lr: [0.0009600665883320689], Loss: 5.933157, Acc:0.398386, Semantic loss: 2.900465, BCE loss: 2.124142, SB loss: 0.908550
2025-01-17 11:16:50,823 Epoch: [0/20] Iter:[180/192], Time: 0.80, lr: [0.0009577118805211255], Loss: 5.857011, Acc:0.403898, Semantic loss: 2.851300, BCE loss: 2.115546, SB loss: 0.890164
2025-01-17 11:16:58,712 Epoch: [0/20] Iter:[190/192], Time: 0.80, lr: [0.0009553565292586524], Loss: 5.784171, Acc:0.407821, Semantic loss: 2.813461, BCE loss: 2.094006, SB loss: 0.876703
