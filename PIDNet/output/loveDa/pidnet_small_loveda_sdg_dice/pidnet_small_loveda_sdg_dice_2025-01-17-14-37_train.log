2025-01-17 14:37:11,562 Namespace(cfg='configs/loveDa/pidnet_small_loveda_sdg_dice.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-17 14:37:11,562 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/cityscapes/val.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: True
  USE_FOCAL: False
  USE_OHEM: False
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 20
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: False
  AUG1: False
  AUG2: False
  AUG3: False
  AUG4: False
  AUG_CHANCE: False
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 1
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.001
  LR_D1: 0.001
  LR_D2: 0.001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-17 14:37:11,717 Attention!!!
2025-01-17 14:37:11,717 Loaded 302 parameters!
2025-01-17 14:37:11,718 Over!!!
2025-01-17 14:37:14,830 Epoch: [0/20] Iter:[0/192], Time: 2.95, lr: [0.001], Loss: 5.948222, Acc:0.184890, Semantic loss: 0.985518, BCE loss: 3.982374, SB loss: 0.980329
2025-01-17 14:37:21,939 Epoch: [0/20] Iter:[10/192], Time: 0.90, lr: [0.0009976559445324194], Loss: 5.740465, Acc:0.147796, Semantic loss: 0.983772, BCE loss: 3.783951, SB loss: 0.972743
2025-01-17 14:37:29,008 Epoch: [0/20] Iter:[20/192], Time: 0.81, lr: [0.000995311276959276], Loss: 5.304256, Acc:0.146595, Semantic loss: 0.984096, BCE loss: 3.351127, SB loss: 0.969032
2025-01-17 14:37:36,091 Epoch: [0/20] Iter:[30/192], Time: 0.78, lr: [0.000992965995517728], Loss: 5.076314, Acc:0.144015, Semantic loss: 0.983685, BCE loss: 3.126969, SB loss: 0.965660
2025-01-17 14:37:43,004 Epoch: [0/20] Iter:[40/192], Time: 0.76, lr: [0.0009906200984352155], Loss: 4.849168, Acc:0.147471, Semantic loss: 0.984264, BCE loss: 2.901253, SB loss: 0.963651
2025-01-17 14:37:50,236 Epoch: [0/20] Iter:[50/192], Time: 0.75, lr: [0.0009882735839293803], Loss: 4.685003, Acc:0.150720, Semantic loss: 0.984514, BCE loss: 2.738087, SB loss: 0.962403
2025-01-17 14:37:57,363 Epoch: [0/20] Iter:[60/192], Time: 0.74, lr: [0.000985926450207989], Loss: 4.618158, Acc:0.153864, Semantic loss: 0.984588, BCE loss: 2.671689, SB loss: 0.961881
2025-01-17 14:38:04,517 Epoch: [0/20] Iter:[70/192], Time: 0.74, lr: [0.0009835786954688485], Loss: 4.569301, Acc:0.156757, Semantic loss: 0.984636, BCE loss: 2.623457, SB loss: 0.961208
2025-01-17 14:38:11,408 Epoch: [0/20] Iter:[80/192], Time: 0.73, lr: [0.000981230317899726], Loss: 4.490751, Acc:0.159371, Semantic loss: 0.984369, BCE loss: 2.545344, SB loss: 0.961039
2025-01-17 14:38:18,554 Epoch: [0/20] Iter:[90/192], Time: 0.73, lr: [0.0009788813156782663], Loss: 4.409185, Acc:0.161400, Semantic loss: 0.984488, BCE loss: 2.463419, SB loss: 0.961278
2025-01-17 14:38:25,621 Epoch: [0/20] Iter:[100/192], Time: 0.73, lr: [0.0009765316869719068], Loss: 4.353816, Acc:0.162715, Semantic loss: 0.984429, BCE loss: 2.408047, SB loss: 0.961340
2025-01-17 14:38:32,850 Epoch: [0/20] Iter:[110/192], Time: 0.73, lr: [0.0009741814299377942], Loss: 4.317197, Acc:0.165073, Semantic loss: 0.984398, BCE loss: 2.371425, SB loss: 0.961375
2025-01-17 14:38:39,980 Epoch: [0/20] Iter:[120/192], Time: 0.73, lr: [0.0009718305427226986], Loss: 4.258550, Acc:0.168484, Semantic loss: 0.984342, BCE loss: 2.312766, SB loss: 0.961442
2025-01-17 14:38:47,235 Epoch: [0/20] Iter:[130/192], Time: 0.73, lr: [0.0009694790234629266], Loss: 4.230837, Acc:0.170056, Semantic loss: 0.984210, BCE loss: 2.285225, SB loss: 0.961402
2025-01-17 14:38:55,659 Epoch: [0/20] Iter:[140/192], Time: 0.74, lr: [0.0009671268702842339], Loss: 4.188150, Acc:0.172170, Semantic loss: 0.984181, BCE loss: 2.242407, SB loss: 0.961562
2025-01-17 14:39:03,057 Epoch: [0/20] Iter:[150/192], Time: 0.74, lr: [0.0009647740813017376], Loss: 4.146613, Acc:0.173264, Semantic loss: 0.984126, BCE loss: 2.200622, SB loss: 0.961865
2025-01-17 14:39:10,099 Epoch: [0/20] Iter:[160/192], Time: 0.73, lr: [0.0009624206546198262], Loss: 4.118768, Acc:0.175371, Semantic loss: 0.984012, BCE loss: 2.172692, SB loss: 0.962064
2025-01-17 14:39:17,163 Epoch: [0/20] Iter:[170/192], Time: 0.73, lr: [0.0009600665883320689], Loss: 4.091889, Acc:0.176316, Semantic loss: 0.983882, BCE loss: 2.145669, SB loss: 0.962338
2025-01-17 14:39:24,368 Epoch: [0/20] Iter:[180/192], Time: 0.73, lr: [0.0009577118805211255], Loss: 4.076417, Acc:0.178551, Semantic loss: 0.983833, BCE loss: 2.130189, SB loss: 0.962396
2025-01-17 14:39:31,607 Epoch: [0/20] Iter:[190/192], Time: 0.73, lr: [0.0009553565292586524], Loss: 4.057288, Acc:0.180360, Semantic loss: 0.983765, BCE loss: 2.111129, SB loss: 0.962394
