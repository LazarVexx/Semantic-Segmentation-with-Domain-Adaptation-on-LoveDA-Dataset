2025-01-25 19:16:07,598 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-25 19:16:07,598 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa-Rural/train.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.7
  SB_WEIGHTS: 0.5
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 1024
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: True
  AUG: True
  AUG1: False
  AUG2: True
  AUG3: True
  AUG4: False
  AUG_CHANCE: True
  BASE_SIZE: 720
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  D1: False
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: LS
  IGNORE_LABEL: 0
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.0002
  LR: 0.001
  LR_D1: 0.0001
  LR_D2: 0.0001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: adam
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: True
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-25 19:16:07,757 Attention!!!
2025-01-25 19:16:07,758 Loaded 302 parameters!
2025-01-25 19:16:07,758 Over!!!
2025-01-25 19:16:14,043 Epoch: [0/20] Iter:[0/286], Time: 6.03, lr: [0.0002], Loss: 4.780138, Loss_D1: 0.493442, Loss_D2: 0.505309, Acc:0.155735, Semantic loss: 0.590168
2025-01-25 19:16:33,918 Epoch: [0/20] Iter:[10/286], Time: 2.36, lr: [0.00019968528716020792], Loss: 3.499404, Loss_D1: 0.259479, Loss_D2: 0.273549, Acc:0.260344, Semantic loss: 0.479898
2025-01-25 19:16:51,513 Epoch: [0/20] Iter:[20/286], Time: 2.07, lr: [0.00019937051919947566], Loss: 2.929448, Loss_D1: 0.262999, Loss_D2: 0.251590, Acc:0.279291, Semantic loss: 0.424679
2025-01-25 19:17:09,343 Epoch: [0/20] Iter:[30/286], Time: 1.98, lr: [0.00019905569601142006], Loss: 2.626673, Loss_D1: 0.248734, Loss_D2: 0.254610, Acc:0.305992, Semantic loss: 0.381071
2025-01-25 19:17:26,795 Epoch: [0/20] Iter:[40/286], Time: 1.92, lr: [0.00019874081748926526], Loss: 2.464232, Loss_D1: 0.263131, Loss_D2: 0.254333, Acc:0.329727, Semantic loss: 0.342407
2025-01-25 19:17:44,597 Epoch: [0/20] Iter:[50/286], Time: 1.89, lr: [0.00019842588352584058], Loss: 2.331425, Loss_D1: 0.242728, Loss_D2: 0.248426, Acc:0.342072, Semantic loss: 0.322755
2025-01-25 19:18:02,118 Epoch: [0/20] Iter:[60/286], Time: 1.87, lr: [0.00019811089401357843], Loss: 2.195965, Loss_D1: 0.239734, Loss_D2: 0.251774, Acc:0.357410, Semantic loss: 0.301706
2025-01-25 19:18:19,740 Epoch: [0/20] Iter:[70/286], Time: 1.86, lr: [0.00019779584884451205], Loss: 2.136915, Loss_D1: 0.256802, Loss_D2: 0.248990, Acc:0.368417, Semantic loss: 0.288983
2025-01-25 19:18:37,379 Epoch: [0/20] Iter:[80/286], Time: 1.84, lr: [0.00019748074791027339], Loss: 2.096978, Loss_D1: 0.249878, Loss_D2: 0.251625, Acc:0.375583, Semantic loss: 0.278474
2025-01-25 19:18:54,938 Epoch: [0/20] Iter:[90/286], Time: 1.83, lr: [0.00019716559110209074], Loss: 2.043135, Loss_D1: 0.242943, Loss_D2: 0.251805, Acc:0.381569, Semantic loss: 0.269001
2025-01-25 19:19:12,630 Epoch: [0/20] Iter:[100/286], Time: 1.83, lr: [0.00019685037831078682], Loss: 1.983465, Loss_D1: 0.249565, Loss_D2: 0.239382, Acc:0.386759, Semantic loss: 0.260038
2025-01-25 19:19:30,023 Epoch: [0/20] Iter:[110/286], Time: 1.82, lr: [0.00019653510942677625], Loss: 1.942666, Loss_D1: 0.255364, Loss_D2: 0.255451, Acc:0.388468, Semantic loss: 0.254672
2025-01-25 19:19:47,543 Epoch: [0/20] Iter:[120/286], Time: 1.81, lr: [0.00019621978434006338], Loss: 1.911612, Loss_D1: 0.246072, Loss_D2: 0.253507, Acc:0.393377, Semantic loss: 0.247586
2025-01-25 19:20:05,247 Epoch: [0/20] Iter:[130/286], Time: 1.81, lr: [0.00019590440294024007], Loss: 1.904552, Loss_D1: 0.241552, Loss_D2: 0.252935, Acc:0.399439, Semantic loss: 0.243264
2025-01-25 19:20:22,874 Epoch: [0/20] Iter:[140/286], Time: 1.81, lr: [0.00019558896511648338], Loss: 1.865253, Loss_D1: 0.231963, Loss_D2: 0.243418, Acc:0.404306, Semantic loss: 0.238383
2025-01-25 19:20:40,517 Epoch: [0/20] Iter:[150/286], Time: 1.80, lr: [0.00019527347075755326], Loss: 1.848104, Loss_D1: 0.244910, Loss_D2: 0.251991, Acc:0.406128, Semantic loss: 0.235040
2025-01-25 19:20:58,152 Epoch: [0/20] Iter:[160/286], Time: 1.80, lr: [0.00019495791975179016], Loss: 1.836872, Loss_D1: 0.247854, Loss_D2: 0.247951, Acc:0.409965, Semantic loss: 0.231874
2025-01-25 19:21:15,861 Epoch: [0/20] Iter:[170/286], Time: 1.80, lr: [0.0001946423119871129], Loss: 1.808858, Loss_D1: 0.230928, Loss_D2: 0.250040, Acc:0.413002, Semantic loss: 0.227831
2025-01-25 19:21:33,308 Epoch: [0/20] Iter:[180/286], Time: 1.80, lr: [0.000194326647351016], Loss: 1.785778, Loss_D1: 0.250847, Loss_D2: 0.254075, Acc:0.416925, Semantic loss: 0.225624
2025-01-25 19:21:51,253 Epoch: [0/20] Iter:[190/286], Time: 1.80, lr: [0.00019401092573056758], Loss: 1.771299, Loss_D1: 0.236495, Loss_D2: 0.254606, Acc:0.420481, Semantic loss: 0.223009
2025-01-25 19:22:09,031 Epoch: [0/20] Iter:[200/286], Time: 1.80, lr: [0.00019369514701240685], Loss: 1.759463, Loss_D1: 0.233976, Loss_D2: 0.247756, Acc:0.423346, Semantic loss: 0.220782
2025-01-25 19:22:26,658 Epoch: [0/20] Iter:[210/286], Time: 1.79, lr: [0.00019337931108274169], Loss: 1.749222, Loss_D1: 0.232294, Loss_D2: 0.255308, Acc:0.425015, Semantic loss: 0.218254
2025-01-25 19:22:44,271 Epoch: [0/20] Iter:[220/286], Time: 1.79, lr: [0.00019306341782734628], Loss: 1.737656, Loss_D1: 0.238609, Loss_D2: 0.250847, Acc:0.428933, Semantic loss: 0.215542
2025-01-25 19:23:01,824 Epoch: [0/20] Iter:[230/286], Time: 1.79, lr: [0.0001927474671315586], Loss: 1.731828, Loss_D1: 0.273126, Loss_D2: 0.256583, Acc:0.430391, Semantic loss: 0.213867
2025-01-25 19:23:19,319 Epoch: [0/20] Iter:[240/286], Time: 1.79, lr: [0.00019243145888027797], Loss: 1.716048, Loss_D1: 0.233571, Loss_D2: 0.245176, Acc:0.432212, Semantic loss: 0.211700
2025-01-25 19:23:36,704 Epoch: [0/20] Iter:[250/286], Time: 1.79, lr: [0.0001921153929579627], Loss: 1.713203, Loss_D1: 0.229147, Loss_D2: 0.249899, Acc:0.433983, Semantic loss: 0.209970
2025-01-25 19:23:54,156 Epoch: [0/20] Iter:[260/286], Time: 1.79, lr: [0.0001917992692486273], Loss: 1.697887, Loss_D1: 0.227247, Loss_D2: 0.249237, Acc:0.434259, Semantic loss: 0.208422
2025-01-25 19:24:11,563 Epoch: [0/20] Iter:[270/286], Time: 1.78, lr: [0.00019148308763584034], Loss: 1.687716, Loss_D1: 0.237026, Loss_D2: 0.238474, Acc:0.436873, Semantic loss: 0.206355
2025-01-25 19:24:28,973 Epoch: [0/20] Iter:[280/286], Time: 1.78, lr: [0.00019116684800272153], Loss: 1.674979, Loss_D1: 0.242430, Loss_D2: 0.245568, Acc:0.438480, Semantic loss: 0.204277
2025-01-25 19:29:59,402 0 [0.         0.43485234 0.16898059 0.07666923 0.11746133 0.02683349
 0.07066184 0.00524531] 0.12867201667921863
2025-01-25 19:29:59,403 1 [0.         0.46258077 0.23273309 0.15174807 0.13142397 0.00870882
 0.0969555  0.03297248] 0.15958895939607745
2025-01-25 19:29:59,404 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-25 19:29:59,642 Loss: 1.236, MeanIU:  0.1596, Best_mIoU:  0.1596
2025-01-25 19:29:59,642 [0.         0.46258077 0.23273309 0.15174807 0.13142397 0.00870882
 0.0969555  0.03297248]
2025-01-25 19:30:01,421 Epoch: [1/20] Iter:[0/286], Time: 1.77, lr: [0.0003819541526485999], Loss: 1.353054, Loss_D1: 0.265396, Loss_D2: 0.257759, Acc:0.667066, Semantic loss: 0.112355
2025-01-25 19:30:22,436 Epoch: [1/20] Iter:[10/286], Time: 2.07, lr: [0.00038132148722882446], Loss: 1.392546, Loss_D1: 0.241019, Loss_D2: 0.235296, Acc:0.471800, Semantic loss: 0.166371
2025-01-25 19:30:47,771 Epoch: [1/20] Iter:[20/286], Time: 2.29, lr: [0.0003806887051563718], Loss: 1.420779, Loss_D1: 0.247806, Loss_D2: 0.241691, Acc:0.491630, Semantic loss: 0.162710
2025-01-25 19:31:12,463 Epoch: [1/20] Iter:[30/286], Time: 2.35, lr: [0.0003800558061942085], Loss: 1.426052, Loss_D1: 0.243197, Loss_D2: 0.244897, Acc:0.480198, Semantic loss: 0.163157
