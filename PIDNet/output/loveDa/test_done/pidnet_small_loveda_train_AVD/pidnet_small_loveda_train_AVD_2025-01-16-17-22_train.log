2025-01-16 17:22:33,690 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-16 17:22:33,692 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa/target.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.9
  SB_WEIGHTS: 1.0
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: True
  AUG: False
  AUG1: False
  AUG2: False
  AUG3: False
  AUG4: False
  AUG_CHANCE: False
  BASE_SIZE: 1440
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 255
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADVT1: 0.0002
  LR: 0.01
  LR_D1: 0.0001
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-16 17:22:33,932 Attention!!!
2025-01-16 17:22:33,932 Loaded 302 parameters!
2025-01-16 17:22:33,933 Over!!!
2025-01-16 17:22:44,039 Epoch: [0/20] Iter:[0/192], Time: 8.28, lr: [0.01], Loss: 0.695913, Loss_D: 0.003610, Acc:0.109682, Semantic loss: 0.704940, BCE loss: 3.269089
2025-01-16 17:23:09,893 Epoch: [0/20] Iter:[10/192], Time: 3.10, lr: [0.009976559445324192], Loss: 0.695426, Loss_D: 0.039706, Acc:0.130837, Semantic loss: 0.654609, BCE loss: 3.624116
2025-01-16 17:23:38,469 Epoch: [0/20] Iter:[20/192], Time: 2.99, lr: [0.009953112769592761], Loss: 0.694104, Loss_D: 0.075773, Acc:0.129125, Semantic loss: 0.662189, BCE loss: 3.573870
2025-01-16 17:24:02,872 Epoch: [0/20] Iter:[30/192], Time: 2.81, lr: [0.009929659955177281], Loss: 0.693879, Loss_D: 0.111864, Acc:0.130566, Semantic loss: 0.662289, BCE loss: 3.612428
2025-01-16 17:24:23,416 Epoch: [0/20] Iter:[40/192], Time: 2.63, lr: [0.009906200984352154], Loss: 0.695816, Loss_D: 0.147887, Acc:0.131852, Semantic loss: 0.659223, BCE loss: 3.547256
2025-01-16 17:24:44,227 Epoch: [0/20] Iter:[50/192], Time: 2.52, lr: [0.009882735839293803], Loss: 0.697723, Loss_D: 0.183773, Acc:0.130386, Semantic loss: 0.661273, BCE loss: 3.588121
2025-01-16 17:25:04,744 Epoch: [0/20] Iter:[60/192], Time: 2.44, lr: [0.00985926450207989], Loss: 0.697779, Loss_D: 0.219685, Acc:0.131370, Semantic loss: 0.660199, BCE loss: 3.669074
2025-01-16 17:25:26,413 Epoch: [0/20] Iter:[70/192], Time: 2.40, lr: [0.009835786954688485], Loss: 0.700907, Loss_D: 0.255542, Acc:0.130687, Semantic loss: 0.660100, BCE loss: 3.706432
2025-01-16 17:25:46,029 Epoch: [0/20] Iter:[80/192], Time: 2.35, lr: [0.00981230317899726], Loss: 0.701296, Loss_D: 0.291631, Acc:0.129643, Semantic loss: 0.662674, BCE loss: 3.718955
2025-01-16 17:26:06,933 Epoch: [0/20] Iter:[90/192], Time: 2.32, lr: [0.009788813156782662], Loss: 0.699278, Loss_D: 0.327867, Acc:0.127919, Semantic loss: 0.663651, BCE loss: 3.744182
2025-01-16 17:26:30,077 Epoch: [0/20] Iter:[100/192], Time: 2.32, lr: [0.009765316869719067], Loss: 0.699581, Loss_D: 0.363825, Acc:0.127096, Semantic loss: 0.665325, BCE loss: 3.759680
2025-01-16 17:26:49,878 Epoch: [0/20] Iter:[110/192], Time: 2.29, lr: [0.009741814299377942], Loss: 0.698639, Loss_D: 0.399758, Acc:0.126786, Semantic loss: 0.665154, BCE loss: 3.752395
2025-01-16 17:27:10,308 Epoch: [0/20] Iter:[120/192], Time: 2.27, lr: [0.009718305427226986], Loss: 0.698715, Loss_D: 0.435606, Acc:0.126455, Semantic loss: 0.664927, BCE loss: 3.763675
2025-01-16 17:27:34,533 Epoch: [0/20] Iter:[130/192], Time: 2.28, lr: [0.009694790234629266], Loss: 0.699967, Loss_D: 0.471479, Acc:0.127184, Semantic loss: 0.664353, BCE loss: 3.743072
2025-01-16 17:27:58,544 Epoch: [0/20] Iter:[140/192], Time: 2.29, lr: [0.009671268702842338], Loss: 0.699687, Loss_D: 0.507795, Acc:0.127077, Semantic loss: 0.664296, BCE loss: 3.737823
2025-01-16 17:28:20,292 Epoch: [0/20] Iter:[150/192], Time: 2.28, lr: [0.009647740813017376], Loss: 0.699428, Loss_D: 0.543698, Acc:0.126975, Semantic loss: 0.664320, BCE loss: 3.723513
2025-01-16 17:28:46,913 Epoch: [0/20] Iter:[160/192], Time: 2.31, lr: [0.009624206546198262], Loss: 0.698920, Loss_D: 0.579667, Acc:0.125772, Semantic loss: 0.666369, BCE loss: 3.721490
2025-01-16 17:29:08,791 Epoch: [0/20] Iter:[170/192], Time: 2.30, lr: [0.009600665883320689], Loss: 0.700598, Loss_D: 0.615351, Acc:0.125627, Semantic loss: 0.666865, BCE loss: 3.726631
2025-01-16 17:29:31,401 Epoch: [0/20] Iter:[180/192], Time: 2.30, lr: [0.009577118805211254], Loss: 0.701374, Loss_D: 0.650859, Acc:0.126131, Semantic loss: 0.665388, BCE loss: 3.746736
2025-01-16 17:29:52,939 Epoch: [0/20] Iter:[190/192], Time: 2.29, lr: [0.009553565292586523], Loss: 0.702892, Loss_D: 0.686343, Acc:0.126185, Semantic loss: 0.665996, BCE loss: 3.750799
2025-01-16 17:33:36,474 0 [0.         0.08569805 0.01632236 0.01184493 0.05821127 0.03891402
 0.05291985 0.02482777] 0.036092282532074846
2025-01-16 17:33:36,474 1 [0.         0.30494701 0.03412292 0.01361208 0.09024488 0.03761582
 0.01658765 0.01606493] 0.06414941061422623
2025-01-16 17:33:36,475 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-16 17:33:36,642 Loss: 4.324, MeanIU:  0.0641, Best_mIoU:  0.0641
2025-01-16 17:33:36,643 [0.         0.30494701 0.03412292 0.01361208 0.09024488 0.03761582
 0.01658765 0.01606493]
2025-01-16 17:33:38,393 Epoch: [1/20] Iter:[0/192], Time: 1.75, lr: [0.009548853816214998], Loss: 0.678754, Loss_D: 0.003477, Acc:0.155343, Semantic loss: 0.639394, BCE loss: 2.784894
2025-01-16 17:34:21,036 Epoch: [1/20] Iter:[10/192], Time: 4.04, lr: [0.009525292556561479], Loss: 0.725755, Loss_D: 0.038779, Acc:0.134030, Semantic loss: 0.658482, BCE loss: 4.081823
2025-01-16 17:35:03,701 Epoch: [1/20] Iter:[20/192], Time: 4.15, lr: [0.00950172481957719], Loss: 0.717899, Loss_D: 0.074483, Acc:0.129355, Semantic loss: 0.658271, BCE loss: 3.972404
2025-01-16 17:35:49,646 Epoch: [1/20] Iter:[30/192], Time: 4.29, lr: [0.009478150585620286], Loss: 0.719743, Loss_D: 0.109891, Acc:0.127388, Semantic loss: 0.662590, BCE loss: 4.003609
2025-01-16 17:36:33,588 Epoch: [1/20] Iter:[40/192], Time: 4.32, lr: [0.009454569834934885], Loss: 0.718192, Loss_D: 0.145991, Acc:0.126130, Semantic loss: 0.664522, BCE loss: 3.976478
2025-01-16 17:37:18,799 Epoch: [1/20] Iter:[50/192], Time: 4.36, lr: [0.009430982547650114], Loss: 0.723199, Loss_D: 0.181377, Acc:0.126969, Semantic loss: 0.664165, BCE loss: 3.921063
2025-01-16 17:38:09,360 Epoch: [1/20] Iter:[60/192], Time: 4.47, lr: [0.009407388703779091], Loss: 0.722202, Loss_D: 0.216943, Acc:0.127128, Semantic loss: 0.664295, BCE loss: 3.922133
2025-01-16 17:38:49,349 Epoch: [1/20] Iter:[70/192], Time: 4.40, lr: [0.009383788283217955], Loss: 0.721864, Loss_D: 0.251898, Acc:0.127392, Semantic loss: 0.663263, BCE loss: 3.903730
2025-01-16 17:39:41,845 Epoch: [1/20] Iter:[80/192], Time: 4.51, lr: [0.00936018126574482], Loss: 0.725443, Loss_D: 0.286533, Acc:0.127451, Semantic loss: 0.662717, BCE loss: 3.886948
2025-01-16 17:40:31,780 Epoch: [1/20] Iter:[90/192], Time: 4.56, lr: [0.009336567631018769], Loss: 0.730269, Loss_D: 0.321003, Acc:0.125969, Semantic loss: 0.665103, BCE loss: 3.901575
2025-01-16 17:41:16,426 Epoch: [1/20] Iter:[100/192], Time: 4.55, lr: [0.009312947358578814], Loss: 0.728083, Loss_D: 0.357732, Acc:0.125202, Semantic loss: 0.666372, BCE loss: 3.915085
2025-01-16 17:42:09,554 Epoch: [1/20] Iter:[110/192], Time: 4.62, lr: [0.009289320427842841], Loss: 0.727494, Loss_D: 0.392395, Acc:0.125477, Semantic loss: 0.666965, BCE loss: 3.900996
2025-01-16 17:42:57,793 Epoch: [1/20] Iter:[120/192], Time: 4.64, lr: [0.009265686818106552], Loss: 0.729783, Loss_D: 0.427645, Acc:0.125978, Semantic loss: 0.666419, BCE loss: 3.899180
2025-01-16 17:43:35,890 Epoch: [1/20] Iter:[130/192], Time: 4.57, lr: [0.009242046508542393], Loss: 0.730462, Loss_D: 0.463139, Acc:0.125913, Semantic loss: 0.666180, BCE loss: 3.882432
2025-01-16 17:44:21,105 Epoch: [1/20] Iter:[140/192], Time: 4.57, lr: [0.009218399478198466], Loss: 0.731834, Loss_D: 0.497956, Acc:0.126208, Semantic loss: 0.665760, BCE loss: 3.884322
2025-01-16 17:45:04,091 Epoch: [1/20] Iter:[150/192], Time: 4.55, lr: [0.009194745705997428], Loss: 0.735858, Loss_D: 0.532249, Acc:0.126653, Semantic loss: 0.664933, BCE loss: 3.881244
2025-01-16 17:45:55,329 Epoch: [1/20] Iter:[160/192], Time: 4.59, lr: [0.00917108517073538], Loss: 0.737545, Loss_D: 0.566032, Acc:0.125599, Semantic loss: 0.666322, BCE loss: 3.855458
2025-01-16 17:46:48,422 Epoch: [1/20] Iter:[170/192], Time: 4.63, lr: [0.00914741785108075], Loss: 0.742264, Loss_D: 0.600161, Acc:0.125694, Semantic loss: 0.666027, BCE loss: 3.862701
2025-01-16 17:47:32,252 Epoch: [1/20] Iter:[180/192], Time: 4.62, lr: [0.00912374372557314], Loss: 0.742522, Loss_D: 0.635104, Acc:0.125442, Semantic loss: 0.666195, BCE loss: 3.840217
2025-01-16 17:48:29,487 Epoch: [1/20] Iter:[190/192], Time: 4.67, lr: [0.009100062772622186], Loss: 0.743912, Loss_D: 0.670198, Acc:0.125488, Semantic loss: 0.666321, BCE loss: 3.835657
2025-01-16 17:53:00,940 0 [0.         0.08258701 0.01659878 0.0114234  0.0591503  0.03783515
 0.05458136 0.0244391 ] 0.03582688835712075
2025-01-16 17:53:00,941 1 [0.         0.26425412 0.0311706  0.01448531 0.08620998 0.03589847
 0.01753943 0.01418954] 0.05796843183982145
2025-01-16 17:53:00,943 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-16 17:53:01,087 Loss: 4.280, MeanIU:  0.0580, Best_mIoU:  0.0641
2025-01-16 17:53:01,088 [0.         0.26425412 0.0311706  0.01448531 0.08620998 0.03589847
 0.01753943 0.01418954]
2025-01-16 17:53:05,729 Epoch: [2/20] Iter:[0/192], Time: 4.64, lr: [0.009095325760829623], Loss: 0.724584, Loss_D: 0.003131, Acc:0.133143, Semantic loss: 0.660847, BCE loss: 4.232686
2025-01-16 17:53:52,036 Epoch: [2/20] Iter:[10/192], Time: 4.63, lr: [0.009071636586262652], Loss: 0.772960, Loss_D: 0.038132, Acc:0.133494, Semantic loss: 0.666101, BCE loss: 4.009922
