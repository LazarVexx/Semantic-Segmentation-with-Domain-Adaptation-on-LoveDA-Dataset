2025-01-16 21:16:33,748 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-16 21:16:33,748 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa/target.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.9
  SB_WEIGHTS: 1.0
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 3
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: True
  AUG: False
  AUG1: False
  AUG2: False
  AUG3: False
  AUG4: False
  AUG_CHANCE: False
  BASE_SIZE: 1440
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 255
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV1: 0.001
  LAMBDA_ADV2: 0.001
  LR: 0.01
  LR_D1: 0.01
  LR_D2: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-16 21:16:33,916 Attention!!!
2025-01-16 21:16:33,916 Loaded 302 parameters!
2025-01-16 21:16:33,916 Over!!!
2025-01-16 21:16:39,909 Epoch: [0/20] Iter:[0/192], Time: 5.79, lr: [0.01], Loss: 0.000007, Loss_D1: 0.001814, Loss_D2: 0.001799, Acc:0.159502, Semantic loss: 0.593100
2025-01-16 21:16:56,259 Epoch: [0/20] Iter:[10/192], Time: 13.98, lr: [0.009976559445324192], Loss: 0.005584, Loss_D1: 0.000000, Loss_D2: 2.224730, Acc:0.212689, Semantic loss: 0.533179
2025-01-16 21:17:12,700 Epoch: [0/20] Iter:[20/192], Time: 22.15, lr: [0.009953112769592761], Loss: 0.004202, Loss_D1: 0.690185, Loss_D2: 2.807243, Acc:0.270741, Semantic loss: 0.486777
2025-01-16 21:17:28,875 Epoch: [0/20] Iter:[30/192], Time: 30.32, lr: [0.009929659955177281], Loss: 0.006671, Loss_D1: 0.431482, Loss_D2: 0.000000, Acc:0.301917, Semantic loss: 0.453526
2025-01-16 21:17:45,099 Epoch: [0/20] Iter:[40/192], Time: 38.48, lr: [0.009906200984352154], Loss: 0.005842, Loss_D1: 0.000000, Loss_D2: 1.854697, Acc:0.322534, Semantic loss: 0.418524
2025-01-16 21:18:02,096 Epoch: [0/20] Iter:[50/192], Time: 46.69, lr: [0.009882735839293803], Loss: 0.005080, Loss_D1: 0.006511, Loss_D2: 0.000000, Acc:0.344097, Semantic loss: 0.396960
2025-01-16 21:18:18,581 Epoch: [0/20] Iter:[60/192], Time: 54.93, lr: [0.00985926450207989], Loss: 0.004417, Loss_D1: 0.000038, Loss_D2: 0.000000, Acc:0.348211, Semantic loss: 0.378996
2025-01-16 21:18:34,989 Epoch: [0/20] Iter:[70/192], Time: 63.17, lr: [0.009835786954688485], Loss: 0.003866, Loss_D1: 0.054113, Loss_D2: 0.000000, Acc:0.362607, Semantic loss: 0.360674
2025-01-16 21:18:51,083 Epoch: [0/20] Iter:[80/192], Time: 71.40, lr: [0.00981230317899726], Loss: 0.003413, Loss_D1: 0.006869, Loss_D2: 0.000205, Acc:0.369587, Semantic loss: 0.345426
2025-01-16 21:19:07,744 Epoch: [0/20] Iter:[90/192], Time: 79.61, lr: [0.009788813156782662], Loss: 0.003067, Loss_D1: 0.000205, Loss_D2: 0.141491, Acc:0.379697, Semantic loss: 0.331388
2025-01-16 21:19:24,206 Epoch: [0/20] Iter:[100/192], Time: 87.83, lr: [0.009765316869719067], Loss: 0.002817, Loss_D1: 0.004965, Loss_D2: 0.545275, Acc:0.390778, Semantic loss: 0.319587
2025-01-16 21:19:40,694 Epoch: [0/20] Iter:[110/192], Time: 96.06, lr: [0.009741814299377942], Loss: 0.002676, Loss_D1: 0.002911, Loss_D2: 0.000000, Acc:0.396329, Semantic loss: 0.310939
2025-01-16 21:19:57,188 Epoch: [0/20] Iter:[120/192], Time: 104.30, lr: [0.009718305427226986], Loss: 0.002514, Loss_D1: 0.002239, Loss_D2: 0.548585, Acc:0.400220, Semantic loss: 0.302734
2025-01-16 21:20:13,405 Epoch: [0/20] Iter:[130/192], Time: 112.52, lr: [0.009694790234629266], Loss: 0.002339, Loss_D1: 0.001763, Loss_D2: 0.006915, Acc:0.405866, Semantic loss: 0.295874
2025-01-16 21:20:29,810 Epoch: [0/20] Iter:[140/192], Time: 120.73, lr: [0.009671268702842338], Loss: 0.002184, Loss_D1: 0.001747, Loss_D2: 0.000000, Acc:0.408515, Semantic loss: 0.289327
2025-01-16 21:20:46,955 Epoch: [0/20] Iter:[150/192], Time: 128.98, lr: [0.009647740813017376], Loss: 0.002043, Loss_D1: 0.001879, Loss_D2: 0.078324, Acc:0.411859, Semantic loss: 0.284144
2025-01-16 21:21:03,958 Epoch: [0/20] Iter:[160/192], Time: 137.26, lr: [0.009624206546198262], Loss: 0.001918, Loss_D1: 0.001940, Loss_D2: 0.000006, Acc:0.417082, Semantic loss: 0.278213
2025-01-16 21:21:20,756 Epoch: [0/20] Iter:[170/192], Time: 145.54, lr: [0.009600665883320689], Loss: 0.001807, Loss_D1: 0.002378, Loss_D2: 0.006814, Acc:0.420098, Semantic loss: 0.274120
2025-01-16 21:21:37,788 Epoch: [0/20] Iter:[180/192], Time: 153.86, lr: [0.009577118805211254], Loss: 0.001708, Loss_D1: 0.001185, Loss_D2: 0.000139, Acc:0.423000, Semantic loss: 0.270262
2025-01-16 21:21:54,530 Epoch: [0/20] Iter:[190/192], Time: 162.18, lr: [0.009553565292586523], Loss: 0.001619, Loss_D1: 0.001441, Loss_D2: 0.004461, Acc:0.429049, Semantic loss: 0.265038
2025-01-16 21:25:56,966 0 [0.         0.33647269 0.06965656 0.03254965 0.20100761 0.04591402
 0.08173973 0.00216959] 0.09618873187548369
2025-01-16 21:25:56,966 1 [0.         0.42643738 0.09668979 0.09981847 0.15803588 0.01744362
 0.00156142 0.00102794] 0.1001268123356326
2025-01-16 21:25:56,967 2 [0.         0.09326626 0.02783277 0.02499931 0.0904437  0.03280052
 0.04243599 0.11720248] 0.05362262873264257
2025-01-16 21:25:56,968 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-16 21:25:57,150 Loss: 1.481, MeanIU:  0.0536, Best_mIoU:  0.0536
2025-01-16 21:25:57,150 [0.         0.09326626 0.02783277 0.02499931 0.0904437  0.03280052
 0.04243599 0.11720248]
2025-01-16 21:25:58,737 Epoch: [1/20] Iter:[0/192], Time: 1.59, lr: [0.009548853816214998], Loss: 0.000015, Loss_D1: 0.001964, Loss_D2: 0.000334, Acc:0.468541, Semantic loss: 0.146311
2025-01-16 21:26:14,287 Epoch: [1/20] Iter:[10/192], Time: 9.38, lr: [0.009525292556561479], Loss: 0.000009, Loss_D1: 0.001729, Loss_D2: 0.004533, Acc:0.454361, Semantic loss: 0.182179
2025-01-16 21:26:29,706 Epoch: [1/20] Iter:[20/192], Time: 17.10, lr: [0.00950172481957719], Loss: 0.000009, Loss_D1: 0.001760, Loss_D2: 0.000964, Acc:0.486284, Semantic loss: 0.175240
2025-01-16 21:26:44,965 Epoch: [1/20] Iter:[30/192], Time: 24.79, lr: [0.009478150585620286], Loss: 0.000009, Loss_D1: 0.001327, Loss_D2: 0.003297, Acc:0.500246, Semantic loss: 0.176885
2025-01-16 21:27:00,444 Epoch: [1/20] Iter:[40/192], Time: 32.48, lr: [0.009454569834934885], Loss: 0.000009, Loss_D1: 0.002945, Loss_D2: 0.000502, Acc:0.497378, Semantic loss: 0.181771
2025-01-16 21:27:16,007 Epoch: [1/20] Iter:[50/192], Time: 40.20, lr: [0.009430982547650114], Loss: 0.000009, Loss_D1: 0.001548, Loss_D2: 0.002990, Acc:0.487996, Semantic loss: 0.183196
2025-01-16 21:27:31,491 Epoch: [1/20] Iter:[60/192], Time: 47.94, lr: [0.009407388703779091], Loss: 0.000009, Loss_D1: 0.000604, Loss_D2: 0.002260, Acc:0.483490, Semantic loss: 0.182657
2025-01-16 21:27:46,947 Epoch: [1/20] Iter:[70/192], Time: 55.67, lr: [0.009383788283217955], Loss: 0.000008, Loss_D1: 0.002017, Loss_D2: 0.001785, Acc:0.487624, Semantic loss: 0.181441
2025-01-16 21:28:02,665 Epoch: [1/20] Iter:[80/192], Time: 63.43, lr: [0.00936018126574482], Loss: 0.000008, Loss_D1: 0.002487, Loss_D2: 0.001574, Acc:0.492055, Semantic loss: 0.180814
2025-01-16 21:28:18,622 Epoch: [1/20] Iter:[90/192], Time: 71.21, lr: [0.009336567631018769], Loss: 0.000008, Loss_D1: 0.001195, Loss_D2: 0.000938, Acc:0.489864, Semantic loss: 0.181007
2025-01-16 21:28:34,713 Epoch: [1/20] Iter:[100/192], Time: 79.05, lr: [0.009312947358578814], Loss: 0.000008, Loss_D1: 0.002385, Loss_D2: 0.001948, Acc:0.489440, Semantic loss: 0.180049
2025-01-16 21:28:50,650 Epoch: [1/20] Iter:[110/192], Time: 86.91, lr: [0.009289320427842841], Loss: 0.000008, Loss_D1: 0.002635, Loss_D2: 0.001525, Acc:0.491164, Semantic loss: 0.178597
2025-01-16 21:29:06,919 Epoch: [1/20] Iter:[120/192], Time: 94.81, lr: [0.009265686818106552], Loss: 0.000008, Loss_D1: 0.001497, Loss_D2: 0.001490, Acc:0.493781, Semantic loss: 0.178566
2025-01-16 21:29:22,952 Epoch: [1/20] Iter:[130/192], Time: 102.73, lr: [0.009242046508542393], Loss: 0.000008, Loss_D1: 0.002457, Loss_D2: 0.001681, Acc:0.496391, Semantic loss: 0.178174
2025-01-16 21:29:39,328 Epoch: [1/20] Iter:[140/192], Time: 110.67, lr: [0.009218399478198466], Loss: 0.000008, Loss_D1: 0.001147, Loss_D2: 0.001432, Acc:0.497651, Semantic loss: 0.178700
2025-01-16 21:29:55,752 Epoch: [1/20] Iter:[150/192], Time: 118.65, lr: [0.009194745705997428], Loss: 0.000008, Loss_D1: 0.001651, Loss_D2: 0.001459, Acc:0.499667, Semantic loss: 0.178194
2025-01-16 21:30:12,160 Epoch: [1/20] Iter:[160/192], Time: 126.67, lr: [0.00917108517073538], Loss: 0.000008, Loss_D1: 0.002750, Loss_D2: 0.001471, Acc:0.501803, Semantic loss: 0.177430
2025-01-16 21:30:27,872 Epoch: [1/20] Iter:[170/192], Time: 134.68, lr: [0.00914741785108075], Loss: 0.000008, Loss_D1: 0.001456, Loss_D2: 0.001091, Acc:0.505208, Semantic loss: 0.176986
2025-01-16 21:30:43,649 Epoch: [1/20] Iter:[180/192], Time: 142.68, lr: [0.00912374372557314], Loss: 0.000511, Loss_D1: 0.001487, Loss_D2: 0.000000, Acc:0.505610, Semantic loss: 0.177582
2025-01-16 21:30:59,208 Epoch: [1/20] Iter:[190/192], Time: 150.65, lr: [0.009100062772622186], Loss: 0.000728, Loss_D1: 0.001471, Loss_D2: 1.871066, Acc:0.507073, Semantic loss: 0.177180
2025-01-16 21:34:58,472 0 [0.         0.37519694 0.09581665 0.05130241 0.26079889 0.05194108
 0.07274211 0.00097335] 0.11359642771999802
2025-01-16 21:34:58,472 1 [0.         0.4395868  0.08241762 0.09822097 0.18129044 0.03647684
 0.0011454  0.00045237] 0.10494880467182596
2025-01-16 21:34:58,473 2 [0.         0.11049431 0.02929797 0.02027582 0.05819434 0.02621379
 0.03660675 0.08079127] 0.04523428194204694
2025-01-16 21:34:58,473 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_AVDcheckpoint.pth.tar
2025-01-16 21:34:58,585 Loss: 1.501, MeanIU:  0.0452, Best_mIoU:  0.0536
2025-01-16 21:34:58,585 [0.         0.11049431 0.02929797 0.02027582 0.05819434 0.02621379
 0.03660675 0.08079127]
2025-01-16 21:35:00,229 Epoch: [2/20] Iter:[0/192], Time: 1.64, lr: [0.009095325760829623], Loss: 0.000003, Loss_D1: 0.002050, Loss_D2: 31.564781, Acc:0.408886, Semantic loss: 0.187548
2025-01-16 21:35:15,801 Epoch: [2/20] Iter:[10/192], Time: 9.44, lr: [0.009071636586262652], Loss: 0.009353, Loss_D1: 0.001826, Loss_D2: 0.000000, Acc:0.501411, Semantic loss: 0.161353
2025-01-16 21:35:31,517 Epoch: [2/20] Iter:[20/192], Time: 17.26, lr: [0.009047940536290279], Loss: 0.012871, Loss_D1: 0.002365, Loss_D2: 3.536350, Acc:0.510406, Semantic loss: 0.165836
2025-01-16 21:35:47,060 Epoch: [2/20] Iter:[30/192], Time: 25.09, lr: [0.009024237588898336], Loss: 0.013193, Loss_D1: 0.001984, Loss_D2: 0.106579, Acc:0.514879, Semantic loss: 0.165960
2025-01-16 21:36:02,628 Epoch: [2/20] Iter:[40/192], Time: 32.89, lr: [0.009000527721937697], Loss: 0.012292, Loss_D1: 0.001635, Loss_D2: 17.640400, Acc:0.509151, Semantic loss: 0.167132
2025-01-16 21:36:18,215 Epoch: [2/20] Iter:[50/192], Time: 40.68, lr: [0.008976810913123051], Loss: 0.017817, Loss_D1: 0.001146, Loss_D2: 0.000000, Acc:0.516935, Semantic loss: 0.166960
2025-01-16 21:36:33,783 Epoch: [2/20] Iter:[60/192], Time: 48.48, lr: [0.008953087140031669], Loss: 0.016336, Loss_D1: 0.001756, Loss_D2: 0.000000, Acc:0.521302, Semantic loss: 0.166261
2025-01-16 21:36:49,332 Epoch: [2/20] Iter:[70/192], Time: 56.27, lr: [0.008929356380102142], Loss: 0.014717, Loss_D1: 0.000000, Loss_D2: 0.000000, Acc:0.519728, Semantic loss: 0.169456
2025-01-16 21:37:04,860 Epoch: [2/20] Iter:[80/192], Time: 64.06, lr: [0.008905618610633112], Loss: 0.016081, Loss_D1: 0.306896, Loss_D2: 0.538121, Acc:0.524050, Semantic loss: 0.168837
2025-01-16 21:37:20,202 Epoch: [2/20] Iter:[90/192], Time: 71.83, lr: [0.008881873808781991], Loss: 0.016183, Loss_D1: 0.000000, Loss_D2: 0.000000, Acc:0.527469, Semantic loss: 0.167860
2025-01-16 21:37:35,739 Epoch: [2/20] Iter:[100/192], Time: 79.59, lr: [0.008858121951563658], Loss: 0.026617, Loss_D1: 0.000000, Loss_D2: 0.000000, Acc:0.525582, Semantic loss: 0.167373
2025-01-16 21:37:51,488 Epoch: [2/20] Iter:[110/192], Time: 87.36, lr: [0.008834363015849136], Loss: 0.029839, Loss_D1: 0.000000, Loss_D2: 0.000000, Acc:0.524134, Semantic loss: 0.166932
