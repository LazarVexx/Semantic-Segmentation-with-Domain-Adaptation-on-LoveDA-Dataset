2025-01-16 18:20:54,412 Namespace(cfg='configs/loveDa/pidnet_small_loveda_train_NO_AVD.yaml', seed=304, opts=['GPUS', '[0]', 'TRAIN.BATCH_SIZE_PER_GPU', '6'])
2025-01-16 18:20:54,412 AUTO_RESUME: False
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: loveDa
  EXTRA_TRAIN_SET: 
  NUM_CLASSES: 8
  ROOT: data/
  SOURCE_DATASET: loveDA-Urban
  SOURCE_TEST_SET: list/loveDA-Urban/val.lst
  SOURCE_TRAIN_SET: list/loveDA-Urban/train.lst
  TARGET_DATASET: loveDA-Rural
  TARGET_SET: list/loveDa/target.lst
  TARGET_TEST_SET: list/loveDA-Rural/val.lst
  TARGET_TRAIN_SET: list/loveDA-Rural/train.lst
  TEST_SET: list/loveDa-Rural/val.lst
  TRAIN_SET: list/loveDa/train.lst
GPUS: (0,)
LOG_DIR: log
LOSS:
  BALANCE_WEIGHTS: [0.4, 1.0]
  CLASS_BALANCE: False
  OHEMKEEP: 131072
  OHEMTHRES: 0.9
  SB_WEIGHTS: 1.0
  USE_DICE: False
  USE_FOCAL: False
  USE_OHEM: True
MODEL:
  ALIGN_CORNERS: True
  NAME: pidnet_small
  NUM_OUTPUTS: 2
  PRETRAINED: pretrained_models/imagenet/PIDNet_S_ImageNet.pth.tar
OUTPUT_DIR: output
PIN_MEMORY: True
PRINT_FREQ: 10
TEST:
  BASE_SIZE: 2048
  BATCH_SIZE_PER_GPU: 6
  FLIP_TEST: False
  IMAGE_SIZE: [1024, 1024]
  MODEL_FILE: 
  MULTI_SCALE: False
  OUTPUT_INDEX: 1
TRAIN:
  ADVERSARIAL: False
  AUG: False
  AUG1: False
  AUG2: False
  AUG3: False
  AUG4: False
  AUG_CHANCE: False
  BASE_SIZE: 1440
  BATCH_SIZE_PER_GPU: 6
  BEGIN_EPOCH: 0
  END_EPOCH: 20
  EVAL_INTERVAL: 1
  EXTRA_EPOCH: 0
  EXTRA_LR: 0.001
  FLIP: True
  GAN: Vanilla
  IGNORE_LABEL: 255
  IMAGE_SIZE: [720, 720]
  LAMBDA_ADV: 0.001
  LR: 0.01
  LR_D1: 0.01
  MOMENTUM: 0.9
  MULTI_SCALE: True
  NESTEROV: False
  OPTIMIZER: sgd
  RESUME: False
  SCALE_FACTOR: 16
  SCHEDULER: False
  SHUFFLE: True
  WD: 0.0005
WORKERS: 0
2025-01-16 18:20:54,558 Attention!!!
2025-01-16 18:20:54,558 Loaded 302 parameters!
2025-01-16 18:20:54,558 Over!!!
2025-01-16 18:20:59,003 Epoch: [0/20] Iter:[0/192], Time: 2.95, lr: [0.01], Loss: 5.627950, Acc:0.142494, Semantic loss: 0.573717, BCE loss: 4.626530, SB loss: 0.427704
2025-01-16 18:21:06,490 Epoch: [0/20] Iter:[10/192], Time: 0.94, lr: [0.009976559445324192], Loss: 3.821042, Acc:0.241663, Semantic loss: 0.517980, BCE loss: 2.948654, SB loss: 0.354408
2025-01-16 18:21:13,638 Epoch: [0/20] Iter:[20/192], Time: 0.83, lr: [0.009953112769592761], Loss: 3.239323, Acc:0.274120, Semantic loss: 0.483371, BCE loss: 2.427789, SB loss: 0.328163
2025-01-16 18:21:20,786 Epoch: [0/20] Iter:[30/192], Time: 0.79, lr: [0.009929659955177281], Loss: 2.972329, Acc:0.305173, Semantic loss: 0.432234, BCE loss: 2.250129, SB loss: 0.289966
2025-01-16 18:21:27,923 Epoch: [0/20] Iter:[40/192], Time: 0.78, lr: [0.009906200984352154], Loss: 2.788505, Acc:0.335707, Semantic loss: 0.406729, BCE loss: 2.106505, SB loss: 0.275272
2025-01-16 18:21:35,148 Epoch: [0/20] Iter:[50/192], Time: 0.76, lr: [0.009882735839293803], Loss: 2.664933, Acc:0.345225, Semantic loss: 0.386047, BCE loss: 2.023934, SB loss: 0.254951
2025-01-16 18:21:42,154 Epoch: [0/20] Iter:[60/192], Time: 0.75, lr: [0.00985926450207989], Loss: 2.567303, Acc:0.351248, Semantic loss: 0.370635, BCE loss: 1.952659, SB loss: 0.244009
2025-01-16 18:21:49,302 Epoch: [0/20] Iter:[70/192], Time: 0.75, lr: [0.009835786954688485], Loss: 2.502351, Acc:0.362278, Semantic loss: 0.353378, BCE loss: 1.917981, SB loss: 0.230992
2025-01-16 18:21:56,288 Epoch: [0/20] Iter:[80/192], Time: 0.74, lr: [0.00981230317899726], Loss: 2.432171, Acc:0.369784, Semantic loss: 0.339676, BCE loss: 1.871692, SB loss: 0.220804
2025-01-16 18:22:03,453 Epoch: [0/20] Iter:[90/192], Time: 0.74, lr: [0.009788813156782662], Loss: 2.405154, Acc:0.379916, Semantic loss: 0.326538, BCE loss: 1.866471, SB loss: 0.212145
2025-01-16 18:22:10,672 Epoch: [0/20] Iter:[100/192], Time: 0.74, lr: [0.009765316869719067], Loss: 2.378399, Acc:0.385626, Semantic loss: 0.316995, BCE loss: 1.857419, SB loss: 0.203985
2025-01-16 18:22:17,763 Epoch: [0/20] Iter:[110/192], Time: 0.74, lr: [0.009741814299377942], Loss: 2.344918, Acc:0.393315, Semantic loss: 0.306741, BCE loss: 1.842368, SB loss: 0.195808
2025-01-16 18:22:24,946 Epoch: [0/20] Iter:[120/192], Time: 0.73, lr: [0.009718305427226986], Loss: 2.311098, Acc:0.399186, Semantic loss: 0.298756, BCE loss: 1.822363, SB loss: 0.189980
2025-01-16 18:22:32,002 Epoch: [0/20] Iter:[130/192], Time: 0.73, lr: [0.009694790234629266], Loss: 2.298996, Acc:0.399478, Semantic loss: 0.293162, BCE loss: 1.819502, SB loss: 0.186332
2025-01-16 18:22:39,113 Epoch: [0/20] Iter:[140/192], Time: 0.73, lr: [0.009671268702842338], Loss: 2.267143, Acc:0.406409, Semantic loss: 0.286207, BCE loss: 1.800326, SB loss: 0.180610
2025-01-16 18:22:46,441 Epoch: [0/20] Iter:[150/192], Time: 0.73, lr: [0.009647740813017376], Loss: 2.236388, Acc:0.409182, Semantic loss: 0.281143, BCE loss: 1.778200, SB loss: 0.177044
2025-01-16 18:22:53,721 Epoch: [0/20] Iter:[160/192], Time: 0.73, lr: [0.009624206546198262], Loss: 2.212909, Acc:0.410975, Semantic loss: 0.276776, BCE loss: 1.762097, SB loss: 0.174036
2025-01-16 18:23:01,199 Epoch: [0/20] Iter:[170/192], Time: 0.73, lr: [0.009600665883320689], Loss: 2.195708, Acc:0.414061, Semantic loss: 0.272478, BCE loss: 1.752430, SB loss: 0.170800
2025-01-16 18:23:08,776 Epoch: [0/20] Iter:[180/192], Time: 0.73, lr: [0.009577118805211254], Loss: 2.168520, Acc:0.419984, Semantic loss: 0.267436, BCE loss: 1.734270, SB loss: 0.166814
2025-01-16 18:23:15,956 Epoch: [0/20] Iter:[190/192], Time: 0.73, lr: [0.009553565292586523], Loss: 2.147997, Acc:0.421670, Semantic loss: 0.264091, BCE loss: 1.719664, SB loss: 0.164242
2025-01-16 18:26:52,638 0 [0.         0.3669818  0.10686949 0.03790184 0.03520359 0.07326311
 0.07884151 0.00197118] 0.0876290672625178
2025-01-16 18:26:52,639 1 [0.         0.44953099 0.09681778 0.0713256  0.03815486 0.08103571
 0.07454136 0.00665452] 0.10225760418013291
2025-01-16 18:26:52,640 => saving checkpoint to output\loveDa\pidnet_small_loveda_train_NO_AVDcheckpoint.pth.tar
2025-01-16 18:26:52,818 Loss: 1.418, MeanIU:  0.1023, Best_mIoU:  0.1023
2025-01-16 18:26:52,818 [0.         0.44953099 0.09681778 0.0713256  0.03815486 0.08103571
 0.07454136 0.00665452]
2025-01-16 18:26:53,567 Epoch: [1/20] Iter:[0/192], Time: 0.65, lr: [0.009548853816214998], Loss: 2.012718, Acc:0.441263, Semantic loss: 0.211011, BCE loss: 1.677290, SB loss: 0.124416
2025-01-16 18:27:00,555 Epoch: [1/20] Iter:[10/192], Time: 0.69, lr: [0.009525292556561479], Loss: 1.799254, Acc:0.465050, Semantic loss: 0.193399, BCE loss: 1.495365, SB loss: 0.110490
2025-01-16 18:27:07,603 Epoch: [1/20] Iter:[20/192], Time: 0.70, lr: [0.00950172481957719], Loss: 1.770021, Acc:0.479065, Semantic loss: 0.187932, BCE loss: 1.470115, SB loss: 0.111974
2025-01-16 18:27:14,838 Epoch: [1/20] Iter:[30/192], Time: 0.71, lr: [0.009478150585620286], Loss: 1.758339, Acc:0.477050, Semantic loss: 0.187113, BCE loss: 1.460725, SB loss: 0.110501
2025-01-16 18:27:22,236 Epoch: [1/20] Iter:[40/192], Time: 0.72, lr: [0.009454569834934885], Loss: 1.739753, Acc:0.475603, Semantic loss: 0.183316, BCE loss: 1.447719, SB loss: 0.108718
2025-01-16 18:27:29,478 Epoch: [1/20] Iter:[50/192], Time: 0.72, lr: [0.009430982547650114], Loss: 1.765984, Acc:0.481114, Semantic loss: 0.184416, BCE loss: 1.472401, SB loss: 0.109168
